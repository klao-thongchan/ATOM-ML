{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"ATOM"},{"location":"about/","text":"About What is it? Automated Tool for Optimized Modelling (ATOM) is an open-source Python package designed to help data scientists fasten up the exploration phase of their machine learning projects. ATOM is a low-code, easy-to-use library, capable of running experiments quickly and efficiently, enabling the user to go from raw data to generating insights in just a few lines of code. Click here to get started. What can I do with it? ATOM is an end-to-end solution for machine learning pipelines. It supports the user from raw data ingestion to the final results' analysis. Click on the icons to read more about its main functionalities. Data cleaning Feature engineering Model selection Hyperparameter tuning Model training Model predictions Experiment logging Analysis & Interpretability Who is it intended for? Data scientists that want to fasten up the exploration phase of their machine learning projects. Data scientists that want to run a simple modelling experiment without having to spend too much time on coding. Data scientists that are new to Python and are not (yet) familiar with all the relevant machine learning packages. Data analysts without extensive knowledge of machine learning that want to try out model-based solutions. Anyone who wants to rapidly build a Proof of Concept, for example during a hackathon. Anyone who is new to the field of machine learning and wants a low-code, easy to learn package, to get started building predictive pipelines. Citing ATOM If you use ATOM in a scientific publication, please consider citing this documentation page as the resource. ATOM\u2019s first stable release v2.0.3 was made publicly available in November 2019. A formatted version of the citation would look like this: ATOM v2.0.3, November 2019. URL https://tvdboom.github.io/ATOM/ BibTeX entry: @Manual{ATOM, title = {ATOM: A Python package for fast exploration of machine learning pipelines}, author = {Mavs}, year={2019}, mont={November}, note = {ATOM version 2.0.3}, url = {https://tvdboom.github.io/ATOM/}, } Support ATOM recognizes the support from JetBrains by providing core project contributors with a set of developer tools free of charge.","title":"About"},{"location":"about/#about","text":"","title":"About"},{"location":"about/#what-is-it","text":"Automated Tool for Optimized Modelling (ATOM) is an open-source Python package designed to help data scientists fasten up the exploration phase of their machine learning projects. ATOM is a low-code, easy-to-use library, capable of running experiments quickly and efficiently, enabling the user to go from raw data to generating insights in just a few lines of code. Click here to get started.","title":"What is it?"},{"location":"about/#what-can-i-do-with-it","text":"ATOM is an end-to-end solution for machine learning pipelines. It supports the user from raw data ingestion to the final results' analysis. Click on the icons to read more about its main functionalities. Data cleaning Feature engineering Model selection Hyperparameter tuning Model training Model predictions Experiment logging Analysis & Interpretability","title":"What can I do with it?"},{"location":"about/#who-is-it-intended-for","text":"Data scientists that want to fasten up the exploration phase of their machine learning projects. Data scientists that want to run a simple modelling experiment without having to spend too much time on coding. Data scientists that are new to Python and are not (yet) familiar with all the relevant machine learning packages. Data analysts without extensive knowledge of machine learning that want to try out model-based solutions. Anyone who wants to rapidly build a Proof of Concept, for example during a hackathon. Anyone who is new to the field of machine learning and wants a low-code, easy to learn package, to get started building predictive pipelines.","title":"Who is it intended for?"},{"location":"about/#citing-atom","text":"If you use ATOM in a scientific publication, please consider citing this documentation page as the resource. ATOM\u2019s first stable release v2.0.3 was made publicly available in November 2019. A formatted version of the citation would look like this: ATOM v2.0.3, November 2019. URL https://tvdboom.github.io/ATOM/ BibTeX entry: @Manual{ATOM, title = {ATOM: A Python package for fast exploration of machine learning pipelines}, author = {Mavs}, year={2019}, mont={November}, note = {ATOM version 2.0.3}, url = {https://tvdboom.github.io/ATOM/}, }","title":"Citing ATOM"},{"location":"about/#support","text":"ATOM recognizes the support from JetBrains by providing core project contributors with a set of developer tools free of charge.","title":"Support"},{"location":"contributing/","text":"Contributing Are you interested in contributing to ATOM? Do you want to report a bug? Do you have a question? Before you do, please read the following guidelines. Submission context Question or problem? For quick questions there's no need to open an issue. Check first if the question isn't already answered on the FAQ section. If not, reach us through the discussions page or on the slack channel. Report a bug? If you found a bug in the source code, you can help by submitting an issue to the issue tracker in the GitHub repository. Even better, you can submit a Pull Request with a fix. However, before doing so, please read the submission guidelines . Missing a feature? You can request a new feature by submitting an issue to the GitHub Repository. If you would like to implement a new feature, please submit an issue with a proposal for your work first. Please consider what kind of change it is: For a major feature , first open an issue and outline your proposal so that it can be discussed. This will also allow us to better coordinate our efforts, prevent duplication of work, and help you to craft the change so that it is successfully accepted into the project. Small features and bugs can be crafted and directly submitted as a Pull Request. However, there is no guarantee that your feature will make it into master , as it's always a matter of opinion whether if benefits the overall functionality of the project. Project layout The latest stable release of ATOM is on the master branch, whereas the latest version of ATOM in development is on the development branch. Make sure you are looking at and working on the correct branch if you're looking to contribute code. In terms of directory structure: All of ATOM's code sources are in the atom directory. The documentation sources are in the docs_sources directory. Images in the documentation are in the docs_sources/img directory. Tutorial notebooks are in the examples directory. If you want to include the example to the documentation as well, add the .ipynb file to docs_sources/examples and update the mkdocs.yml file accordingly. Unit tests are in the tests directory. Make sure to add the tests to the file corresponding to the module in the atom directory with the code that is being tested. Make sure to familiarize yourself with the project layout before making any major contributions, and especially make sure to send all code changes to the development branch. Submission guidelines Submitting an issue Before you submit an issue, please search the issue tracker , maybe an issue for your problem already exists and the discussion might inform you of workarounds readily available. We want to fix all the issues as soon as possible, but before fixing a bug we need to reproduce and confirm it. In order to reproduce bugs we will systematically ask you to provide a minimal reproduction scenario using the custom issue template. Submitting a pull request Before you submit a pull request, please work through this checklist to make sure that you have done everything necessary so we can efficiently review and accept your changes. Update the documentation so all of your changes are reflected there. Adhere to PEP 8 standards. Use a maximum of 89 characters per line. Try to keep docstrings below 73 characters. Update the project unit tests to test your code changes as thoroughly as possible. Make sure that your code is properly commented with docstrings and comments explaining your rationale behind non-obvious coding practices. Run isort : isort atom tests . Run flake8 : flake8 --show-source --statistics atom tests . If your contribution requires a new library dependency: Double-check that the new dependency is easy to install via pip and Anaconda. The library should support Python 3.8 and higher. Make sure the code works with the latest version of the library. Update the dependencies in the documentation. Add the library with the minimum required version to pyproject.toml . After submitting your pull request, GitHub will automatically run the tests on your changes and make sure that the updated code builds successfully. The checks are run on Python 3.8, 3.9 and 3.10, on Ubuntu and Windows. We also use services that automatically check code style and test coverage.","title":"Contributing"},{"location":"contributing/#contributing","text":"Are you interested in contributing to ATOM? Do you want to report a bug? Do you have a question? Before you do, please read the following guidelines.","title":"Contributing"},{"location":"contributing/#submission-context","text":"","title":"Submission context"},{"location":"contributing/#question-or-problem","text":"For quick questions there's no need to open an issue. Check first if the question isn't already answered on the FAQ section. If not, reach us through the discussions page or on the slack channel.","title":"Question or problem?"},{"location":"contributing/#report-a-bug","text":"If you found a bug in the source code, you can help by submitting an issue to the issue tracker in the GitHub repository. Even better, you can submit a Pull Request with a fix. However, before doing so, please read the submission guidelines .","title":"Report a bug?"},{"location":"contributing/#missing-a-feature","text":"You can request a new feature by submitting an issue to the GitHub Repository. If you would like to implement a new feature, please submit an issue with a proposal for your work first. Please consider what kind of change it is: For a major feature , first open an issue and outline your proposal so that it can be discussed. This will also allow us to better coordinate our efforts, prevent duplication of work, and help you to craft the change so that it is successfully accepted into the project. Small features and bugs can be crafted and directly submitted as a Pull Request. However, there is no guarantee that your feature will make it into master , as it's always a matter of opinion whether if benefits the overall functionality of the project.","title":"Missing a feature?"},{"location":"contributing/#project-layout","text":"The latest stable release of ATOM is on the master branch, whereas the latest version of ATOM in development is on the development branch. Make sure you are looking at and working on the correct branch if you're looking to contribute code. In terms of directory structure: All of ATOM's code sources are in the atom directory. The documentation sources are in the docs_sources directory. Images in the documentation are in the docs_sources/img directory. Tutorial notebooks are in the examples directory. If you want to include the example to the documentation as well, add the .ipynb file to docs_sources/examples and update the mkdocs.yml file accordingly. Unit tests are in the tests directory. Make sure to add the tests to the file corresponding to the module in the atom directory with the code that is being tested. Make sure to familiarize yourself with the project layout before making any major contributions, and especially make sure to send all code changes to the development branch.","title":"Project layout"},{"location":"contributing/#submission-guidelines","text":"","title":"Submission guidelines"},{"location":"contributing/#submitting-an-issue","text":"Before you submit an issue, please search the issue tracker , maybe an issue for your problem already exists and the discussion might inform you of workarounds readily available. We want to fix all the issues as soon as possible, but before fixing a bug we need to reproduce and confirm it. In order to reproduce bugs we will systematically ask you to provide a minimal reproduction scenario using the custom issue template.","title":"Submitting an issue"},{"location":"contributing/#submitting-a-pull-request","text":"Before you submit a pull request, please work through this checklist to make sure that you have done everything necessary so we can efficiently review and accept your changes. Update the documentation so all of your changes are reflected there. Adhere to PEP 8 standards. Use a maximum of 89 characters per line. Try to keep docstrings below 73 characters. Update the project unit tests to test your code changes as thoroughly as possible. Make sure that your code is properly commented with docstrings and comments explaining your rationale behind non-obvious coding practices. Run isort : isort atom tests . Run flake8 : flake8 --show-source --statistics atom tests . If your contribution requires a new library dependency: Double-check that the new dependency is easy to install via pip and Anaconda. The library should support Python 3.8 and higher. Make sure the code works with the latest version of the library. Update the dependencies in the documentation. Add the library with the minimum required version to pyproject.toml . After submitting your pull request, GitHub will automatically run the tests on your changes and make sure that the updated code builds successfully. The checks are run on Python 3.8, 3.9 and 3.10, on Ubuntu and Windows. We also use services that automatically check code style and test coverage.","title":"Submitting a pull request"},{"location":"dependencies/","text":"Dependencies Python & OS As of the moment, ATOM supports the following Python versions: Python 3.8 Python 3.9 Python 3.10 And operating systems: Linux (Ubuntu, Fedora, etc...) Windows 8.1+ macOS (not tested) Packages Required ATOM is built on top of several existing Python libraries. These packages are necessary for its correct functioning. category-encoders (>=2.4.1) explainerdashboard (>=0.4.0) dill (>=0.3.5) evalml (>=054.0) gplearn (>=0.4.1) gradio (>=3.1.7) imbalanced-learn (>=0.9.0) featuretools (>=1.12.0) joblib (>=1.1.0) matplotlib (>=3.5.0) mlflow (>=1.26) nltk (>=3.7) numpy (~=1.22) optuna (>=2.10.1) pandas (>=1.3.5) pandas-profiling (>=3.2.0) seaborn (>=0.11.0) shap (>=0.41) schemdraw (>=0.14) scikit-learn (>=1.1.0) scikit-learn-intelex (>=2021.6.3) scipy (>=1.7.3) typeguard (>=2.13) wordcloud (>=1.8.1) zoofs (>=0.1.24) Optional You can install some optional packages to be able to use some well-known machine learning estimators that are not provided by sklearn but are among ATOM's predefined models . Install them using pip install atom-ml[models] . catboost (>=1.0.4) lightgbm (>=3.3.2) xgboost (>=0.90) Development The development dependencies are not installed with the package, and are not required for any of its functionalities. These libraries are only necessary to contribute to the project. Install them using pip install atom-ml[dev] . Linting isort (>=5.10.1) flake8 (>=5.0.4) flake8-pyproject (>=1.1.0) Testing pytest (>=7.1.0) pytest-cov (>=3.0.0) tensorflow (>=2.3.1) Documentation mike (>=1.1.2) mkdocs (>=1.2.3) mkdocs-autorefs (>=0.4.1) mkdocs-jupyter (>=0.20.1) mkdocs-material (>=8.4.1) mkdocs-simple-hooks (>=0.1.5) pyyaml (>=6.0)","title":"Dependencies"},{"location":"dependencies/#dependencies","text":"","title":"Dependencies"},{"location":"dependencies/#python-os","text":"As of the moment, ATOM supports the following Python versions: Python 3.8 Python 3.9 Python 3.10 And operating systems: Linux (Ubuntu, Fedora, etc...) Windows 8.1+ macOS (not tested)","title":"Python &amp; OS"},{"location":"dependencies/#packages","text":"","title":"Packages"},{"location":"dependencies/#required","text":"ATOM is built on top of several existing Python libraries. These packages are necessary for its correct functioning. category-encoders (>=2.4.1) explainerdashboard (>=0.4.0) dill (>=0.3.5) evalml (>=054.0) gplearn (>=0.4.1) gradio (>=3.1.7) imbalanced-learn (>=0.9.0) featuretools (>=1.12.0) joblib (>=1.1.0) matplotlib (>=3.5.0) mlflow (>=1.26) nltk (>=3.7) numpy (~=1.22) optuna (>=2.10.1) pandas (>=1.3.5) pandas-profiling (>=3.2.0) seaborn (>=0.11.0) shap (>=0.41) schemdraw (>=0.14) scikit-learn (>=1.1.0) scikit-learn-intelex (>=2021.6.3) scipy (>=1.7.3) typeguard (>=2.13) wordcloud (>=1.8.1) zoofs (>=0.1.24)","title":"Required"},{"location":"dependencies/#optional","text":"You can install some optional packages to be able to use some well-known machine learning estimators that are not provided by sklearn but are among ATOM's predefined models . Install them using pip install atom-ml[models] . catboost (>=1.0.4) lightgbm (>=3.3.2) xgboost (>=0.90)","title":"Optional"},{"location":"dependencies/#development","text":"The development dependencies are not installed with the package, and are not required for any of its functionalities. These libraries are only necessary to contribute to the project. Install them using pip install atom-ml[dev] . Linting isort (>=5.10.1) flake8 (>=5.0.4) flake8-pyproject (>=1.1.0) Testing pytest (>=7.1.0) pytest-cov (>=3.0.0) tensorflow (>=2.3.1) Documentation mike (>=1.1.2) mkdocs (>=1.2.3) mkdocs-autorefs (>=0.4.1) mkdocs-jupyter (>=0.20.1) mkdocs-material (>=8.4.1) mkdocs-simple-hooks (>=0.1.5) pyyaml (>=6.0)","title":"Development"},{"location":"faq/","text":"Frequently asked questions Here we try to give answers to some questions that have popped up regularly. If you have any other questions, don't hesitate to create a new discussion or post them on the Slack channel ! How does ATOM relate to AutoML? There is, indeed, a text editor with the same name and a similar logo as this package. Is this a shameless copy? No. When I started the project, I didn't know about the text editor, and it doesn't require much thinking to come up with the idea of replacing the letter O of the word atom with the image of an atom. Is this package related to the Atom text editor? ATOM is not an AutoML tool since it does not automate the search for an optimal pipeline like well known AutoML tools such as auto-sklearn or EvalML do. Instead, ATOM helps the user find the optimal pipeline himself. One of the goals of this package is to help data scientists produce explainable pipelines, and using an AutoML black box function would impede that. That said, it is possible to integrate a EvalML pipeline with atom through the automl method. Is it possible to run deep learning models? Yes. Deep learning models can be added as custom models to the pipeline as long as they follow sklearn's API . For more information, see the deep learning section of the user guide. Can I run atom's methods on just a subset of the columns? Yes, all data cleaning and feature engineering methods accept a columns parameter to only transform the selected features. For example, to only impute the numerical columns in the dataset we could type atom.impute(strat_num=\"mean\", columns=atom.numerical) . The parameter accepts column names, column indices, dtypes or a slice object. How can I compare the same model on different datasets? In many occasions you might want to test how a model performs on datasets processed with different pipelines. For this, atom has the branch system . Create a new branch for every new pipeline you want to test and use the plot methods to compare all models, independent of the branch it was trained on. Can I train models through atom using a GPU? Yes. Refer to the user guide to see what algorithms and models have a GPU implementation. Be aware that it requires additional software and hardware dependencies. How are numerical and categorical columns differentiated? The columns are separated using a dataframe's select_dtypes method. Numerical columns are selected using include=\"number\" whereas categorical columns are selected using exclude=\"number\" . Can I run unsupervised learning pipelines? No. As for now, ATOM only supports supervised machine learning pipelines. However, various unsupervised algorithms can be chosen as strategy in the Pruner class to detect and remove outliers from the dataset. Is there a way to plot multiple models in the same shap plot? No. Unfortunately, there is no way to plot multiple models in the same shap plot since the plots are made by the shap package and passed as matplotlib.axes objects to atom. This means that it's not within the reach of this package to implement such a utility. Can I merge a sklearn pipeline with atom? Yes. Like any other transformer, it is possible to add a sklearn pipeline to atom using the add method. Every transformer in the pipeline is merged independently. The pipeline is not allowed to end with a model since atom manages its own models. If that is the case, add the pipeline using atom.add(pipeline[:-1]) . Is it possible to initialize atom with an existing train and test set? Yes. If you already have a separated train and test set you can initialize atom in two ways: atom = ATOMClassifier(train, test) atom = ATOMClassifier((X_train, y_train), (X_test, y_test)) Make sure the train and test size have the same number of columns! If atom is initialized in any of these two ways, the test_size parameter is ignored. Can I train the models using cross-validation? Applying cross-validation means transforming every step of the pipeline multiple times, each with different results. Doing this would prevent ATOM from being able to show the transformation results after every pre-processing step, which means losing the ability to inspect how a transformer changed the dataset. For this reason, it is not possible to apply cross-validation until after a model has been trained. After a model has been trained, the pipeline is defined, and cross-validation can be applied using the cross_validate method. See here an example using cross-validation. Is there a way to process datetime features? Yes, the FeatureExtractor class can automatically extract useful features (day, month, year, etc...) from datetime columns. The extracted features are always encoded to numerical values, so they can be fed directly to a model.","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"Here we try to give answers to some questions that have popped up regularly. If you have any other questions, don't hesitate to create a new discussion or post them on the Slack channel ! How does ATOM relate to AutoML? There is, indeed, a text editor with the same name and a similar logo as this package. Is this a shameless copy? No. When I started the project, I didn't know about the text editor, and it doesn't require much thinking to come up with the idea of replacing the letter O of the word atom with the image of an atom. Is this package related to the Atom text editor? ATOM is not an AutoML tool since it does not automate the search for an optimal pipeline like well known AutoML tools such as auto-sklearn or EvalML do. Instead, ATOM helps the user find the optimal pipeline himself. One of the goals of this package is to help data scientists produce explainable pipelines, and using an AutoML black box function would impede that. That said, it is possible to integrate a EvalML pipeline with atom through the automl method. Is it possible to run deep learning models? Yes. Deep learning models can be added as custom models to the pipeline as long as they follow sklearn's API . For more information, see the deep learning section of the user guide. Can I run atom's methods on just a subset of the columns? Yes, all data cleaning and feature engineering methods accept a columns parameter to only transform the selected features. For example, to only impute the numerical columns in the dataset we could type atom.impute(strat_num=\"mean\", columns=atom.numerical) . The parameter accepts column names, column indices, dtypes or a slice object. How can I compare the same model on different datasets? In many occasions you might want to test how a model performs on datasets processed with different pipelines. For this, atom has the branch system . Create a new branch for every new pipeline you want to test and use the plot methods to compare all models, independent of the branch it was trained on. Can I train models through atom using a GPU? Yes. Refer to the user guide to see what algorithms and models have a GPU implementation. Be aware that it requires additional software and hardware dependencies. How are numerical and categorical columns differentiated? The columns are separated using a dataframe's select_dtypes method. Numerical columns are selected using include=\"number\" whereas categorical columns are selected using exclude=\"number\" . Can I run unsupervised learning pipelines? No. As for now, ATOM only supports supervised machine learning pipelines. However, various unsupervised algorithms can be chosen as strategy in the Pruner class to detect and remove outliers from the dataset. Is there a way to plot multiple models in the same shap plot? No. Unfortunately, there is no way to plot multiple models in the same shap plot since the plots are made by the shap package and passed as matplotlib.axes objects to atom. This means that it's not within the reach of this package to implement such a utility. Can I merge a sklearn pipeline with atom? Yes. Like any other transformer, it is possible to add a sklearn pipeline to atom using the add method. Every transformer in the pipeline is merged independently. The pipeline is not allowed to end with a model since atom manages its own models. If that is the case, add the pipeline using atom.add(pipeline[:-1]) . Is it possible to initialize atom with an existing train and test set? Yes. If you already have a separated train and test set you can initialize atom in two ways: atom = ATOMClassifier(train, test) atom = ATOMClassifier((X_train, y_train), (X_test, y_test)) Make sure the train and test size have the same number of columns! If atom is initialized in any of these two ways, the test_size parameter is ignored. Can I train the models using cross-validation? Applying cross-validation means transforming every step of the pipeline multiple times, each with different results. Doing this would prevent ATOM from being able to show the transformation results after every pre-processing step, which means losing the ability to inspect how a transformer changed the dataset. For this reason, it is not possible to apply cross-validation until after a model has been trained. After a model has been trained, the pipeline is defined, and cross-validation can be applied using the cross_validate method. See here an example using cross-validation. Is there a way to process datetime features? Yes, the FeatureExtractor class can automatically extract useful features (day, month, year, etc...) from datetime columns. The extracted features are always encoded to numerical values, so they can be fed directly to a model.","title":"Frequently asked questions"},{"location":"getting_started/","text":"Getting started Installation Standard installation Install ATOM's newest release easily via pip : pip install -U atom-ml or via conda : conda install -c conda-forge atom-ml Note Since atom was already taken, download the package under the name atom-ml ! Optional dependencies To install the optional dependencies , add [models] after the package's name. pip install -U atom-ml[models] Latest source Sometimes, new features and bug fixes are already implemented in the development branch, but waiting for the next release to be made available. If you can't wait for that, it's possible to install the package directly from git. pip install git+https://github.com/tvdboom/ATOM.git@development#egg=atom-ml Don't forget to include #egg=atom-ml to explicitly name the project, this way pip can track metadata for it without having to have run the setup.py script. Contributing If you are planning to contribute to the project, you'll need the development dependencies . Install them adding [dev] after the package's name. pip install -U atom-ml[dev] Click here for a complete list of package files for all versions published on PyPI. Usage ATOM contains a variety of classes and functions to perform data cleaning, feature engineering, model training, plotting and much more. The easiest way to use everything ATOM has to offer is through one of the main classes: ATOMClassifier for binary or multiclass classification tasks. ATOMRegressor for regression tasks. Let's walk you through an example. Click on the Google Colab badge on top of this section to run this example yourself. Make the necessary imports and load the data. import pandas as pd from atom import ATOMClassifier # Load the Australian Weather dataset X = pd . read_csv ( \"https://raw.githubusercontent.com/tvdboom/ATOM/master/examples/datasets/weatherAUS.csv\" ) X . head () Initialize the ATOMClassifier or ATOMRegressor class. These two classes are convenient wrappers for the whole machine learning pipeline. Contrary to sklearn's API, they are initialized providing the data you want to manipulate. This data is stored in the instance and can be accessed at any moment through atom's data attributes . You can either let atom split the dataset into a train and test set or provide the sets yourself. atom = ATOMClassifier ( X , y = \"RainTomorrow\" , test_size = 0.3 , verbose = 2 ) Data transformations are applied through atom's methods. For example, calling the impute method will initialize an Imputer instance, fit it on the training set and transform the whole dataset. The transformations are applied immediately after calling the method (no fit and transform commands necessary). atom . impute ( strat_num = \"median\" , strat_cat = \"most_frequent\" ) atom . encode ( strategy = \"LeaveOneOut\" , max_onehot = 8 ) Similarly, models are trained and evaluated using the run method. Here, we fit both a Random Forest and AdaBoost model, and apply hyperparameter tuning . atom . run ( models = [ \"RF\" , \"AdaB\" ], metric = \"auc\" , n_calls = 10 , n_initial_points = 4 ) Lastly, visualize the result using the integrated plots . atom . plot_roc () atom . rf . plot_confusion_matrix ( normalize = True )","title":"Getting started"},{"location":"getting_started/#getting-started","text":"","title":"Getting started"},{"location":"getting_started/#installation","text":"Standard installation Install ATOM's newest release easily via pip : pip install -U atom-ml or via conda : conda install -c conda-forge atom-ml Note Since atom was already taken, download the package under the name atom-ml ! Optional dependencies To install the optional dependencies , add [models] after the package's name. pip install -U atom-ml[models] Latest source Sometimes, new features and bug fixes are already implemented in the development branch, but waiting for the next release to be made available. If you can't wait for that, it's possible to install the package directly from git. pip install git+https://github.com/tvdboom/ATOM.git@development#egg=atom-ml Don't forget to include #egg=atom-ml to explicitly name the project, this way pip can track metadata for it without having to have run the setup.py script. Contributing If you are planning to contribute to the project, you'll need the development dependencies . Install them adding [dev] after the package's name. pip install -U atom-ml[dev] Click here for a complete list of package files for all versions published on PyPI.","title":"Installation"},{"location":"getting_started/#usage","text":"ATOM contains a variety of classes and functions to perform data cleaning, feature engineering, model training, plotting and much more. The easiest way to use everything ATOM has to offer is through one of the main classes: ATOMClassifier for binary or multiclass classification tasks. ATOMRegressor for regression tasks. Let's walk you through an example. Click on the Google Colab badge on top of this section to run this example yourself. Make the necessary imports and load the data. import pandas as pd from atom import ATOMClassifier # Load the Australian Weather dataset X = pd . read_csv ( \"https://raw.githubusercontent.com/tvdboom/ATOM/master/examples/datasets/weatherAUS.csv\" ) X . head () Initialize the ATOMClassifier or ATOMRegressor class. These two classes are convenient wrappers for the whole machine learning pipeline. Contrary to sklearn's API, they are initialized providing the data you want to manipulate. This data is stored in the instance and can be accessed at any moment through atom's data attributes . You can either let atom split the dataset into a train and test set or provide the sets yourself. atom = ATOMClassifier ( X , y = \"RainTomorrow\" , test_size = 0.3 , verbose = 2 ) Data transformations are applied through atom's methods. For example, calling the impute method will initialize an Imputer instance, fit it on the training set and transform the whole dataset. The transformations are applied immediately after calling the method (no fit and transform commands necessary). atom . impute ( strat_num = \"median\" , strat_cat = \"most_frequent\" ) atom . encode ( strategy = \"LeaveOneOut\" , max_onehot = 8 ) Similarly, models are trained and evaluated using the run method. Here, we fit both a Random Forest and AdaBoost model, and apply hyperparameter tuning . atom . run ( models = [ \"RF\" , \"AdaB\" ], metric = \"auc\" , n_calls = 10 , n_initial_points = 4 ) Lastly, visualize the result using the integrated plots . atom . plot_roc () atom . rf . plot_confusion_matrix ( normalize = True )","title":"Usage"},{"location":"license/","text":"MIT License Copyright \u00a9 2022 Mavs Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#mit-license","text":"Copyright \u00a9 2022 Mavs Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"API/ATOM/atomclassifier/","text":"ATOMClassifier class atom.api. ATOMClassifier (*arrays, y=-1, index=False, shuffle=True, stratify=True, n_rows=1, test_size=0.2, holdout_size=None, n_jobs=1, device=\"cpu\", engine=\"sklearn\", verbose=0, warnings=False, logger=None, experiment=None, random_state=None) [source] Main class for binary and multiclass classification tasks. Apply all data transformations and model management provided by the package on a given dataset. Note that, contrary to sklearn's API, the instance contains the dataset on which to perform the analysis. Calling a method will automatically apply it on the dataset it contains. All data cleaning , feature engineering , model training , predicting , and plotting functionality can be accessed from an instacne of this class. Parameters *arrays: sequence of indexables Dataset containing features and target. Allowed formats are: X X, y train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) X, train, test: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str or sequence Target column corresponding to X. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. y: int, str or sequence, default=-1 Target column corresponding to X. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. This parameter is ignored if the target column is provided through arrays . index: bool, int, str or sequence, default=False Handle the index in the resulting dataframe. If False: Reset to RangeIndex . If True: Use the provided index. If int: Position of the column to use as index. If str: Name of the column to use as index. If sequence: Array with shape=(n_samples,) to use as index. test_size: int or float, default=0.2 If <=1: Fraction of the dataset to include in the test set. If >1: Number of rows to include in the test set. This parameter is ignored if the test set is provided through arrays . holdout_size: int, float or None, default=None If None: No holdout data set is kept apart. If <=1: Fraction of the dataset to include in the holdout set. If >1: Number of rows to include in the holdout set. This parameter is ignored if the holdout set is provided through arrays . shuffle: bool, default=True Whether to shuffle the dataset before splitting the train and test set. Be aware that not shuffling the dataset can cause an unequal distribution of target classes over the sets. stratify: bool, int, str or sequence, default=True Handle stratification of the target classes over the data sets. If False: The data is split randomly. If True: The data is stratified over the target column. Else: Name or position of the columns to stratify by. The columns can't contain NaN values. This parameter is ignored if shuffle=False or if the test set is provided through arrays . n_rows: int or float, default=1 Subsample of the dataset to use. The default value selects all the rows. If <=1: Fraction of the dataset to select. If >1: Exact number of rows to select. Only if arrays is X or X, y. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If <-1: Use number of cores - 1 + n_jobs . device: str, default=\"cpu\" Device on which to train the estimators. Use any string that follows the SYCL_DEVICE_FILTER filter selector, e.g. device=\"gpu\" to use the GPU. Read more in the user guide . engine: str, default=\"sklearn\" Execution engine to use for the estimators. Refer to the user guide for an explanation regarding every choice. Choose from: \"sklearn\" (only if device=\"cpu\") \"sklearnex\" \"cuml\" (only if device=\"gpu\") verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of python's warnings filters . Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic name. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState used by np.random . See Also ATOMRegressor Main class for regression tasks. Example >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> # Initialize atom >>> atom = ATOMClassifier ( X , y , logger = \"auto\" , n_jobs = 2 , verbose = 2 ) << ================== ATOM ================== >> Algorithm task: binary classification. Parallel processing with 2 cores. Dataset stats ==================== >> Shape: (569, 31) Memory: 138.96 kB Scaled: False Outlier values: 160 (1.1%) ------------------------------------- Train set size: 456 Test set size: 113 ------------------------------------- | | dataset | train | test | | - | ----------- | ----------- | ----------- | | 0 | 212 (1.0) | 170 (1.0) | 42 (1.0) | | 1 | 357 (1.7) | 286 (1.7) | 71 (1.7) | >>> # Apply data cleaning and feature engineering methods >>> atom . balance ( strategy = \"smote\" ) Oversampling with SMOTE... --> Adding 116 samples to class 0. >>> atom . feature_selection ( strategy = \"rfecv\" , solver = \"xgb\" , n_features = 22 ) Fitting FeatureSelector... Performing feature selection ... --> RFECV selected 26 features from the dataset. --> Dropping feature mean perimeter (rank 4). --> Dropping feature mean symmetry (rank 3). --> Dropping feature perimeter error (rank 2). --> Dropping feature worst compactness (rank 5). >>> # Train models >>> atom . run ( ... models = [ \"LR\" , \"RF\" , \"XGB\" ], ... metric = \"precision\" , ... n_bootstrap = 4 , ... ) Training ========================= >> Models: LR, RF, XGB Metric: precision Results for Logistic Regression: Fit --------------------------------------------- Train evaluation --> precision: 0.9895 Test evaluation --> precision: 0.9467 Time elapsed: 0.028s ------------------------------------------------- Total time: 0.028s Results for Random Forest: Fit --------------------------------------------- Train evaluation --> precision: 1.0 Test evaluation --> precision: 0.9221 Time elapsed: 0.181s ------------------------------------------------- Total time: 0.181s Results for XGBoost: Fit --------------------------------------------- Train evaluation --> precision: 1.0 Test evaluation --> precision: 0.9091 Time elapsed: 0.124s ------------------------------------------------- Total time: 0.124s Final results ==================== >> Duration: 0.333s ------------------------------------- Logistic Regression --> precision: 0.9467 ! Random Forest --> precision: 0.9221 XGBoost --> precision: 0.9091 >>> # Analyze the results >>> atom . plot_results () Magic methods The class contains some magic methods to help you access some of its elements faster. Note that methods that apply on the pipeline can return different results per branch. __repr__: Prints an overview of atom's branches, models, metric and errors. __len__: Returns the length of the dataset. __iter__: Iterate over the pipeline's transformers. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a branch, model, column or subset of the dataset. Attributes Data attributes The data attributes are used to access the dataset and its properties. Updating the dataset will automatically update the response of these attributes accordingly. Attributes pipeline: pd.Series Transformers fitted on the data. Use this attribute only to access the individual instances. To visualize the pipeline, use the plot_pipeline method. mapping: dict Encoded values and their respective mapped values. The column name is the key to its mapping dictionary. Only for columns mapped to a single column (e.g. Ordinal, Leave-one-out, etc...). dataset: pd.DataFrame Complete data set. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Features of the training set. y_train: pd.Series Target column of the training set. X_test: pd.DataFrame Features of the test set. y_test: pd.Series Target column of the test set. shape: Tuple[int, int] Shape of the dataset (n_rows, n_cols). columns: pd.Series Name of all the columns. n_columns: int Number of columns. features: pd.Series Name of the features. n_features: int Number of features. target: str Name of the target column. scaled: bool Whether the feature set is scaled. A data set is considered scaled when it has mean=0 and std=1, or when atom has a scaler in the pipeline. Returns None for sparse datasets . duplicates: pd.Series Number of duplicate rows in the dataset. nans: pd.Series Columns with the number of missing values in them. n_nans: int Number of samples containing missing values. numerical: pd.Series Names of the numerical features in the dataset. n_numerical: int Number of numerical features in the dataset. categorical: pd.Series Names of the categorical features in the dataset. n_categorical: int Number of categorical features in the dataset. outliers: pd.Series Columns in training set with amount of outlier values. n_outliers: int Number of samples in the training set containing outliers. classes: pd.DataFrame Distribution of target classes per data set. n_classes: int Number of classes in the target column. Utility attributes The utility attributes are used to access information about the models in the instance after training . Attributes models: str or list Return the names of all models. metric: str or list Return the name of the metric. errors: dict Return the errors encountered during model training. winners: list Return the model names ordered by performance. winner: models Return the best performing model. Tracking attributes The tracking attributes are used to customize what elements of the experiment are tracked. Read more in the user guide . Attributes log_bo: bool Whether to track every trial of the hyperparameter tuning. log_model: bool Whether to save the model's estimator after fitting. log_plots: bool Whether to save plots as artifacts. log_data: bool Whether to save the train and test sets. log_pipeline: bool Whether to save the model's pipeline. Plot attributes The plot attributes are used to customize the plot's aesthetics. Read more in the user guide . Attributes style: str Plotting style . palette: str Color palette . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes. Utility methods The class contains a variety of utility methods to handle the data and manage the pipeline. add Add a transformer to the pipeline. apply Apply a function to the dataset. automl Search for an optimized pipeline in an automated fashion. available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. distribution Get statistics on column distributions. evaluate Get all models' scores for the provided metrics. export_pipeline Export atom's pipeline. get_class_weight Return class weights for a balanced dataset. inverse_transform Inversely transform new data through the pipeline. log Print message and save to log file. merge Merge another instance of the same class into this one. report Create an extensive profile analysis report of the data. reset Reset the instance to it's initial state. reset_aesthetics Reset the plot aesthetics to their default values. save Save the instance to a pickle file. save_data Save the data in the current branch to a .csv file. shrink Converts the columns to the smallest possible matching dtype. stacking Add a Stacking model to the pipeline. stats Print basic information about the dataset. status Get an overview of the branches and models. transform Transform new data through the pipeline. voting Add a Voting model to the pipeline. method add (transformer, columns=None, train_only=False, **fit_params) [source] Add a transformer to the pipeline. If the transformer is not fitted, it is fitted on the complete training set. Afterwards, the data set is transformed and the estimator is added to atom's pipeline. If the estimator is a sklearn Pipeline, every estimator is merged independently with atom. Warning The transformer should have fit and/or transform methods with arguments X (accepting a dataframe-like object of shape=(n_samples, n_features)) and/or y (accepting a sequence of shape=(n_samples,)). The transform method should return a feature set as a dataframe-like object of shape=(n_samples, n_features) and/or a target column as a sequence of shape=(n_samples,). Note If the transform method doesn't return a dataframe: The column naming happens as follows. If the transformer has a get_feature_names or get_feature_names_out method, it is used. If not, and it returns the same number of columns, the names are kept equal. If the number of columns change, old columns will keep their name (as long as the column is unchanged) and new columns will receive the name x[N-1] , where N stands for the n-th feature. This means that a transformer should only transform, add or drop columns, not combinations of these. The index remains the same as before the transformation. This means that the transformer should not add, remove or shuffle rows unless it returns a dataframe. Note If the transformer has a n_jobs and/or random_state parameter that is left to its default value, it adopts atom's value. Parameters transformer: Transformer Estimator to add to the pipeline. Should implement a transform method. columns: int, str, slice, sequence or None, default=None Names, indices or dtypes of the columns in the dataset to transform. If None, transform all columns. Add ! in front of a name or dtype to exclude that column, e.g. atom.add(Transformer(), columns=\"!Location\") transforms all columns except Location`. You can either include or exclude columns, not combinations of these. The target column is always included if required by the transformer. train_only: bool, default=False Whether to apply the estimator only on the training set or on the complete dataset. Note that if True, the transformation is skipped when making predictions on new data. **fit_params Additional keyword arguments for the transformer's fit method. method apply (func, inverse_func=None, kw_args=None, inv_kw_args=None, **kwargs) [source] Apply a function to the dataset. The function should have signature func(dataset, **kw_args) -> dataset . This method is useful for stateless transformations such as taking the log, doing custom scaling, etc... Note This approach is preferred over changing the dataset directly through the property's @setter since the transformation is stored in the pipeline. Tip Use atom . apply ( lambda df : df . drop ( \"column_name\" , axis = 1 )) to store the removal of columns in the pipeline. Parameters func: callable Function to apply. inverse_func: callable or None, default=None Inverse function of func . If None, the inverse_transform method returns the input unchanged. kw_args: dict or None, default=None Additional keyword arguments for the function. inv_kw_args: dict or None, default=None Additional keyword arguments for the inverse function. method automl (**kwargs) [source] Search for an optimized pipeline in an automated fashion. Automated machine learning (AutoML) automates the selection, composition and parameterization of machine learning pipelines. Automating the machine learning often provides faster, more accurate outputs than hand-coded algorithms. ATOM uses the evalML package for AutoML optimization. The resulting transformers and final estimator are merged with atom's pipeline (check the pipeline and models attributes after the method finishes running). The created AutoMLSearch instance can be accessed through the evalml attribute. Warning AutoML algorithms aren't intended to run for only a few minutes. The method may need a very long time to achieve optimal results. Parameters **kwargs Additional keyword arguments for the AutoMLSearch instance. method available_models () [source] Give an overview of the available predefined models. Returns pd.DataFrame Information about the available predefined models . Columns include: - acronym: Model's acronym (used to call the model). - fullname: Complete name of the model. - estimator: The model's underlying estimator. - module: The estimator's module. - needs_scaling: Whether the model requires feature scaling. - accepts_sparse: Whether the model accepts sparse matrices. - supports_engines: List of engines supported by the model. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] Create a figure with multiple plots. This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Clear attributes from all models. Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the instance. The cleared attributes per model are: Prediction attributes Metric scores Shap values App instance Dashboard instance method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving . Deleted models are not removed from any active mlflow experiment . Parameters models: int, str, slice, sequence or None, default=None Names, positions or regex pattern of the models to delete. If None, all models are deleted. method distribution (distributions=None, columns=None) [source] Get statistics on column distributions. Compute the Kolmogorov-Smirnov test for various distributions against columns in the dataset. Only for numerical columns. Missing values are ignored. Tip Use the plot_distribution method to plot a column's distribution. Parameters distributions: str, sequence or None, default=None Names of the distributions in scipy.stats to get the statistics on. If None, a selection of the most common ones is used. columns: int, str, slice, sequence or None, default=None Names, positions or dtypes of the columns in the dataset to perform the test on. If None, select all numerical columns. Returns pd.DataFrame Statistic results with multiindex levels: - dist: Name of the distribution. - stat: Statistic results: - score: KS-test score. - p_value: Corresponding p-value. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get all models' scores for the provided metrics. Parameters metric: str, func, scorer, sequence or None, default=None Metric to calculate. If None, it returns an overview of the most common metrics per task. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: - The task is binary classification. - The model has a predict_proba method. - The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns pd.DataFrame Scores of the models. method export_pipeline (model=None, memory=None, verbose=None) [source] Export atom's pipeline. Optionally, you can add a model as final estimator. The returned pipeline is already fitted on the training set. Info The returned pipeline behaves similarly to sklearn's Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always returns pandas objects. Uses transformers that are only applied on the training set to fit the pipeline, not to make predictions. Parameters model: str or None, default=None Name of the model to add as a final estimator to the pipeline. If the model used automated feature scaling , the scaler is added before the model. If None, only the transformers are added. memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. - If None or False: No caching is performed. - If True: A default temp directory is used. - If str: Path to the caching directory. - If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns Pipeline Sklearn-like Pipeline object with all transformers in the current branch. method get_class_weight (dataset=\"train\") [source] Return class weights for a balanced dataset. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns dict Classes with the corresponding weights. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be used to transform only the target column. Parameters X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None Target column corresponding to X. - If None: y is ignored in the transformers. - If int: Position of the target column in X. - If str: Name of the target column in X. - Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level for the transformers. If None, it uses the transformer's own verbosity. Returns pd.DataFrame Original feature set. Only returned if provided. y: pd.Series Original target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters other: Runner Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method report (dataset=\"dataset\", n_rows=None, filename=None, **kwargs) [source] Create an extensive profile analysis report of the data. ATOM uses the pandas-profiling package for the analysis. The report is rendered directly in the notebook. The created ProfileReport instance can be accessed through the profile attribute. Warning This method can be slow for large datasets. Parameters dataset: str, default=\"dataset\" Data set to get the report from. n_rows: int or None, default=None Number of (randomly picked) rows to process. None to use all rows. filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ProfileReport instance. method reset () [source] Reset the instance to it's initial state. Deletes all branches and models. The dataset is also reset to its form after initialization. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method save_data (filename=\"auto\", dataset=\"dataset\") [source] Save the data in the current branch to a .csv file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. dataset: str, default=\"dataset\" Data set to save. method shrink (obj2cat=True, int2uint=False, dense2sparse=False, columns=None) [source] Converts the columns to the smallest possible matching dtype. Parameters obj2cat: bool, default=True Whether to convert object to category . Only if the number of categories would be less than 30% of the length of the column. int2uint: bool, default=False Whether to convert int to uint (unsigned integer). Only if the values in the column are strictly positive. dense2sparse: bool, default=False Whether to convert all features to sparse format. The value that is compressed is the most frequent value in the column. columns: int, str, slice, sequence or None, default=None Names, positions or dtypes of the columns in the dataset to shrink. If None, transform all columns. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's stacking instance. The model's acronyms can be used for the final_estimator parameter. method stats (_vb=-2) [source] Print basic information about the dataset. Tip For classification tasks, the count and balance of classes is shown, followed by the ratio (between parentheses) of the class with respect to the rest of the classes in the same data set, i.e. the class with the fewest samples is followed by (1.0) . This information can be used to quickly assess if the data set is unbalanced. Parameters _vb: int, default=-2 Internal parameter to always print if called by user. method status () [source] Get an overview of the branches and models. This method prints the same information as the __repr__ and also saves it to the logger. method transform (X=None, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None Target column corresponding to X. - If None: y is ignored in the transformers. - If int: Position of the target column in X. - If str: Name of the target column in X. - Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level for the transformers. If None, it uses the transformer's own verbosity. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's voting instance. Data cleaning The data cleaning methods can help you scale the data, handle missing values, categorical columns, outliers and unbalanced datasets. All attributes of the data cleaning classes are attached to atom after running. Read more in the user guide . Tip Use the report method to examine the data and help you determine suitable parameters for the data cleaning methods. balance Balance the number of rows per class in the target column. clean Applies standard data cleaning steps on the dataset. discretize Bin continuous data into intervals. encode Perform encoding of categorical features. impute Handle missing values in the dataset. normalize Transform the data to follow a Normal/Gaussian distribution. prune Prune outliers from the training set. scale Scale the data. method balance (strategy=\"adasyn\", **kwargs) [source] Balance the number of rows per class in the target column. When oversampling, the newly created samples have an increasing integer index for numerical indices, and an index of the form [estimator]_N for non-numerical indices, where N stands for the N-th sample in the data set. See the Balancer class for a description of the parameters. Note This transformation is only applied to the training set in order to maintain the original distribution of target classes in the test set. Tip Use atom's classes attribute for an overview of the target class distribution per data set. method clean (drop_types=None, strip_categorical=True, drop_duplicates=False, drop_missing_target=True, encode_target=True, **kwargs) [source] Applies standard data cleaning steps on the dataset. Use the parameters to choose which transformations to perform. The available steps are: Drop columns with specific data types. Strip categorical features from white spaces. Drop duplicate rows. Drop rows with missing values in the target column. Encode the target column (can't be True for regression tasks). See the Cleaner class for a description of the parameters. method discretize (strategy=\"quantile\", bins=5, labels=None, **kwargs) [source] Bin continuous data into intervals. For each feature, the bin edges are computed during fit and, together with the number of bins, they will define the intervals. Ignores numerical columns. See the Discretizer class for a description of the parameters. Tip Use the plot_distribution method to visualize a column's distribution and decide on the bins. method encode (strategy=\"LeaveOneOut\", max_onehot=10, ordinal=None, rare_to_value=None, value=\"rare\", **kwargs) [source] Perform encoding of categorical features. The encoding type depends on the number of classes in the column: If n_classes=2 or ordinal feature, use Ordinal-encoding. If 2 < n_classes <= max_onehot , use OneHot-encoding. If n_classes > max_onehot , use strategy -encoding. Missing values are propagated to the output column. Unknown classes encountered during transforming are imputed according to the selected strategy. Rare classes can be replaced with a value in order to prevent too high cardinality. See the Encoder class for a description of the parameters. Note This method only encodes the categorical features. It does not encode the target column! Use the clean method for that. Tip Use the categorical attribute for a list of the categorical features in the dataset. method impute (strat_num=\"drop\", strat_cat=\"drop\", max_nan_rows=None, max_nan_cols=None, **kwargs) [source] Handle missing values in the dataset. Impute or remove missing values according to the selected strategy. Also removes rows and columns with too many missing values. Use the missing attribute to customize what are considered \"missing values\". See the Imputer class for a description of the parameters. Tip Use the nans attribute to check the amount of missing values per column. method normalize (strategy=\"yeojohnson\", **kwargs) [source] Transform the data to follow a Normal/Gaussian distribution. This transformation is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Missing values are disregarded in fit and maintained in transform. Ignores categorical columns. See the Normalizer class for a description of the parameters. Tip Use the plot_distribution method to examine a column's distribution. method prune (strategy=\"zscore\", method=\"drop\", max_sigma=3, include_target=False, **kwargs) [source] Prune outliers from the training set. Replace or remove outliers. The definition of outlier depends on the selected strategy and can greatly differ from one another. Ignores categorical columns. See the Pruner class for a description of the parameters. Note This transformation is only applied to the training set in order to maintain the original distribution of samples in the test set. Tip Use the outliers attribute to check the number of outliers per column. method scale (strategy=\"standard\", **kwargs) [source] Scale the data. Apply one of sklearn's scalers. Ignores categorical columns. See the Scaler class for a description of the parameters. Tip Use the scaled attribute to check whether the dataset is scaled. NLP The Natural Language Processing (NLP) transformers help to convert raw text to meaningful numeric values, ready to be ingested by a model. All transformations are applied only on the column in the dataset called corpus . Read more in the user guide . textclean Applies standard text cleaning to the corpus. textnormalize Normalize the corpus. tokenize Tokenize the corpus. vectorize Vectorize the corpus. method textclean (decode=True, lower_case=True, drop_email=True, regex_email=None, drop_url=True, regex_url=None, drop_html=True, regex_html=None, drop_emoji=True, regex_emoji=None, drop_number=True, regex_number=None, drop_punctuation=True, **kwargs) [source] Applies standard text cleaning to the corpus. Transformations include normalizing characters and dropping noise from the text (emails, HTML tags, URLs, etc...). The transformations are applied on the column named corpus , in the same order the parameters are presented. If there is no column with that name, an exception is raised. See the TextCleaner class for a description of the parameters. method textnormalize (stopwords=True, custom_stopwords=None, stem=False, lemmatize=True, **kwargs) [source] Normalize the corpus. Convert words to a more uniform standard. The transformations are applied on the column named corpus , in the same order the parameters are presented. If there is no column with that name, an exception is raised. If the provided documents are strings, words are separated by spaces. See the TextNormalizer class for a description of the parameters. method tokenize (bigram_freq=None, trigram_freq=None, quadgram_freq=None, **kwargs) [source] Tokenize the corpus. Convert documents into sequences of words. Additionally, create n-grams (represented by words united with underscores, e.g. \"New_York\") based on their frequency in the corpus. The transformations are applied on the column named corpus . If there is no column with that name, an exception is raised. See the Tokenizer class for a description of the parameters. method vectorize (strategy=\"bow\", return_sparse=True, **kwargs) [source] Vectorize the corpus. Transform the corpus into meaningful vectors of numbers. The transformation is applied on the column named corpus . If there is no column with that name, an exception is raised. If strategy=\"bow\" or \"tfidf\", the transformed columns are named after the word they are embedding with the prefix corpus_ . If strategy=\"hashing\", the columns are named hash[N], where N stands for the n-th hashed column. See the Vectorizer class for a description of the parameters. Feature engineering To further pre-process the data, it's possible to extract features from datetime columns, create new non-linear features transforming the existing ones, group similar features or, if the dataset is too large, remove features. Read more in the user guide . feature_extraction Extract features from datetime columns. feature_generation Generate new features. feature_grouping Extract statistics from similar features. feature_selection Reduce the number of features in the data. method feature_extraction (features=['day', 'month', 'year'], fmt=None, encoding_type=\"ordinal\", drop_columns=True, **kwargs) [source] Extract features from datetime columns. Create new features extracting datetime elements (day, month, year, etc...) from the provided columns. Columns of dtype datetime64 are used as is. Categorical columns that can be successfully converted to a datetime format (less than 30% NaT values after conversion) are also used. See the FeatureExtractor class for a description of the parameters. method feature_generation (strategy=\"dfs\", n_features=None, operators=None, **kwargs) [source] Generate new features. Create new combinations of existing features to capture the non-linear relations between the original features. See the FeatureGenerator class for a description of the parameters. method feature_grouping (group, name=None, operators=None, drop_columns=True, **kwargs) [source] Extract statistics from similar features. Replace groups of features with related characteristics with new features that summarize statistical properties of te group. The statistical operators are calculated over every row of the group. The group names and features can be accessed through the groups method. See the FeatureGrouper class for a description of the parameters. method feature_selection (strategy=None, solver=None, n_features=None, min_repeated=2, max_repeated=1.0, max_correlation=1.0, **kwargs) [source] Reduce the number of features in the data. Apply feature selection or dimensionality reduction, either to improve the estimators' accuracy or to boost their performance on very high-dimensional datasets. Additionally, remove multicollinear and low variance features. See the FeatureSelector class for a description of the parameters. Note When strategy=\"univariate\" and solver=None, f_classif or f_regression is used as default solver. When strategy is \"sfs\", \"rfecv\" or any of the advanced strategies and no scoring is specified, atom's metric (if it exists) is used as scoring. Training The training methods are where the models are fitted to the data and their performance is evaluated against selected metric. There are three methods to call the three different training approaches. All relevant attributes and methods from the training classes are attached to atom for convenience. These include the errors, winner and results attributes, as well as the models , and the prediction and plotting methods. Read more in the user guide . run Train and evaluate the models in a direct fashion. successive_halving Fit the models in a successive halving fashion. train_sizing Train and evaluate the models in a train sizing fashion. method run (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, **kwargs) [source] Train and evaluate the models in a direct fashion. Contrary to successive_halving and train_sizing , the direct approach only iterates once over the models, using the full dataset. The following steps are applied to every model: Apply hyperparameter tuning (optional). Fit the model on the training set using the best combination of hyperparameters found. Evaluate the model on the test set. Train the model on various bootstrapped samples of the training set and evaluate again on the test set (optional). See the DirectClassifier or DirectRegressor class for a description of the parameters. method successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_runs=0, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, **kwargs) [source] Fit the models in a successive halving fashion. The successive halving technique is a bandit-based algorithm that fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason, it is recommended to only use this technique with similar models, e.g. only using tree-based models. The following steps are applied to every model (per iteration): Apply hyperparameter tuning (optional). Fit the model on the training set using the best combination of hyperparameters found. Evaluate the model on the test set. Train the model on various bootstrapped samples of the training set and evaluate again on the test set (optional). See the SuccessiveHalvingClassifier or SuccessiveHalvingRegressor class for a description of the parameters. method train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=5, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, **kwargs) [source] Train and evaluate the models in a train sizing fashion. When training models, there is usually a trade-off between model performance and computation time, that is regulated by the number of samples in the training set. This method can be used to create insights in this trade-off, and help determine the optimal size of the training set. The models are fitted multiple times, ever-increasing the number of samples in the training set. The following steps are applied to every model (per iteration): Apply hyperparameter tuning (optional). Fit the model on the training set using the best combination of hyperparameters found. Evaluate the model on the test set. Train the model on various bootstrapped samples of the training set and evaluate again on the test set (optional). See the TrainSizingClassifier or TrainSizingRegressor class for a description of the parameters.","title":"ATOMClassifier"},{"location":"API/ATOM/atomclassifier/#atomclassifier","text":"class atom.api. ATOMClassifier (*arrays, y=-1, index=False, shuffle=True, stratify=True, n_rows=1, test_size=0.2, holdout_size=None, n_jobs=1, device=\"cpu\", engine=\"sklearn\", verbose=0, warnings=False, logger=None, experiment=None, random_state=None) [source] Main class for binary and multiclass classification tasks. Apply all data transformations and model management provided by the package on a given dataset. Note that, contrary to sklearn's API, the instance contains the dataset on which to perform the analysis. Calling a method will automatically apply it on the dataset it contains. All data cleaning , feature engineering , model training , predicting , and plotting functionality can be accessed from an instacne of this class. Parameters *arrays: sequence of indexables Dataset containing features and target. Allowed formats are: X X, y train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) X, train, test: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str or sequence Target column corresponding to X. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. y: int, str or sequence, default=-1 Target column corresponding to X. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. This parameter is ignored if the target column is provided through arrays . index: bool, int, str or sequence, default=False Handle the index in the resulting dataframe. If False: Reset to RangeIndex . If True: Use the provided index. If int: Position of the column to use as index. If str: Name of the column to use as index. If sequence: Array with shape=(n_samples,) to use as index. test_size: int or float, default=0.2 If <=1: Fraction of the dataset to include in the test set. If >1: Number of rows to include in the test set. This parameter is ignored if the test set is provided through arrays . holdout_size: int, float or None, default=None If None: No holdout data set is kept apart. If <=1: Fraction of the dataset to include in the holdout set. If >1: Number of rows to include in the holdout set. This parameter is ignored if the holdout set is provided through arrays . shuffle: bool, default=True Whether to shuffle the dataset before splitting the train and test set. Be aware that not shuffling the dataset can cause an unequal distribution of target classes over the sets. stratify: bool, int, str or sequence, default=True Handle stratification of the target classes over the data sets. If False: The data is split randomly. If True: The data is stratified over the target column. Else: Name or position of the columns to stratify by. The columns can't contain NaN values. This parameter is ignored if shuffle=False or if the test set is provided through arrays . n_rows: int or float, default=1 Subsample of the dataset to use. The default value selects all the rows. If <=1: Fraction of the dataset to select. If >1: Exact number of rows to select. Only if arrays is X or X, y. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If <-1: Use number of cores - 1 + n_jobs . device: str, default=\"cpu\" Device on which to train the estimators. Use any string that follows the SYCL_DEVICE_FILTER filter selector, e.g. device=\"gpu\" to use the GPU. Read more in the user guide . engine: str, default=\"sklearn\" Execution engine to use for the estimators. Refer to the user guide for an explanation regarding every choice. Choose from: \"sklearn\" (only if device=\"cpu\") \"sklearnex\" \"cuml\" (only if device=\"gpu\") verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of python's warnings filters . Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic name. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState used by np.random . See Also ATOMRegressor Main class for regression tasks.","title":"ATOMClassifier"},{"location":"API/ATOM/atomclassifier/#example","text":">>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> # Initialize atom >>> atom = ATOMClassifier ( X , y , logger = \"auto\" , n_jobs = 2 , verbose = 2 ) << ================== ATOM ================== >> Algorithm task: binary classification. Parallel processing with 2 cores. Dataset stats ==================== >> Shape: (569, 31) Memory: 138.96 kB Scaled: False Outlier values: 160 (1.1%) ------------------------------------- Train set size: 456 Test set size: 113 ------------------------------------- | | dataset | train | test | | - | ----------- | ----------- | ----------- | | 0 | 212 (1.0) | 170 (1.0) | 42 (1.0) | | 1 | 357 (1.7) | 286 (1.7) | 71 (1.7) | >>> # Apply data cleaning and feature engineering methods >>> atom . balance ( strategy = \"smote\" ) Oversampling with SMOTE... --> Adding 116 samples to class 0. >>> atom . feature_selection ( strategy = \"rfecv\" , solver = \"xgb\" , n_features = 22 ) Fitting FeatureSelector... Performing feature selection ... --> RFECV selected 26 features from the dataset. --> Dropping feature mean perimeter (rank 4). --> Dropping feature mean symmetry (rank 3). --> Dropping feature perimeter error (rank 2). --> Dropping feature worst compactness (rank 5). >>> # Train models >>> atom . run ( ... models = [ \"LR\" , \"RF\" , \"XGB\" ], ... metric = \"precision\" , ... n_bootstrap = 4 , ... ) Training ========================= >> Models: LR, RF, XGB Metric: precision Results for Logistic Regression: Fit --------------------------------------------- Train evaluation --> precision: 0.9895 Test evaluation --> precision: 0.9467 Time elapsed: 0.028s ------------------------------------------------- Total time: 0.028s Results for Random Forest: Fit --------------------------------------------- Train evaluation --> precision: 1.0 Test evaluation --> precision: 0.9221 Time elapsed: 0.181s ------------------------------------------------- Total time: 0.181s Results for XGBoost: Fit --------------------------------------------- Train evaluation --> precision: 1.0 Test evaluation --> precision: 0.9091 Time elapsed: 0.124s ------------------------------------------------- Total time: 0.124s Final results ==================== >> Duration: 0.333s ------------------------------------- Logistic Regression --> precision: 0.9467 ! Random Forest --> precision: 0.9221 XGBoost --> precision: 0.9091 >>> # Analyze the results >>> atom . plot_results ()","title":"Example"},{"location":"API/ATOM/atomclassifier/#magic-methods","text":"The class contains some magic methods to help you access some of its elements faster. Note that methods that apply on the pipeline can return different results per branch. __repr__: Prints an overview of atom's branches, models, metric and errors. __len__: Returns the length of the dataset. __iter__: Iterate over the pipeline's transformers. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a branch, model, column or subset of the dataset.","title":"Magic methods"},{"location":"API/ATOM/atomclassifier/#attributes","text":"","title":"Attributes"},{"location":"API/ATOM/atomclassifier/#data-attributes","text":"The data attributes are used to access the dataset and its properties. Updating the dataset will automatically update the response of these attributes accordingly. Attributes pipeline: pd.Series Transformers fitted on the data. Use this attribute only to access the individual instances. To visualize the pipeline, use the plot_pipeline method. mapping: dict Encoded values and their respective mapped values. The column name is the key to its mapping dictionary. Only for columns mapped to a single column (e.g. Ordinal, Leave-one-out, etc...). dataset: pd.DataFrame Complete data set. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Features of the training set. y_train: pd.Series Target column of the training set. X_test: pd.DataFrame Features of the test set. y_test: pd.Series Target column of the test set. shape: Tuple[int, int] Shape of the dataset (n_rows, n_cols). columns: pd.Series Name of all the columns. n_columns: int Number of columns. features: pd.Series Name of the features. n_features: int Number of features. target: str Name of the target column. scaled: bool Whether the feature set is scaled. A data set is considered scaled when it has mean=0 and std=1, or when atom has a scaler in the pipeline. Returns None for sparse datasets . duplicates: pd.Series Number of duplicate rows in the dataset. nans: pd.Series Columns with the number of missing values in them. n_nans: int Number of samples containing missing values. numerical: pd.Series Names of the numerical features in the dataset. n_numerical: int Number of numerical features in the dataset. categorical: pd.Series Names of the categorical features in the dataset. n_categorical: int Number of categorical features in the dataset. outliers: pd.Series Columns in training set with amount of outlier values. n_outliers: int Number of samples in the training set containing outliers. classes: pd.DataFrame Distribution of target classes per data set. n_classes: int Number of classes in the target column.","title":"Data attributes"},{"location":"API/ATOM/atomclassifier/#utility-attributes","text":"The utility attributes are used to access information about the models in the instance after training . Attributes models: str or list Return the names of all models. metric: str or list Return the name of the metric. errors: dict Return the errors encountered during model training. winners: list Return the model names ordered by performance. winner: models Return the best performing model.","title":"Utility attributes"},{"location":"API/ATOM/atomclassifier/#tracking-attributes","text":"The tracking attributes are used to customize what elements of the experiment are tracked. Read more in the user guide . Attributes log_bo: bool Whether to track every trial of the hyperparameter tuning. log_model: bool Whether to save the model's estimator after fitting. log_plots: bool Whether to save plots as artifacts. log_data: bool Whether to save the train and test sets. log_pipeline: bool Whether to save the model's pipeline.","title":"Tracking attributes"},{"location":"API/ATOM/atomclassifier/#plot-attributes","text":"The plot attributes are used to customize the plot's aesthetics. Read more in the user guide . Attributes style: str Plotting style . palette: str Color palette . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes.","title":"Plot attributes"},{"location":"API/ATOM/atomclassifier/#utility-methods","text":"The class contains a variety of utility methods to handle the data and manage the pipeline. add Add a transformer to the pipeline. apply Apply a function to the dataset. automl Search for an optimized pipeline in an automated fashion. available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. distribution Get statistics on column distributions. evaluate Get all models' scores for the provided metrics. export_pipeline Export atom's pipeline. get_class_weight Return class weights for a balanced dataset. inverse_transform Inversely transform new data through the pipeline. log Print message and save to log file. merge Merge another instance of the same class into this one. report Create an extensive profile analysis report of the data. reset Reset the instance to it's initial state. reset_aesthetics Reset the plot aesthetics to their default values. save Save the instance to a pickle file. save_data Save the data in the current branch to a .csv file. shrink Converts the columns to the smallest possible matching dtype. stacking Add a Stacking model to the pipeline. stats Print basic information about the dataset. status Get an overview of the branches and models. transform Transform new data through the pipeline. voting Add a Voting model to the pipeline. method add (transformer, columns=None, train_only=False, **fit_params) [source] Add a transformer to the pipeline. If the transformer is not fitted, it is fitted on the complete training set. Afterwards, the data set is transformed and the estimator is added to atom's pipeline. If the estimator is a sklearn Pipeline, every estimator is merged independently with atom. Warning The transformer should have fit and/or transform methods with arguments X (accepting a dataframe-like object of shape=(n_samples, n_features)) and/or y (accepting a sequence of shape=(n_samples,)). The transform method should return a feature set as a dataframe-like object of shape=(n_samples, n_features) and/or a target column as a sequence of shape=(n_samples,). Note If the transform method doesn't return a dataframe: The column naming happens as follows. If the transformer has a get_feature_names or get_feature_names_out method, it is used. If not, and it returns the same number of columns, the names are kept equal. If the number of columns change, old columns will keep their name (as long as the column is unchanged) and new columns will receive the name x[N-1] , where N stands for the n-th feature. This means that a transformer should only transform, add or drop columns, not combinations of these. The index remains the same as before the transformation. This means that the transformer should not add, remove or shuffle rows unless it returns a dataframe. Note If the transformer has a n_jobs and/or random_state parameter that is left to its default value, it adopts atom's value. Parameters transformer: Transformer Estimator to add to the pipeline. Should implement a transform method. columns: int, str, slice, sequence or None, default=None Names, indices or dtypes of the columns in the dataset to transform. If None, transform all columns. Add ! in front of a name or dtype to exclude that column, e.g. atom.add(Transformer(), columns=\"!Location\") transforms all columns except Location`. You can either include or exclude columns, not combinations of these. The target column is always included if required by the transformer. train_only: bool, default=False Whether to apply the estimator only on the training set or on the complete dataset. Note that if True, the transformation is skipped when making predictions on new data. **fit_params Additional keyword arguments for the transformer's fit method. method apply (func, inverse_func=None, kw_args=None, inv_kw_args=None, **kwargs) [source] Apply a function to the dataset. The function should have signature func(dataset, **kw_args) -> dataset . This method is useful for stateless transformations such as taking the log, doing custom scaling, etc... Note This approach is preferred over changing the dataset directly through the property's @setter since the transformation is stored in the pipeline. Tip Use atom . apply ( lambda df : df . drop ( \"column_name\" , axis = 1 )) to store the removal of columns in the pipeline. Parameters func: callable Function to apply. inverse_func: callable or None, default=None Inverse function of func . If None, the inverse_transform method returns the input unchanged. kw_args: dict or None, default=None Additional keyword arguments for the function. inv_kw_args: dict or None, default=None Additional keyword arguments for the inverse function. method automl (**kwargs) [source] Search for an optimized pipeline in an automated fashion. Automated machine learning (AutoML) automates the selection, composition and parameterization of machine learning pipelines. Automating the machine learning often provides faster, more accurate outputs than hand-coded algorithms. ATOM uses the evalML package for AutoML optimization. The resulting transformers and final estimator are merged with atom's pipeline (check the pipeline and models attributes after the method finishes running). The created AutoMLSearch instance can be accessed through the evalml attribute. Warning AutoML algorithms aren't intended to run for only a few minutes. The method may need a very long time to achieve optimal results. Parameters **kwargs Additional keyword arguments for the AutoMLSearch instance. method available_models () [source] Give an overview of the available predefined models. Returns pd.DataFrame Information about the available predefined models . Columns include: - acronym: Model's acronym (used to call the model). - fullname: Complete name of the model. - estimator: The model's underlying estimator. - module: The estimator's module. - needs_scaling: Whether the model requires feature scaling. - accepts_sparse: Whether the model accepts sparse matrices. - supports_engines: List of engines supported by the model. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] Create a figure with multiple plots. This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Clear attributes from all models. Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the instance. The cleared attributes per model are: Prediction attributes Metric scores Shap values App instance Dashboard instance method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving . Deleted models are not removed from any active mlflow experiment . Parameters models: int, str, slice, sequence or None, default=None Names, positions or regex pattern of the models to delete. If None, all models are deleted. method distribution (distributions=None, columns=None) [source] Get statistics on column distributions. Compute the Kolmogorov-Smirnov test for various distributions against columns in the dataset. Only for numerical columns. Missing values are ignored. Tip Use the plot_distribution method to plot a column's distribution. Parameters distributions: str, sequence or None, default=None Names of the distributions in scipy.stats to get the statistics on. If None, a selection of the most common ones is used. columns: int, str, slice, sequence or None, default=None Names, positions or dtypes of the columns in the dataset to perform the test on. If None, select all numerical columns. Returns pd.DataFrame Statistic results with multiindex levels: - dist: Name of the distribution. - stat: Statistic results: - score: KS-test score. - p_value: Corresponding p-value. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get all models' scores for the provided metrics. Parameters metric: str, func, scorer, sequence or None, default=None Metric to calculate. If None, it returns an overview of the most common metrics per task. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: - The task is binary classification. - The model has a predict_proba method. - The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns pd.DataFrame Scores of the models. method export_pipeline (model=None, memory=None, verbose=None) [source] Export atom's pipeline. Optionally, you can add a model as final estimator. The returned pipeline is already fitted on the training set. Info The returned pipeline behaves similarly to sklearn's Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always returns pandas objects. Uses transformers that are only applied on the training set to fit the pipeline, not to make predictions. Parameters model: str or None, default=None Name of the model to add as a final estimator to the pipeline. If the model used automated feature scaling , the scaler is added before the model. If None, only the transformers are added. memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. - If None or False: No caching is performed. - If True: A default temp directory is used. - If str: Path to the caching directory. - If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns Pipeline Sklearn-like Pipeline object with all transformers in the current branch. method get_class_weight (dataset=\"train\") [source] Return class weights for a balanced dataset. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns dict Classes with the corresponding weights. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be used to transform only the target column. Parameters X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None Target column corresponding to X. - If None: y is ignored in the transformers. - If int: Position of the target column in X. - If str: Name of the target column in X. - Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level for the transformers. If None, it uses the transformer's own verbosity. Returns pd.DataFrame Original feature set. Only returned if provided. y: pd.Series Original target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters other: Runner Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method report (dataset=\"dataset\", n_rows=None, filename=None, **kwargs) [source] Create an extensive profile analysis report of the data. ATOM uses the pandas-profiling package for the analysis. The report is rendered directly in the notebook. The created ProfileReport instance can be accessed through the profile attribute. Warning This method can be slow for large datasets. Parameters dataset: str, default=\"dataset\" Data set to get the report from. n_rows: int or None, default=None Number of (randomly picked) rows to process. None to use all rows. filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ProfileReport instance. method reset () [source] Reset the instance to it's initial state. Deletes all branches and models. The dataset is also reset to its form after initialization. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method save_data (filename=\"auto\", dataset=\"dataset\") [source] Save the data in the current branch to a .csv file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. dataset: str, default=\"dataset\" Data set to save. method shrink (obj2cat=True, int2uint=False, dense2sparse=False, columns=None) [source] Converts the columns to the smallest possible matching dtype. Parameters obj2cat: bool, default=True Whether to convert object to category . Only if the number of categories would be less than 30% of the length of the column. int2uint: bool, default=False Whether to convert int to uint (unsigned integer). Only if the values in the column are strictly positive. dense2sparse: bool, default=False Whether to convert all features to sparse format. The value that is compressed is the most frequent value in the column. columns: int, str, slice, sequence or None, default=None Names, positions or dtypes of the columns in the dataset to shrink. If None, transform all columns. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's stacking instance. The model's acronyms can be used for the final_estimator parameter. method stats (_vb=-2) [source] Print basic information about the dataset. Tip For classification tasks, the count and balance of classes is shown, followed by the ratio (between parentheses) of the class with respect to the rest of the classes in the same data set, i.e. the class with the fewest samples is followed by (1.0) . This information can be used to quickly assess if the data set is unbalanced. Parameters _vb: int, default=-2 Internal parameter to always print if called by user. method status () [source] Get an overview of the branches and models. This method prints the same information as the __repr__ and also saves it to the logger. method transform (X=None, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None Target column corresponding to X. - If None: y is ignored in the transformers. - If int: Position of the target column in X. - If str: Name of the target column in X. - Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level for the transformers. If None, it uses the transformer's own verbosity. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's voting instance.","title":"Utility methods"},{"location":"API/ATOM/atomclassifier/#data-cleaning","text":"The data cleaning methods can help you scale the data, handle missing values, categorical columns, outliers and unbalanced datasets. All attributes of the data cleaning classes are attached to atom after running. Read more in the user guide . Tip Use the report method to examine the data and help you determine suitable parameters for the data cleaning methods. balance Balance the number of rows per class in the target column. clean Applies standard data cleaning steps on the dataset. discretize Bin continuous data into intervals. encode Perform encoding of categorical features. impute Handle missing values in the dataset. normalize Transform the data to follow a Normal/Gaussian distribution. prune Prune outliers from the training set. scale Scale the data. method balance (strategy=\"adasyn\", **kwargs) [source] Balance the number of rows per class in the target column. When oversampling, the newly created samples have an increasing integer index for numerical indices, and an index of the form [estimator]_N for non-numerical indices, where N stands for the N-th sample in the data set. See the Balancer class for a description of the parameters. Note This transformation is only applied to the training set in order to maintain the original distribution of target classes in the test set. Tip Use atom's classes attribute for an overview of the target class distribution per data set. method clean (drop_types=None, strip_categorical=True, drop_duplicates=False, drop_missing_target=True, encode_target=True, **kwargs) [source] Applies standard data cleaning steps on the dataset. Use the parameters to choose which transformations to perform. The available steps are: Drop columns with specific data types. Strip categorical features from white spaces. Drop duplicate rows. Drop rows with missing values in the target column. Encode the target column (can't be True for regression tasks). See the Cleaner class for a description of the parameters. method discretize (strategy=\"quantile\", bins=5, labels=None, **kwargs) [source] Bin continuous data into intervals. For each feature, the bin edges are computed during fit and, together with the number of bins, they will define the intervals. Ignores numerical columns. See the Discretizer class for a description of the parameters. Tip Use the plot_distribution method to visualize a column's distribution and decide on the bins. method encode (strategy=\"LeaveOneOut\", max_onehot=10, ordinal=None, rare_to_value=None, value=\"rare\", **kwargs) [source] Perform encoding of categorical features. The encoding type depends on the number of classes in the column: If n_classes=2 or ordinal feature, use Ordinal-encoding. If 2 < n_classes <= max_onehot , use OneHot-encoding. If n_classes > max_onehot , use strategy -encoding. Missing values are propagated to the output column. Unknown classes encountered during transforming are imputed according to the selected strategy. Rare classes can be replaced with a value in order to prevent too high cardinality. See the Encoder class for a description of the parameters. Note This method only encodes the categorical features. It does not encode the target column! Use the clean method for that. Tip Use the categorical attribute for a list of the categorical features in the dataset. method impute (strat_num=\"drop\", strat_cat=\"drop\", max_nan_rows=None, max_nan_cols=None, **kwargs) [source] Handle missing values in the dataset. Impute or remove missing values according to the selected strategy. Also removes rows and columns with too many missing values. Use the missing attribute to customize what are considered \"missing values\". See the Imputer class for a description of the parameters. Tip Use the nans attribute to check the amount of missing values per column. method normalize (strategy=\"yeojohnson\", **kwargs) [source] Transform the data to follow a Normal/Gaussian distribution. This transformation is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Missing values are disregarded in fit and maintained in transform. Ignores categorical columns. See the Normalizer class for a description of the parameters. Tip Use the plot_distribution method to examine a column's distribution. method prune (strategy=\"zscore\", method=\"drop\", max_sigma=3, include_target=False, **kwargs) [source] Prune outliers from the training set. Replace or remove outliers. The definition of outlier depends on the selected strategy and can greatly differ from one another. Ignores categorical columns. See the Pruner class for a description of the parameters. Note This transformation is only applied to the training set in order to maintain the original distribution of samples in the test set. Tip Use the outliers attribute to check the number of outliers per column. method scale (strategy=\"standard\", **kwargs) [source] Scale the data. Apply one of sklearn's scalers. Ignores categorical columns. See the Scaler class for a description of the parameters. Tip Use the scaled attribute to check whether the dataset is scaled.","title":"Data cleaning"},{"location":"API/ATOM/atomclassifier/#nlp","text":"The Natural Language Processing (NLP) transformers help to convert raw text to meaningful numeric values, ready to be ingested by a model. All transformations are applied only on the column in the dataset called corpus . Read more in the user guide . textclean Applies standard text cleaning to the corpus. textnormalize Normalize the corpus. tokenize Tokenize the corpus. vectorize Vectorize the corpus. method textclean (decode=True, lower_case=True, drop_email=True, regex_email=None, drop_url=True, regex_url=None, drop_html=True, regex_html=None, drop_emoji=True, regex_emoji=None, drop_number=True, regex_number=None, drop_punctuation=True, **kwargs) [source] Applies standard text cleaning to the corpus. Transformations include normalizing characters and dropping noise from the text (emails, HTML tags, URLs, etc...). The transformations are applied on the column named corpus , in the same order the parameters are presented. If there is no column with that name, an exception is raised. See the TextCleaner class for a description of the parameters. method textnormalize (stopwords=True, custom_stopwords=None, stem=False, lemmatize=True, **kwargs) [source] Normalize the corpus. Convert words to a more uniform standard. The transformations are applied on the column named corpus , in the same order the parameters are presented. If there is no column with that name, an exception is raised. If the provided documents are strings, words are separated by spaces. See the TextNormalizer class for a description of the parameters. method tokenize (bigram_freq=None, trigram_freq=None, quadgram_freq=None, **kwargs) [source] Tokenize the corpus. Convert documents into sequences of words. Additionally, create n-grams (represented by words united with underscores, e.g. \"New_York\") based on their frequency in the corpus. The transformations are applied on the column named corpus . If there is no column with that name, an exception is raised. See the Tokenizer class for a description of the parameters. method vectorize (strategy=\"bow\", return_sparse=True, **kwargs) [source] Vectorize the corpus. Transform the corpus into meaningful vectors of numbers. The transformation is applied on the column named corpus . If there is no column with that name, an exception is raised. If strategy=\"bow\" or \"tfidf\", the transformed columns are named after the word they are embedding with the prefix corpus_ . If strategy=\"hashing\", the columns are named hash[N], where N stands for the n-th hashed column. See the Vectorizer class for a description of the parameters.","title":"NLP"},{"location":"API/ATOM/atomclassifier/#feature-engineering","text":"To further pre-process the data, it's possible to extract features from datetime columns, create new non-linear features transforming the existing ones, group similar features or, if the dataset is too large, remove features. Read more in the user guide . feature_extraction Extract features from datetime columns. feature_generation Generate new features. feature_grouping Extract statistics from similar features. feature_selection Reduce the number of features in the data. method feature_extraction (features=['day', 'month', 'year'], fmt=None, encoding_type=\"ordinal\", drop_columns=True, **kwargs) [source] Extract features from datetime columns. Create new features extracting datetime elements (day, month, year, etc...) from the provided columns. Columns of dtype datetime64 are used as is. Categorical columns that can be successfully converted to a datetime format (less than 30% NaT values after conversion) are also used. See the FeatureExtractor class for a description of the parameters. method feature_generation (strategy=\"dfs\", n_features=None, operators=None, **kwargs) [source] Generate new features. Create new combinations of existing features to capture the non-linear relations between the original features. See the FeatureGenerator class for a description of the parameters. method feature_grouping (group, name=None, operators=None, drop_columns=True, **kwargs) [source] Extract statistics from similar features. Replace groups of features with related characteristics with new features that summarize statistical properties of te group. The statistical operators are calculated over every row of the group. The group names and features can be accessed through the groups method. See the FeatureGrouper class for a description of the parameters. method feature_selection (strategy=None, solver=None, n_features=None, min_repeated=2, max_repeated=1.0, max_correlation=1.0, **kwargs) [source] Reduce the number of features in the data. Apply feature selection or dimensionality reduction, either to improve the estimators' accuracy or to boost their performance on very high-dimensional datasets. Additionally, remove multicollinear and low variance features. See the FeatureSelector class for a description of the parameters. Note When strategy=\"univariate\" and solver=None, f_classif or f_regression is used as default solver. When strategy is \"sfs\", \"rfecv\" or any of the advanced strategies and no scoring is specified, atom's metric (if it exists) is used as scoring.","title":"Feature engineering"},{"location":"API/ATOM/atomclassifier/#training","text":"The training methods are where the models are fitted to the data and their performance is evaluated against selected metric. There are three methods to call the three different training approaches. All relevant attributes and methods from the training classes are attached to atom for convenience. These include the errors, winner and results attributes, as well as the models , and the prediction and plotting methods. Read more in the user guide . run Train and evaluate the models in a direct fashion. successive_halving Fit the models in a successive halving fashion. train_sizing Train and evaluate the models in a train sizing fashion. method run (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, **kwargs) [source] Train and evaluate the models in a direct fashion. Contrary to successive_halving and train_sizing , the direct approach only iterates once over the models, using the full dataset. The following steps are applied to every model: Apply hyperparameter tuning (optional). Fit the model on the training set using the best combination of hyperparameters found. Evaluate the model on the test set. Train the model on various bootstrapped samples of the training set and evaluate again on the test set (optional). See the DirectClassifier or DirectRegressor class for a description of the parameters. method successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_runs=0, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, **kwargs) [source] Fit the models in a successive halving fashion. The successive halving technique is a bandit-based algorithm that fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason, it is recommended to only use this technique with similar models, e.g. only using tree-based models. The following steps are applied to every model (per iteration): Apply hyperparameter tuning (optional). Fit the model on the training set using the best combination of hyperparameters found. Evaluate the model on the test set. Train the model on various bootstrapped samples of the training set and evaluate again on the test set (optional). See the SuccessiveHalvingClassifier or SuccessiveHalvingRegressor class for a description of the parameters. method train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=5, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, **kwargs) [source] Train and evaluate the models in a train sizing fashion. When training models, there is usually a trade-off between model performance and computation time, that is regulated by the number of samples in the training set. This method can be used to create insights in this trade-off, and help determine the optimal size of the training set. The models are fitted multiple times, ever-increasing the number of samples in the training set. The following steps are applied to every model (per iteration): Apply hyperparameter tuning (optional). Fit the model on the training set using the best combination of hyperparameters found. Evaluate the model on the test set. Train the model on various bootstrapped samples of the training set and evaluate again on the test set (optional). See the TrainSizingClassifier or TrainSizingRegressor class for a description of the parameters.","title":"Training"},{"location":"API/ATOM/atomloader/","text":"ATOMLoader function atom.api. ATOMLoader (filename, data=None, transform_data=True, verbose=None) [source] Load a class instance from a pickle file. If the file is an atom instance that was saved using save_data=False , it's possible to load new data into it and apply all data transformations. Parameters filename: str Name of the pickle file to load. data: sequence of indexables or None, default=None Original dataset. Only use this parameter if the file is an atom instance that was saved using save_data=False . Allowed formats are: X X, y train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) X, train, test: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str or sequence Target column corresponding to X. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. transform_data: bool, default=True If False, the data is left as provided. If True, it is transformed through all the steps in the instance's pipeline. This parameter is ignored if the loaded pickle is not an atom instance. verbose: int or None, default=None Verbosity level of the transformations applied on the new data. If None, use the verbosity from the loaded instance. This parameter is ignored if transform_data=False . Returns class instance Unpickled instance. Example >>> from atom import ATOMClassifier , ATOMLoader >>> from sklearn.datasets import load_breast_cancer >>> atom = ATOMClassifier ( X , y ) >>> atom . scale () >>> atom . run ([ \"LR\" , \"RF\" , \"SGD\" ], metric = \"AP\" ) >>> atom . save ( \"atom\" , save_data = False ) # Save atom to a pickle file # Load the class and add the data to the new instance >>> atom_2 = ATOMLoader ( \"atom\" , data = ( X , y ), verbose = 2 ) Transforming data for branch master: Scaling features... ATOMClassifier successfully loaded. >>> print ( atom_2 . results ) metric_train metric_test time_fit time LR 0.998179 0.998570 0.016s 0.016s RF 1.000000 0.995568 0.141s 0.141s SGD 0.998773 0.994313 0.016s 0.016s","title":"ATOMLoader"},{"location":"API/ATOM/atomloader/#atomloader","text":"function atom.api. ATOMLoader (filename, data=None, transform_data=True, verbose=None) [source] Load a class instance from a pickle file. If the file is an atom instance that was saved using save_data=False , it's possible to load new data into it and apply all data transformations. Parameters filename: str Name of the pickle file to load. data: sequence of indexables or None, default=None Original dataset. Only use this parameter if the file is an atom instance that was saved using save_data=False . Allowed formats are: X X, y train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) X, train, test: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str or sequence Target column corresponding to X. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. transform_data: bool, default=True If False, the data is left as provided. If True, it is transformed through all the steps in the instance's pipeline. This parameter is ignored if the loaded pickle is not an atom instance. verbose: int or None, default=None Verbosity level of the transformations applied on the new data. If None, use the verbosity from the loaded instance. This parameter is ignored if transform_data=False . Returns class instance Unpickled instance.","title":"ATOMLoader"},{"location":"API/ATOM/atomloader/#example","text":">>> from atom import ATOMClassifier , ATOMLoader >>> from sklearn.datasets import load_breast_cancer >>> atom = ATOMClassifier ( X , y ) >>> atom . scale () >>> atom . run ([ \"LR\" , \"RF\" , \"SGD\" ], metric = \"AP\" ) >>> atom . save ( \"atom\" , save_data = False ) # Save atom to a pickle file # Load the class and add the data to the new instance >>> atom_2 = ATOMLoader ( \"atom\" , data = ( X , y ), verbose = 2 ) Transforming data for branch master: Scaling features... ATOMClassifier successfully loaded. >>> print ( atom_2 . results ) metric_train metric_test time_fit time LR 0.998179 0.998570 0.016s 0.016s RF 1.000000 0.995568 0.141s 0.141s SGD 0.998773 0.994313 0.016s 0.016s","title":"Example"},{"location":"API/ATOM/atommodel/","text":"ATOMModel function atom.api. ATOMModel (estimator, acronym=None, fullname=None, needs_scaling=False) [source] Convert an estimator to a model that can be ingested by atom. This function adds the relevant attributes to the estimator so that they can be used by atom. Note that only estimators that follow sklearn's API are compatible. Read more about using custom models in the user guide . Parameters estimator: Predictor Custom estimator. Should implement a fit and predict method. acronym: str or None, default=None Model's acronym. Used to call the model from atom. If None, the capital letters in the estimator's __name__ are used (only if two or more, else it uses the entire __name__). fullname: str or None, default=None Full model's name. If None, the estimator's name is used. needs_scaling: bool, default=False Whether the model needs scaled features. Returns estimator Clone of the provided estimator with custom attributes. Example >>> from atom import ATOMRegressor , ATOMModel >>> from sklearn.linear_model import RANSACRegressor >>> ransac = ATOMModel ( ... estimator = RANSACRegressor (), ... acronym = \"RANSAC\" , ... fullname = \"Random Sample Consensus\" , ... needs_scaling = False , ... ) >>> atom = ATOMRegressor ( X , y ) >>> atom . run ( ransac , verbose = 2 ) Training ========================= >> Models: RANSAC Metric: r2 Results for Random Sample Consensus: Fit --------------------------------------------- Train evaluation --> r2: -2.1784 Test evaluation --> r2: -9.4592 Time elapsed: 0.072s ------------------------------------------------- Total time: 0.072s Final results ==================== >> Duration: 0.072s ------------------------------------- Random Sample Consensus --> r2: -9.4592 ~","title":"ATOMModel"},{"location":"API/ATOM/atommodel/#atommodel","text":"function atom.api. ATOMModel (estimator, acronym=None, fullname=None, needs_scaling=False) [source] Convert an estimator to a model that can be ingested by atom. This function adds the relevant attributes to the estimator so that they can be used by atom. Note that only estimators that follow sklearn's API are compatible. Read more about using custom models in the user guide . Parameters estimator: Predictor Custom estimator. Should implement a fit and predict method. acronym: str or None, default=None Model's acronym. Used to call the model from atom. If None, the capital letters in the estimator's __name__ are used (only if two or more, else it uses the entire __name__). fullname: str or None, default=None Full model's name. If None, the estimator's name is used. needs_scaling: bool, default=False Whether the model needs scaled features. Returns estimator Clone of the provided estimator with custom attributes.","title":"ATOMModel"},{"location":"API/ATOM/atommodel/#example","text":">>> from atom import ATOMRegressor , ATOMModel >>> from sklearn.linear_model import RANSACRegressor >>> ransac = ATOMModel ( ... estimator = RANSACRegressor (), ... acronym = \"RANSAC\" , ... fullname = \"Random Sample Consensus\" , ... needs_scaling = False , ... ) >>> atom = ATOMRegressor ( X , y ) >>> atom . run ( ransac , verbose = 2 ) Training ========================= >> Models: RANSAC Metric: r2 Results for Random Sample Consensus: Fit --------------------------------------------- Train evaluation --> r2: -2.1784 Test evaluation --> r2: -9.4592 Time elapsed: 0.072s ------------------------------------------------- Total time: 0.072s Final results ==================== >> Duration: 0.072s ------------------------------------- Random Sample Consensus --> r2: -9.4592 ~","title":"Example"},{"location":"API/ATOM/atomregressor/","text":"ATOMRegressor class atom.api. ATOMRegressor (*arrays, y=-1, index=False, shuffle=True, n_rows=1, test_size=0.2, holdout_size=None, n_jobs=1, device=\"cpu\", engine=\"sklearn\", verbose=0, warnings=False, logger=None, experiment=None, random_state=None) [source] Main class for regression tasks. Apply all data transformations and model management provided by the package on a given dataset. Note that, contrary to sklearn's API, the instance contains the dataset on which to perform the analysis. Calling a method will automatically apply it on the dataset it contains. All data cleaning , feature engineering , model training , predicting , and plotting functionality can be accessed from an instacne of this class. Parameters *arrays: sequence of indexables Dataset containing features and target. Allowed formats are: X X, y train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) X, train, test: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str or sequence Target column corresponding to X. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. y: int, str or sequence, default=-1 Target column corresponding to X. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. This parameter is ignored if the target column is provided through arrays . index: bool, int, str or sequence, default=False Handle the index in the resulting dataframe. If False: Reset to RangeIndex . If True: Use the provided index. If int: Position of the column to use as index. If str: Name of the column to use as index. If sequence: Array with shape=(n_samples,) to use as index. test_size: int or float, default=0.2 If <=1: Fraction of the dataset to include in the test set. If >1: Number of rows to include in the test set. This parameter is ignored if the test set is provided through arrays . holdout_size: int, float or None, default=None If None: No holdout data set is kept apart. If <=1: Fraction of the dataset to include in the holdout set. If >1: Number of rows to include in the holdout set. This parameter is ignored if the holdout set is provided through arrays . shuffle: bool, default=True Whether to shuffle the dataset before splitting the train and test set. Be aware that not shuffling the dataset can cause an unequal distribution of target classes over the sets. n_rows: int or float, default=1 Subsample of the dataset to use. The default value selects all the rows. If <=1: Fraction of the dataset to select. If >1: Exact number of rows to select. Only if arrays is X or X, y. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If <-1: Use number of cores - 1 + n_jobs . device: str, default=\"cpu\" Device on which to train the estimators. Use any string that follows the SYCL_DEVICE_FILTER filter selector, e.g. device=\"gpu\" to use the GPU. Read more in the user guide . engine: str, default=\"sklearn\" Execution engine to use for the estimators. Refer to the user guide for an explanation regarding every choice. Choose from: \"sklearn\" (only if device=\"cpu\") \"sklearnex\" \"cuml\" (only if device=\"gpu\") verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of python's warnings filters . Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic name. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState used by np.random . See Also ATOMClassifier Main class for binary and multiclass classification tasks. Example >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> # Initialize atom >>> atom = ATOMClassifier ( X , y , logger = \"auto\" , n_jobs = 2 , verbose = 2 ) << ================== ATOM ================== >> Algorithm task: binary classification. Parallel processing with 2 cores. Dataset stats ==================== >> Shape: (569, 31) Memory: 138.96 kB Scaled: False Outlier values: 160 (1.1%) ------------------------------------- Train set size: 456 Test set size: 113 ------------------------------------- | | dataset | train | test | | - | ----------- | ----------- | ----------- | | 0 | 212 (1.0) | 170 (1.0) | 42 (1.0) | | 1 | 357 (1.7) | 286 (1.7) | 71 (1.7) | >>> # Apply data cleaning and feature engineering methods >>> atom . balance ( strategy = \"smote\" ) Oversampling with SMOTE... --> Adding 116 samples to class 0. >>> atom . feature_selection ( strategy = \"rfecv\" , solver = \"xgb\" , n_features = 22 ) Fitting FeatureSelector... Performing feature selection ... --> RFECV selected 26 features from the dataset. --> Dropping feature mean perimeter (rank 4). --> Dropping feature mean symmetry (rank 3). --> Dropping feature perimeter error (rank 2). --> Dropping feature worst compactness (rank 5). >>> # Train models >>> atom . run ( ... models = [ \"LR\" , \"RF\" , \"XGB\" ], ... metric = \"precision\" , ... n_bootstrap = 4 , ... ) Training ========================= >> Models: LR, RF, XGB Metric: precision Results for Logistic Regression: Fit --------------------------------------------- Train evaluation --> precision: 0.9895 Test evaluation --> precision: 0.9467 Time elapsed: 0.028s ------------------------------------------------- Total time: 0.028s Results for Random Forest: Fit --------------------------------------------- Train evaluation --> precision: 1.0 Test evaluation --> precision: 0.9221 Time elapsed: 0.181s ------------------------------------------------- Total time: 0.181s Results for XGBoost: Fit --------------------------------------------- Train evaluation --> precision: 1.0 Test evaluation --> precision: 0.9091 Time elapsed: 0.124s ------------------------------------------------- Total time: 0.124s Final results ==================== >> Duration: 0.333s ------------------------------------- Logistic Regression --> precision: 0.9467 ! Random Forest --> precision: 0.9221 XGBoost --> precision: 0.9091 >>> # Analyze the results >>> atom . plot_results () Magic methods The class contains some magic methods to help you access some of its elements faster. Note that methods that apply on the pipeline can return different results per branch. __repr__: Prints an overview of atom's branches, models, metric and errors. __len__: Returns the length of the dataset. __iter__: Iterate over the pipeline's transformers. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a branch, model, column or subset of the dataset. Attributes Data attributes The data attributes are used to access the dataset and its properties. Updating the dataset will automatically update the response of these attributes accordingly. Attributes pipeline: pd.Series Transformers fitted on the data. Use this attribute only to access the individual instances. To visualize the pipeline, use the plot_pipeline method. mapping: dict Encoded values and their respective mapped values. The column name is the key to its mapping dictionary. Only for columns mapped to a single column (e.g. Ordinal, Leave-one-out, etc...). dataset: pd.DataFrame Complete data set. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Features of the training set. y_train: pd.Series Target column of the training set. X_test: pd.DataFrame Features of the test set. y_test: pd.Series Target column of the test set. shape: Tuple[int, int] Shape of the dataset (n_rows, n_cols). columns: pd.Series Name of all the columns. n_columns: int Number of columns. features: pd.Series Name of the features. n_features: int Number of features. target: str Name of the target column. scaled: bool Whether the feature set is scaled. A data set is considered scaled when it has mean=0 and std=1, or when atom has a scaler in the pipeline. Returns None for sparse datasets . duplicates: pd.Series Number of duplicate rows in the dataset. nans: pd.Series Columns with the number of missing values in them. n_nans: int Number of samples containing missing values. numerical: pd.Series Names of the numerical features in the dataset. n_numerical: int Number of numerical features in the dataset. categorical: pd.Series Names of the categorical features in the dataset. n_categorical: int Number of categorical features in the dataset. outliers: pd.Series Columns in training set with amount of outlier values. n_outliers: int Number of samples in the training set containing outliers. Utility attributes The utility attributes are used to access information about the models in the instance after training . Attributes models: str or list Return the names of all models. metric: str or list Return the name of the metric. errors: dict Return the errors encountered during model training. winners: list Return the model names ordered by performance. winner: models Return the best performing model. Tracking attributes The tracking attributes are used to customize what elements of the experiment are tracked. Read more in the user guide . Attributes log_bo: bool Whether to track every trial of the hyperparameter tuning. log_model: bool Whether to save the model's estimator after fitting. log_plots: bool Whether to save plots as artifacts. log_data: bool Whether to save the train and test sets. log_pipeline: bool Whether to save the model's pipeline. Plot attributes The plot attributes are used to customize the plot's aesthetics. Read more in the user guide . Attributes style: str Plotting style . palette: str Color palette . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes. Utility methods The class contains a variety of utility methods to handle the data and manage the pipeline. add Add a transformer to the pipeline. apply Apply a function to the dataset. automl Search for an optimized pipeline in an automated fashion. available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. distribution Get statistics on column distributions. evaluate Get all models' scores for the provided metrics. export_pipeline Export atom's pipeline. get_class_weight Return class weights for a balanced dataset. inverse_transform Inversely transform new data through the pipeline. log Print message and save to log file. merge Merge another instance of the same class into this one. report Create an extensive profile analysis report of the data. reset Reset the instance to it's initial state. reset_aesthetics Reset the plot aesthetics to their default values. save Save the instance to a pickle file. save_data Save the data in the current branch to a .csv file. shrink Converts the columns to the smallest possible matching dtype. stacking Add a Stacking model to the pipeline. stats Print basic information about the dataset. status Get an overview of the branches and models. transform Transform new data through the pipeline. voting Add a Voting model to the pipeline. method add (transformer, columns=None, train_only=False, **fit_params) [source] Add a transformer to the pipeline. If the transformer is not fitted, it is fitted on the complete training set. Afterwards, the data set is transformed and the estimator is added to atom's pipeline. If the estimator is a sklearn Pipeline, every estimator is merged independently with atom. Warning The transformer should have fit and/or transform methods with arguments X (accepting a dataframe-like object of shape=(n_samples, n_features)) and/or y (accepting a sequence of shape=(n_samples,)). The transform method should return a feature set as a dataframe-like object of shape=(n_samples, n_features) and/or a target column as a sequence of shape=(n_samples,). Note If the transform method doesn't return a dataframe: The column naming happens as follows. If the transformer has a get_feature_names or get_feature_names_out method, it is used. If not, and it returns the same number of columns, the names are kept equal. If the number of columns change, old columns will keep their name (as long as the column is unchanged) and new columns will receive the name x[N-1] , where N stands for the n-th feature. This means that a transformer should only transform, add or drop columns, not combinations of these. The index remains the same as before the transformation. This means that the transformer should not add, remove or shuffle rows unless it returns a dataframe. Note If the transformer has a n_jobs and/or random_state parameter that is left to its default value, it adopts atom's value. Parameters transformer: Transformer Estimator to add to the pipeline. Should implement a transform method. columns: int, str, slice, sequence or None, default=None Names, indices or dtypes of the columns in the dataset to transform. If None, transform all columns. Add ! in front of a name or dtype to exclude that column, e.g. atom.add(Transformer(), columns=\"!Location\") transforms all columns except Location`. You can either include or exclude columns, not combinations of these. The target column is always included if required by the transformer. train_only: bool, default=False Whether to apply the estimator only on the training set or on the complete dataset. Note that if True, the transformation is skipped when making predictions on new data. **fit_params Additional keyword arguments for the transformer's fit method. method apply (func, inverse_func=None, kw_args=None, inv_kw_args=None, **kwargs) [source] Apply a function to the dataset. The function should have signature func(dataset, **kw_args) -> dataset . This method is useful for stateless transformations such as taking the log, doing custom scaling, etc... Note This approach is preferred over changing the dataset directly through the property's @setter since the transformation is stored in the pipeline. Tip Use atom . apply ( lambda df : df . drop ( \"column_name\" , axis = 1 )) to store the removal of columns in the pipeline. Parameters func: callable Function to apply. inverse_func: callable or None, default=None Inverse function of func . If None, the inverse_transform method returns the input unchanged. kw_args: dict or None, default=None Additional keyword arguments for the function. inv_kw_args: dict or None, default=None Additional keyword arguments for the inverse function. method automl (**kwargs) [source] Search for an optimized pipeline in an automated fashion. Automated machine learning (AutoML) automates the selection, composition and parameterization of machine learning pipelines. Automating the machine learning often provides faster, more accurate outputs than hand-coded algorithms. ATOM uses the evalML package for AutoML optimization. The resulting transformers and final estimator are merged with atom's pipeline (check the pipeline and models attributes after the method finishes running). The created AutoMLSearch instance can be accessed through the evalml attribute. Warning AutoML algorithms aren't intended to run for only a few minutes. The method may need a very long time to achieve optimal results. Parameters **kwargs Additional keyword arguments for the AutoMLSearch instance. method available_models () [source] Give an overview of the available predefined models. Returns pd.DataFrame Information about the available predefined models . Columns include: - acronym: Model's acronym (used to call the model). - fullname: Complete name of the model. - estimator: The model's underlying estimator. - module: The estimator's module. - needs_scaling: Whether the model requires feature scaling. - accepts_sparse: Whether the model accepts sparse matrices. - supports_engines: List of engines supported by the model. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] Create a figure with multiple plots. This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Clear attributes from all models. Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the instance. The cleared attributes per model are: Prediction attributes Metric scores Shap values App instance Dashboard instance method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving . Deleted models are not removed from any active mlflow experiment . Parameters models: int, str, slice, sequence or None, default=None Names, positions or regex pattern of the models to delete. If None, all models are deleted. method distribution (distributions=None, columns=None) [source] Get statistics on column distributions. Compute the Kolmogorov-Smirnov test for various distributions against columns in the dataset. Only for numerical columns. Missing values are ignored. Tip Use the plot_distribution method to plot a column's distribution. Parameters distributions: str, sequence or None, default=None Names of the distributions in scipy.stats to get the statistics on. If None, a selection of the most common ones is used. columns: int, str, slice, sequence or None, default=None Names, positions or dtypes of the columns in the dataset to perform the test on. If None, select all numerical columns. Returns pd.DataFrame Statistic results with multiindex levels: - dist: Name of the distribution. - stat: Statistic results: - score: KS-test score. - p_value: Corresponding p-value. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get all models' scores for the provided metrics. Parameters metric: str, func, scorer, sequence or None, default=None Metric to calculate. If None, it returns an overview of the most common metrics per task. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: - The task is binary classification. - The model has a predict_proba method. - The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns pd.DataFrame Scores of the models. method export_pipeline (model=None, memory=None, verbose=None) [source] Export atom's pipeline. Optionally, you can add a model as final estimator. The returned pipeline is already fitted on the training set. Info The returned pipeline behaves similarly to sklearn's Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always returns pandas objects. Uses transformers that are only applied on the training set to fit the pipeline, not to make predictions. Parameters model: str or None, default=None Name of the model to add as a final estimator to the pipeline. If the model used automated feature scaling , the scaler is added before the model. If None, only the transformers are added. memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. - If None or False: No caching is performed. - If True: A default temp directory is used. - If str: Path to the caching directory. - If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns Pipeline Sklearn-like Pipeline object with all transformers in the current branch. method get_class_weight (dataset=\"train\") [source] Return class weights for a balanced dataset. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns dict Classes with the corresponding weights. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be used to transform only the target column. Parameters X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None Target column corresponding to X. - If None: y is ignored in the transformers. - If int: Position of the target column in X. - If str: Name of the target column in X. - Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level for the transformers. If None, it uses the transformer's own verbosity. Returns pd.DataFrame Original feature set. Only returned if provided. y: pd.Series Original target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters other: Runner Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method report (dataset=\"dataset\", n_rows=None, filename=None, **kwargs) [source] Create an extensive profile analysis report of the data. ATOM uses the pandas-profiling package for the analysis. The report is rendered directly in the notebook. The created ProfileReport instance can be accessed through the profile attribute. Warning This method can be slow for large datasets. Parameters dataset: str, default=\"dataset\" Data set to get the report from. n_rows: int or None, default=None Number of (randomly picked) rows to process. None to use all rows. filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ProfileReport instance. method reset () [source] Reset the instance to it's initial state. Deletes all branches and models. The dataset is also reset to its form after initialization. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method save_data (filename=\"auto\", dataset=\"dataset\") [source] Save the data in the current branch to a .csv file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. dataset: str, default=\"dataset\" Data set to save. method shrink (obj2cat=True, int2uint=False, dense2sparse=False, columns=None) [source] Converts the columns to the smallest possible matching dtype. Parameters obj2cat: bool, default=True Whether to convert object to category . Only if the number of categories would be less than 30% of the length of the column. int2uint: bool, default=False Whether to convert int to uint (unsigned integer). Only if the values in the column are strictly positive. dense2sparse: bool, default=False Whether to convert all features to sparse format. The value that is compressed is the most frequent value in the column. columns: int, str, slice, sequence or None, default=None Names, positions or dtypes of the columns in the dataset to shrink. If None, transform all columns. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's stacking instance. The model's acronyms can be used for the final_estimator parameter. method stats (_vb=-2) [source] Print basic information about the dataset. Tip For classification tasks, the count and balance of classes is shown, followed by the ratio (between parentheses) of the class with respect to the rest of the classes in the same data set, i.e. the class with the fewest samples is followed by (1.0) . This information can be used to quickly assess if the data set is unbalanced. Parameters _vb: int, default=-2 Internal parameter to always print if called by user. method status () [source] Get an overview of the branches and models. This method prints the same information as the __repr__ and also saves it to the logger. method transform (X=None, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None Target column corresponding to X. - If None: y is ignored in the transformers. - If int: Position of the target column in X. - If str: Name of the target column in X. - Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level for the transformers. If None, it uses the transformer's own verbosity. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's voting instance. Data cleaning The data cleaning methods can help you scale the data, handle missing values, categorical columns and outliers. All attributes of the data cleaning classes are attached to atom after running. Read more in the user guide . Tip Use the report method to examine the data and help you determine suitable parameters for the data cleaning methods. clean Applies standard data cleaning steps on the dataset. discretize Bin continuous data into intervals. encode Perform encoding of categorical features. impute Handle missing values in the dataset. normalize Transform the data to follow a Normal/Gaussian distribution. prune Prune outliers from the training set. scale Scale the data. method clean (drop_types=None, strip_categorical=True, drop_duplicates=False, drop_missing_target=True, encode_target=True, **kwargs) [source] Applies standard data cleaning steps on the dataset. Use the parameters to choose which transformations to perform. The available steps are: Drop columns with specific data types. Strip categorical features from white spaces. Drop duplicate rows. Drop rows with missing values in the target column. Encode the target column (can't be True for regression tasks). See the Cleaner class for a description of the parameters. method discretize (strategy=\"quantile\", bins=5, labels=None, **kwargs) [source] Bin continuous data into intervals. For each feature, the bin edges are computed during fit and, together with the number of bins, they will define the intervals. Ignores numerical columns. See the Discretizer class for a description of the parameters. Tip Use the plot_distribution method to visualize a column's distribution and decide on the bins. method encode (strategy=\"LeaveOneOut\", max_onehot=10, ordinal=None, rare_to_value=None, value=\"rare\", **kwargs) [source] Perform encoding of categorical features. The encoding type depends on the number of classes in the column: If n_classes=2 or ordinal feature, use Ordinal-encoding. If 2 < n_classes <= max_onehot , use OneHot-encoding. If n_classes > max_onehot , use strategy -encoding. Missing values are propagated to the output column. Unknown classes encountered during transforming are imputed according to the selected strategy. Rare classes can be replaced with a value in order to prevent too high cardinality. See the Encoder class for a description of the parameters. Note This method only encodes the categorical features. It does not encode the target column! Use the clean method for that. Tip Use the categorical attribute for a list of the categorical features in the dataset. method impute (strat_num=\"drop\", strat_cat=\"drop\", max_nan_rows=None, max_nan_cols=None, **kwargs) [source] Handle missing values in the dataset. Impute or remove missing values according to the selected strategy. Also removes rows and columns with too many missing values. Use the missing attribute to customize what are considered \"missing values\". See the Imputer class for a description of the parameters. Tip Use the nans attribute to check the amount of missing values per column. method normalize (strategy=\"yeojohnson\", **kwargs) [source] Transform the data to follow a Normal/Gaussian distribution. This transformation is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Missing values are disregarded in fit and maintained in transform. Ignores categorical columns. See the Normalizer class for a description of the parameters. Tip Use the plot_distribution method to examine a column's distribution. method prune (strategy=\"zscore\", method=\"drop\", max_sigma=3, include_target=False, **kwargs) [source] Prune outliers from the training set. Replace or remove outliers. The definition of outlier depends on the selected strategy and can greatly differ from one another. Ignores categorical columns. See the Pruner class for a description of the parameters. Note This transformation is only applied to the training set in order to maintain the original distribution of samples in the test set. Tip Use the outliers attribute to check the number of outliers per column. method scale (strategy=\"standard\", **kwargs) [source] Scale the data. Apply one of sklearn's scalers. Ignores categorical columns. See the Scaler class for a description of the parameters. Tip Use the scaled attribute to check whether the dataset is scaled. NLP The Natural Language Processing (NLP) transformers help to convert raw text to meaningful numeric values, ready to be ingested by a model. All transformations are applied only on the column in the dataset called corpus . Read more in the user guide . textclean Applies standard text cleaning to the corpus. textnormalize Normalize the corpus. tokenize Tokenize the corpus. vectorize Vectorize the corpus. method textclean (decode=True, lower_case=True, drop_email=True, regex_email=None, drop_url=True, regex_url=None, drop_html=True, regex_html=None, drop_emoji=True, regex_emoji=None, drop_number=True, regex_number=None, drop_punctuation=True, **kwargs) [source] Applies standard text cleaning to the corpus. Transformations include normalizing characters and dropping noise from the text (emails, HTML tags, URLs, etc...). The transformations are applied on the column named corpus , in the same order the parameters are presented. If there is no column with that name, an exception is raised. See the TextCleaner class for a description of the parameters. method textnormalize (stopwords=True, custom_stopwords=None, stem=False, lemmatize=True, **kwargs) [source] Normalize the corpus. Convert words to a more uniform standard. The transformations are applied on the column named corpus , in the same order the parameters are presented. If there is no column with that name, an exception is raised. If the provided documents are strings, words are separated by spaces. See the TextNormalizer class for a description of the parameters. method tokenize (bigram_freq=None, trigram_freq=None, quadgram_freq=None, **kwargs) [source] Tokenize the corpus. Convert documents into sequences of words. Additionally, create n-grams (represented by words united with underscores, e.g. \"New_York\") based on their frequency in the corpus. The transformations are applied on the column named corpus . If there is no column with that name, an exception is raised. See the Tokenizer class for a description of the parameters. method vectorize (strategy=\"bow\", return_sparse=True, **kwargs) [source] Vectorize the corpus. Transform the corpus into meaningful vectors of numbers. The transformation is applied on the column named corpus . If there is no column with that name, an exception is raised. If strategy=\"bow\" or \"tfidf\", the transformed columns are named after the word they are embedding with the prefix corpus_ . If strategy=\"hashing\", the columns are named hash[N], where N stands for the n-th hashed column. See the Vectorizer class for a description of the parameters. Feature engineering To further pre-process the data, it's possible to extract features from datetime columns, create new non-linear features transforming the existing ones, group similar features or, if the dataset is too large, remove features. Read more in the user guide . feature_extraction Extract features from datetime columns. feature_generation Generate new features. feature_grouping Extract statistics from similar features. feature_selection Reduce the number of features in the data. method feature_extraction (features=['day', 'month', 'year'], fmt=None, encoding_type=\"ordinal\", drop_columns=True, **kwargs) [source] Extract features from datetime columns. Create new features extracting datetime elements (day, month, year, etc...) from the provided columns. Columns of dtype datetime64 are used as is. Categorical columns that can be successfully converted to a datetime format (less than 30% NaT values after conversion) are also used. See the FeatureExtractor class for a description of the parameters. method feature_generation (strategy=\"dfs\", n_features=None, operators=None, **kwargs) [source] Generate new features. Create new combinations of existing features to capture the non-linear relations between the original features. See the FeatureGenerator class for a description of the parameters. method feature_grouping (group, name=None, operators=None, drop_columns=True, **kwargs) [source] Extract statistics from similar features. Replace groups of features with related characteristics with new features that summarize statistical properties of te group. The statistical operators are calculated over every row of the group. The group names and features can be accessed through the groups method. See the FeatureGrouper class for a description of the parameters. method feature_selection (strategy=None, solver=None, n_features=None, min_repeated=2, max_repeated=1.0, max_correlation=1.0, **kwargs) [source] Reduce the number of features in the data. Apply feature selection or dimensionality reduction, either to improve the estimators' accuracy or to boost their performance on very high-dimensional datasets. Additionally, remove multicollinear and low variance features. See the FeatureSelector class for a description of the parameters. Note When strategy=\"univariate\" and solver=None, f_classif or f_regression is used as default solver. When strategy is \"sfs\", \"rfecv\" or any of the advanced strategies and no scoring is specified, atom's metric (if it exists) is used as scoring. Training The training methods are where the models are fitted to the data and their performance is evaluated against selected metric. There are three methods to call the three different training approaches. All relevant attributes and methods from the training classes are attached to atom for convenience. These include the errors, winner and results attributes, as well as the models , and the prediction and plotting methods. Read more in the user guide . run Train and evaluate the models in a direct fashion. successive_halving Fit the models in a successive halving fashion. train_sizing Train and evaluate the models in a train sizing fashion. method run (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, **kwargs) [source] Train and evaluate the models in a direct fashion. Contrary to successive_halving and train_sizing , the direct approach only iterates once over the models, using the full dataset. The following steps are applied to every model: Apply hyperparameter tuning (optional). Fit the model on the training set using the best combination of hyperparameters found. Evaluate the model on the test set. Train the model on various bootstrapped samples of the training set and evaluate again on the test set (optional). See the DirectClassifier or DirectRegressor class for a description of the parameters. method successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_runs=0, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, **kwargs) [source] Fit the models in a successive halving fashion. The successive halving technique is a bandit-based algorithm that fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason, it is recommended to only use this technique with similar models, e.g. only using tree-based models. The following steps are applied to every model (per iteration): Apply hyperparameter tuning (optional). Fit the model on the training set using the best combination of hyperparameters found. Evaluate the model on the test set. Train the model on various bootstrapped samples of the training set and evaluate again on the test set (optional). See the SuccessiveHalvingClassifier or SuccessiveHalvingRegressor class for a description of the parameters. method train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=5, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, **kwargs) [source] Train and evaluate the models in a train sizing fashion. When training models, there is usually a trade-off between model performance and computation time, that is regulated by the number of samples in the training set. This method can be used to create insights in this trade-off, and help determine the optimal size of the training set. The models are fitted multiple times, ever-increasing the number of samples in the training set. The following steps are applied to every model (per iteration): Apply hyperparameter tuning (optional). Fit the model on the training set using the best combination of hyperparameters found. Evaluate the model on the test set. Train the model on various bootstrapped samples of the training set and evaluate again on the test set (optional). See the TrainSizingClassifier or TrainSizingRegressor class for a description of the parameters.","title":"ATOMRegressor"},{"location":"API/ATOM/atomregressor/#atomregressor","text":"class atom.api. ATOMRegressor (*arrays, y=-1, index=False, shuffle=True, n_rows=1, test_size=0.2, holdout_size=None, n_jobs=1, device=\"cpu\", engine=\"sklearn\", verbose=0, warnings=False, logger=None, experiment=None, random_state=None) [source] Main class for regression tasks. Apply all data transformations and model management provided by the package on a given dataset. Note that, contrary to sklearn's API, the instance contains the dataset on which to perform the analysis. Calling a method will automatically apply it on the dataset it contains. All data cleaning , feature engineering , model training , predicting , and plotting functionality can be accessed from an instacne of this class. Parameters *arrays: sequence of indexables Dataset containing features and target. Allowed formats are: X X, y train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) X, train, test: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str or sequence Target column corresponding to X. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. y: int, str or sequence, default=-1 Target column corresponding to X. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. This parameter is ignored if the target column is provided through arrays . index: bool, int, str or sequence, default=False Handle the index in the resulting dataframe. If False: Reset to RangeIndex . If True: Use the provided index. If int: Position of the column to use as index. If str: Name of the column to use as index. If sequence: Array with shape=(n_samples,) to use as index. test_size: int or float, default=0.2 If <=1: Fraction of the dataset to include in the test set. If >1: Number of rows to include in the test set. This parameter is ignored if the test set is provided through arrays . holdout_size: int, float or None, default=None If None: No holdout data set is kept apart. If <=1: Fraction of the dataset to include in the holdout set. If >1: Number of rows to include in the holdout set. This parameter is ignored if the holdout set is provided through arrays . shuffle: bool, default=True Whether to shuffle the dataset before splitting the train and test set. Be aware that not shuffling the dataset can cause an unequal distribution of target classes over the sets. n_rows: int or float, default=1 Subsample of the dataset to use. The default value selects all the rows. If <=1: Fraction of the dataset to select. If >1: Exact number of rows to select. Only if arrays is X or X, y. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If <-1: Use number of cores - 1 + n_jobs . device: str, default=\"cpu\" Device on which to train the estimators. Use any string that follows the SYCL_DEVICE_FILTER filter selector, e.g. device=\"gpu\" to use the GPU. Read more in the user guide . engine: str, default=\"sklearn\" Execution engine to use for the estimators. Refer to the user guide for an explanation regarding every choice. Choose from: \"sklearn\" (only if device=\"cpu\") \"sklearnex\" \"cuml\" (only if device=\"gpu\") verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of python's warnings filters . Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic name. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState used by np.random . See Also ATOMClassifier Main class for binary and multiclass classification tasks.","title":"ATOMRegressor"},{"location":"API/ATOM/atomregressor/#example","text":">>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> # Initialize atom >>> atom = ATOMClassifier ( X , y , logger = \"auto\" , n_jobs = 2 , verbose = 2 ) << ================== ATOM ================== >> Algorithm task: binary classification. Parallel processing with 2 cores. Dataset stats ==================== >> Shape: (569, 31) Memory: 138.96 kB Scaled: False Outlier values: 160 (1.1%) ------------------------------------- Train set size: 456 Test set size: 113 ------------------------------------- | | dataset | train | test | | - | ----------- | ----------- | ----------- | | 0 | 212 (1.0) | 170 (1.0) | 42 (1.0) | | 1 | 357 (1.7) | 286 (1.7) | 71 (1.7) | >>> # Apply data cleaning and feature engineering methods >>> atom . balance ( strategy = \"smote\" ) Oversampling with SMOTE... --> Adding 116 samples to class 0. >>> atom . feature_selection ( strategy = \"rfecv\" , solver = \"xgb\" , n_features = 22 ) Fitting FeatureSelector... Performing feature selection ... --> RFECV selected 26 features from the dataset. --> Dropping feature mean perimeter (rank 4). --> Dropping feature mean symmetry (rank 3). --> Dropping feature perimeter error (rank 2). --> Dropping feature worst compactness (rank 5). >>> # Train models >>> atom . run ( ... models = [ \"LR\" , \"RF\" , \"XGB\" ], ... metric = \"precision\" , ... n_bootstrap = 4 , ... ) Training ========================= >> Models: LR, RF, XGB Metric: precision Results for Logistic Regression: Fit --------------------------------------------- Train evaluation --> precision: 0.9895 Test evaluation --> precision: 0.9467 Time elapsed: 0.028s ------------------------------------------------- Total time: 0.028s Results for Random Forest: Fit --------------------------------------------- Train evaluation --> precision: 1.0 Test evaluation --> precision: 0.9221 Time elapsed: 0.181s ------------------------------------------------- Total time: 0.181s Results for XGBoost: Fit --------------------------------------------- Train evaluation --> precision: 1.0 Test evaluation --> precision: 0.9091 Time elapsed: 0.124s ------------------------------------------------- Total time: 0.124s Final results ==================== >> Duration: 0.333s ------------------------------------- Logistic Regression --> precision: 0.9467 ! Random Forest --> precision: 0.9221 XGBoost --> precision: 0.9091 >>> # Analyze the results >>> atom . plot_results ()","title":"Example"},{"location":"API/ATOM/atomregressor/#magic-methods","text":"The class contains some magic methods to help you access some of its elements faster. Note that methods that apply on the pipeline can return different results per branch. __repr__: Prints an overview of atom's branches, models, metric and errors. __len__: Returns the length of the dataset. __iter__: Iterate over the pipeline's transformers. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a branch, model, column or subset of the dataset.","title":"Magic methods"},{"location":"API/ATOM/atomregressor/#attributes","text":"","title":"Attributes"},{"location":"API/ATOM/atomregressor/#data-attributes","text":"The data attributes are used to access the dataset and its properties. Updating the dataset will automatically update the response of these attributes accordingly. Attributes pipeline: pd.Series Transformers fitted on the data. Use this attribute only to access the individual instances. To visualize the pipeline, use the plot_pipeline method. mapping: dict Encoded values and their respective mapped values. The column name is the key to its mapping dictionary. Only for columns mapped to a single column (e.g. Ordinal, Leave-one-out, etc...). dataset: pd.DataFrame Complete data set. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Features of the training set. y_train: pd.Series Target column of the training set. X_test: pd.DataFrame Features of the test set. y_test: pd.Series Target column of the test set. shape: Tuple[int, int] Shape of the dataset (n_rows, n_cols). columns: pd.Series Name of all the columns. n_columns: int Number of columns. features: pd.Series Name of the features. n_features: int Number of features. target: str Name of the target column. scaled: bool Whether the feature set is scaled. A data set is considered scaled when it has mean=0 and std=1, or when atom has a scaler in the pipeline. Returns None for sparse datasets . duplicates: pd.Series Number of duplicate rows in the dataset. nans: pd.Series Columns with the number of missing values in them. n_nans: int Number of samples containing missing values. numerical: pd.Series Names of the numerical features in the dataset. n_numerical: int Number of numerical features in the dataset. categorical: pd.Series Names of the categorical features in the dataset. n_categorical: int Number of categorical features in the dataset. outliers: pd.Series Columns in training set with amount of outlier values. n_outliers: int Number of samples in the training set containing outliers.","title":"Data attributes"},{"location":"API/ATOM/atomregressor/#utility-attributes","text":"The utility attributes are used to access information about the models in the instance after training . Attributes models: str or list Return the names of all models. metric: str or list Return the name of the metric. errors: dict Return the errors encountered during model training. winners: list Return the model names ordered by performance. winner: models Return the best performing model.","title":"Utility attributes"},{"location":"API/ATOM/atomregressor/#tracking-attributes","text":"The tracking attributes are used to customize what elements of the experiment are tracked. Read more in the user guide . Attributes log_bo: bool Whether to track every trial of the hyperparameter tuning. log_model: bool Whether to save the model's estimator after fitting. log_plots: bool Whether to save plots as artifacts. log_data: bool Whether to save the train and test sets. log_pipeline: bool Whether to save the model's pipeline.","title":"Tracking attributes"},{"location":"API/ATOM/atomregressor/#plot-attributes","text":"The plot attributes are used to customize the plot's aesthetics. Read more in the user guide . Attributes style: str Plotting style . palette: str Color palette . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes.","title":"Plot attributes"},{"location":"API/ATOM/atomregressor/#utility-methods","text":"The class contains a variety of utility methods to handle the data and manage the pipeline. add Add a transformer to the pipeline. apply Apply a function to the dataset. automl Search for an optimized pipeline in an automated fashion. available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. distribution Get statistics on column distributions. evaluate Get all models' scores for the provided metrics. export_pipeline Export atom's pipeline. get_class_weight Return class weights for a balanced dataset. inverse_transform Inversely transform new data through the pipeline. log Print message and save to log file. merge Merge another instance of the same class into this one. report Create an extensive profile analysis report of the data. reset Reset the instance to it's initial state. reset_aesthetics Reset the plot aesthetics to their default values. save Save the instance to a pickle file. save_data Save the data in the current branch to a .csv file. shrink Converts the columns to the smallest possible matching dtype. stacking Add a Stacking model to the pipeline. stats Print basic information about the dataset. status Get an overview of the branches and models. transform Transform new data through the pipeline. voting Add a Voting model to the pipeline. method add (transformer, columns=None, train_only=False, **fit_params) [source] Add a transformer to the pipeline. If the transformer is not fitted, it is fitted on the complete training set. Afterwards, the data set is transformed and the estimator is added to atom's pipeline. If the estimator is a sklearn Pipeline, every estimator is merged independently with atom. Warning The transformer should have fit and/or transform methods with arguments X (accepting a dataframe-like object of shape=(n_samples, n_features)) and/or y (accepting a sequence of shape=(n_samples,)). The transform method should return a feature set as a dataframe-like object of shape=(n_samples, n_features) and/or a target column as a sequence of shape=(n_samples,). Note If the transform method doesn't return a dataframe: The column naming happens as follows. If the transformer has a get_feature_names or get_feature_names_out method, it is used. If not, and it returns the same number of columns, the names are kept equal. If the number of columns change, old columns will keep their name (as long as the column is unchanged) and new columns will receive the name x[N-1] , where N stands for the n-th feature. This means that a transformer should only transform, add or drop columns, not combinations of these. The index remains the same as before the transformation. This means that the transformer should not add, remove or shuffle rows unless it returns a dataframe. Note If the transformer has a n_jobs and/or random_state parameter that is left to its default value, it adopts atom's value. Parameters transformer: Transformer Estimator to add to the pipeline. Should implement a transform method. columns: int, str, slice, sequence or None, default=None Names, indices or dtypes of the columns in the dataset to transform. If None, transform all columns. Add ! in front of a name or dtype to exclude that column, e.g. atom.add(Transformer(), columns=\"!Location\") transforms all columns except Location`. You can either include or exclude columns, not combinations of these. The target column is always included if required by the transformer. train_only: bool, default=False Whether to apply the estimator only on the training set or on the complete dataset. Note that if True, the transformation is skipped when making predictions on new data. **fit_params Additional keyword arguments for the transformer's fit method. method apply (func, inverse_func=None, kw_args=None, inv_kw_args=None, **kwargs) [source] Apply a function to the dataset. The function should have signature func(dataset, **kw_args) -> dataset . This method is useful for stateless transformations such as taking the log, doing custom scaling, etc... Note This approach is preferred over changing the dataset directly through the property's @setter since the transformation is stored in the pipeline. Tip Use atom . apply ( lambda df : df . drop ( \"column_name\" , axis = 1 )) to store the removal of columns in the pipeline. Parameters func: callable Function to apply. inverse_func: callable or None, default=None Inverse function of func . If None, the inverse_transform method returns the input unchanged. kw_args: dict or None, default=None Additional keyword arguments for the function. inv_kw_args: dict or None, default=None Additional keyword arguments for the inverse function. method automl (**kwargs) [source] Search for an optimized pipeline in an automated fashion. Automated machine learning (AutoML) automates the selection, composition and parameterization of machine learning pipelines. Automating the machine learning often provides faster, more accurate outputs than hand-coded algorithms. ATOM uses the evalML package for AutoML optimization. The resulting transformers and final estimator are merged with atom's pipeline (check the pipeline and models attributes after the method finishes running). The created AutoMLSearch instance can be accessed through the evalml attribute. Warning AutoML algorithms aren't intended to run for only a few minutes. The method may need a very long time to achieve optimal results. Parameters **kwargs Additional keyword arguments for the AutoMLSearch instance. method available_models () [source] Give an overview of the available predefined models. Returns pd.DataFrame Information about the available predefined models . Columns include: - acronym: Model's acronym (used to call the model). - fullname: Complete name of the model. - estimator: The model's underlying estimator. - module: The estimator's module. - needs_scaling: Whether the model requires feature scaling. - accepts_sparse: Whether the model accepts sparse matrices. - supports_engines: List of engines supported by the model. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] Create a figure with multiple plots. This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Clear attributes from all models. Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the instance. The cleared attributes per model are: Prediction attributes Metric scores Shap values App instance Dashboard instance method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving . Deleted models are not removed from any active mlflow experiment . Parameters models: int, str, slice, sequence or None, default=None Names, positions or regex pattern of the models to delete. If None, all models are deleted. method distribution (distributions=None, columns=None) [source] Get statistics on column distributions. Compute the Kolmogorov-Smirnov test for various distributions against columns in the dataset. Only for numerical columns. Missing values are ignored. Tip Use the plot_distribution method to plot a column's distribution. Parameters distributions: str, sequence or None, default=None Names of the distributions in scipy.stats to get the statistics on. If None, a selection of the most common ones is used. columns: int, str, slice, sequence or None, default=None Names, positions or dtypes of the columns in the dataset to perform the test on. If None, select all numerical columns. Returns pd.DataFrame Statistic results with multiindex levels: - dist: Name of the distribution. - stat: Statistic results: - score: KS-test score. - p_value: Corresponding p-value. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get all models' scores for the provided metrics. Parameters metric: str, func, scorer, sequence or None, default=None Metric to calculate. If None, it returns an overview of the most common metrics per task. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: - The task is binary classification. - The model has a predict_proba method. - The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns pd.DataFrame Scores of the models. method export_pipeline (model=None, memory=None, verbose=None) [source] Export atom's pipeline. Optionally, you can add a model as final estimator. The returned pipeline is already fitted on the training set. Info The returned pipeline behaves similarly to sklearn's Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always returns pandas objects. Uses transformers that are only applied on the training set to fit the pipeline, not to make predictions. Parameters model: str or None, default=None Name of the model to add as a final estimator to the pipeline. If the model used automated feature scaling , the scaler is added before the model. If None, only the transformers are added. memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. - If None or False: No caching is performed. - If True: A default temp directory is used. - If str: Path to the caching directory. - If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns Pipeline Sklearn-like Pipeline object with all transformers in the current branch. method get_class_weight (dataset=\"train\") [source] Return class weights for a balanced dataset. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns dict Classes with the corresponding weights. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be used to transform only the target column. Parameters X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None Target column corresponding to X. - If None: y is ignored in the transformers. - If int: Position of the target column in X. - If str: Name of the target column in X. - Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level for the transformers. If None, it uses the transformer's own verbosity. Returns pd.DataFrame Original feature set. Only returned if provided. y: pd.Series Original target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters other: Runner Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method report (dataset=\"dataset\", n_rows=None, filename=None, **kwargs) [source] Create an extensive profile analysis report of the data. ATOM uses the pandas-profiling package for the analysis. The report is rendered directly in the notebook. The created ProfileReport instance can be accessed through the profile attribute. Warning This method can be slow for large datasets. Parameters dataset: str, default=\"dataset\" Data set to get the report from. n_rows: int or None, default=None Number of (randomly picked) rows to process. None to use all rows. filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ProfileReport instance. method reset () [source] Reset the instance to it's initial state. Deletes all branches and models. The dataset is also reset to its form after initialization. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method save_data (filename=\"auto\", dataset=\"dataset\") [source] Save the data in the current branch to a .csv file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. dataset: str, default=\"dataset\" Data set to save. method shrink (obj2cat=True, int2uint=False, dense2sparse=False, columns=None) [source] Converts the columns to the smallest possible matching dtype. Parameters obj2cat: bool, default=True Whether to convert object to category . Only if the number of categories would be less than 30% of the length of the column. int2uint: bool, default=False Whether to convert int to uint (unsigned integer). Only if the values in the column are strictly positive. dense2sparse: bool, default=False Whether to convert all features to sparse format. The value that is compressed is the most frequent value in the column. columns: int, str, slice, sequence or None, default=None Names, positions or dtypes of the columns in the dataset to shrink. If None, transform all columns. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's stacking instance. The model's acronyms can be used for the final_estimator parameter. method stats (_vb=-2) [source] Print basic information about the dataset. Tip For classification tasks, the count and balance of classes is shown, followed by the ratio (between parentheses) of the class with respect to the rest of the classes in the same data set, i.e. the class with the fewest samples is followed by (1.0) . This information can be used to quickly assess if the data set is unbalanced. Parameters _vb: int, default=-2 Internal parameter to always print if called by user. method status () [source] Get an overview of the branches and models. This method prints the same information as the __repr__ and also saves it to the logger. method transform (X=None, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None Target column corresponding to X. - If None: y is ignored in the transformers. - If int: Position of the target column in X. - If str: Name of the target column in X. - Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level for the transformers. If None, it uses the transformer's own verbosity. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's voting instance.","title":"Utility methods"},{"location":"API/ATOM/atomregressor/#data-cleaning","text":"The data cleaning methods can help you scale the data, handle missing values, categorical columns and outliers. All attributes of the data cleaning classes are attached to atom after running. Read more in the user guide . Tip Use the report method to examine the data and help you determine suitable parameters for the data cleaning methods. clean Applies standard data cleaning steps on the dataset. discretize Bin continuous data into intervals. encode Perform encoding of categorical features. impute Handle missing values in the dataset. normalize Transform the data to follow a Normal/Gaussian distribution. prune Prune outliers from the training set. scale Scale the data. method clean (drop_types=None, strip_categorical=True, drop_duplicates=False, drop_missing_target=True, encode_target=True, **kwargs) [source] Applies standard data cleaning steps on the dataset. Use the parameters to choose which transformations to perform. The available steps are: Drop columns with specific data types. Strip categorical features from white spaces. Drop duplicate rows. Drop rows with missing values in the target column. Encode the target column (can't be True for regression tasks). See the Cleaner class for a description of the parameters. method discretize (strategy=\"quantile\", bins=5, labels=None, **kwargs) [source] Bin continuous data into intervals. For each feature, the bin edges are computed during fit and, together with the number of bins, they will define the intervals. Ignores numerical columns. See the Discretizer class for a description of the parameters. Tip Use the plot_distribution method to visualize a column's distribution and decide on the bins. method encode (strategy=\"LeaveOneOut\", max_onehot=10, ordinal=None, rare_to_value=None, value=\"rare\", **kwargs) [source] Perform encoding of categorical features. The encoding type depends on the number of classes in the column: If n_classes=2 or ordinal feature, use Ordinal-encoding. If 2 < n_classes <= max_onehot , use OneHot-encoding. If n_classes > max_onehot , use strategy -encoding. Missing values are propagated to the output column. Unknown classes encountered during transforming are imputed according to the selected strategy. Rare classes can be replaced with a value in order to prevent too high cardinality. See the Encoder class for a description of the parameters. Note This method only encodes the categorical features. It does not encode the target column! Use the clean method for that. Tip Use the categorical attribute for a list of the categorical features in the dataset. method impute (strat_num=\"drop\", strat_cat=\"drop\", max_nan_rows=None, max_nan_cols=None, **kwargs) [source] Handle missing values in the dataset. Impute or remove missing values according to the selected strategy. Also removes rows and columns with too many missing values. Use the missing attribute to customize what are considered \"missing values\". See the Imputer class for a description of the parameters. Tip Use the nans attribute to check the amount of missing values per column. method normalize (strategy=\"yeojohnson\", **kwargs) [source] Transform the data to follow a Normal/Gaussian distribution. This transformation is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Missing values are disregarded in fit and maintained in transform. Ignores categorical columns. See the Normalizer class for a description of the parameters. Tip Use the plot_distribution method to examine a column's distribution. method prune (strategy=\"zscore\", method=\"drop\", max_sigma=3, include_target=False, **kwargs) [source] Prune outliers from the training set. Replace or remove outliers. The definition of outlier depends on the selected strategy and can greatly differ from one another. Ignores categorical columns. See the Pruner class for a description of the parameters. Note This transformation is only applied to the training set in order to maintain the original distribution of samples in the test set. Tip Use the outliers attribute to check the number of outliers per column. method scale (strategy=\"standard\", **kwargs) [source] Scale the data. Apply one of sklearn's scalers. Ignores categorical columns. See the Scaler class for a description of the parameters. Tip Use the scaled attribute to check whether the dataset is scaled.","title":"Data cleaning"},{"location":"API/ATOM/atomregressor/#nlp","text":"The Natural Language Processing (NLP) transformers help to convert raw text to meaningful numeric values, ready to be ingested by a model. All transformations are applied only on the column in the dataset called corpus . Read more in the user guide . textclean Applies standard text cleaning to the corpus. textnormalize Normalize the corpus. tokenize Tokenize the corpus. vectorize Vectorize the corpus. method textclean (decode=True, lower_case=True, drop_email=True, regex_email=None, drop_url=True, regex_url=None, drop_html=True, regex_html=None, drop_emoji=True, regex_emoji=None, drop_number=True, regex_number=None, drop_punctuation=True, **kwargs) [source] Applies standard text cleaning to the corpus. Transformations include normalizing characters and dropping noise from the text (emails, HTML tags, URLs, etc...). The transformations are applied on the column named corpus , in the same order the parameters are presented. If there is no column with that name, an exception is raised. See the TextCleaner class for a description of the parameters. method textnormalize (stopwords=True, custom_stopwords=None, stem=False, lemmatize=True, **kwargs) [source] Normalize the corpus. Convert words to a more uniform standard. The transformations are applied on the column named corpus , in the same order the parameters are presented. If there is no column with that name, an exception is raised. If the provided documents are strings, words are separated by spaces. See the TextNormalizer class for a description of the parameters. method tokenize (bigram_freq=None, trigram_freq=None, quadgram_freq=None, **kwargs) [source] Tokenize the corpus. Convert documents into sequences of words. Additionally, create n-grams (represented by words united with underscores, e.g. \"New_York\") based on their frequency in the corpus. The transformations are applied on the column named corpus . If there is no column with that name, an exception is raised. See the Tokenizer class for a description of the parameters. method vectorize (strategy=\"bow\", return_sparse=True, **kwargs) [source] Vectorize the corpus. Transform the corpus into meaningful vectors of numbers. The transformation is applied on the column named corpus . If there is no column with that name, an exception is raised. If strategy=\"bow\" or \"tfidf\", the transformed columns are named after the word they are embedding with the prefix corpus_ . If strategy=\"hashing\", the columns are named hash[N], where N stands for the n-th hashed column. See the Vectorizer class for a description of the parameters.","title":"NLP"},{"location":"API/ATOM/atomregressor/#feature-engineering","text":"To further pre-process the data, it's possible to extract features from datetime columns, create new non-linear features transforming the existing ones, group similar features or, if the dataset is too large, remove features. Read more in the user guide . feature_extraction Extract features from datetime columns. feature_generation Generate new features. feature_grouping Extract statistics from similar features. feature_selection Reduce the number of features in the data. method feature_extraction (features=['day', 'month', 'year'], fmt=None, encoding_type=\"ordinal\", drop_columns=True, **kwargs) [source] Extract features from datetime columns. Create new features extracting datetime elements (day, month, year, etc...) from the provided columns. Columns of dtype datetime64 are used as is. Categorical columns that can be successfully converted to a datetime format (less than 30% NaT values after conversion) are also used. See the FeatureExtractor class for a description of the parameters. method feature_generation (strategy=\"dfs\", n_features=None, operators=None, **kwargs) [source] Generate new features. Create new combinations of existing features to capture the non-linear relations between the original features. See the FeatureGenerator class for a description of the parameters. method feature_grouping (group, name=None, operators=None, drop_columns=True, **kwargs) [source] Extract statistics from similar features. Replace groups of features with related characteristics with new features that summarize statistical properties of te group. The statistical operators are calculated over every row of the group. The group names and features can be accessed through the groups method. See the FeatureGrouper class for a description of the parameters. method feature_selection (strategy=None, solver=None, n_features=None, min_repeated=2, max_repeated=1.0, max_correlation=1.0, **kwargs) [source] Reduce the number of features in the data. Apply feature selection or dimensionality reduction, either to improve the estimators' accuracy or to boost their performance on very high-dimensional datasets. Additionally, remove multicollinear and low variance features. See the FeatureSelector class for a description of the parameters. Note When strategy=\"univariate\" and solver=None, f_classif or f_regression is used as default solver. When strategy is \"sfs\", \"rfecv\" or any of the advanced strategies and no scoring is specified, atom's metric (if it exists) is used as scoring.","title":"Feature engineering"},{"location":"API/ATOM/atomregressor/#training","text":"The training methods are where the models are fitted to the data and their performance is evaluated against selected metric. There are three methods to call the three different training approaches. All relevant attributes and methods from the training classes are attached to atom for convenience. These include the errors, winner and results attributes, as well as the models , and the prediction and plotting methods. Read more in the user guide . run Train and evaluate the models in a direct fashion. successive_halving Fit the models in a successive halving fashion. train_sizing Train and evaluate the models in a train sizing fashion. method run (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, **kwargs) [source] Train and evaluate the models in a direct fashion. Contrary to successive_halving and train_sizing , the direct approach only iterates once over the models, using the full dataset. The following steps are applied to every model: Apply hyperparameter tuning (optional). Fit the model on the training set using the best combination of hyperparameters found. Evaluate the model on the test set. Train the model on various bootstrapped samples of the training set and evaluate again on the test set (optional). See the DirectClassifier or DirectRegressor class for a description of the parameters. method successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_runs=0, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, **kwargs) [source] Fit the models in a successive halving fashion. The successive halving technique is a bandit-based algorithm that fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason, it is recommended to only use this technique with similar models, e.g. only using tree-based models. The following steps are applied to every model (per iteration): Apply hyperparameter tuning (optional). Fit the model on the training set using the best combination of hyperparameters found. Evaluate the model on the test set. Train the model on various bootstrapped samples of the training set and evaluate again on the test set (optional). See the SuccessiveHalvingClassifier or SuccessiveHalvingRegressor class for a description of the parameters. method train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=5, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, **kwargs) [source] Train and evaluate the models in a train sizing fashion. When training models, there is usually a trade-off between model performance and computation time, that is regulated by the number of samples in the training set. This method can be used to create insights in this trade-off, and help determine the optimal size of the training set. The models are fitted multiple times, ever-increasing the number of samples in the training set. The following steps are applied to every model (per iteration): Apply hyperparameter tuning (optional). Fit the model on the training set using the best combination of hyperparameters found. Evaluate the model on the test set. Train the model on various bootstrapped samples of the training set and evaluate again on the test set (optional). See the TrainSizingClassifier or TrainSizingRegressor class for a description of the parameters.","title":"Training"},{"location":"API/data_cleaning/balancer/","text":"Balancer class atom.data_cleaning. Balancer (strategy=\"ADASYN\", n_jobs=1, verbose=0, logger=None, random_state=None, **kwargs) [source] Balance the number of samples per class in the target column. When oversampling, the newly created samples have an increasing integer index for numerical indices, and an index of the form [estimator]_N for non-numerical indices, where N stands for the N-th sample in the data set. Use only for classification tasks. This class can be accessed from atom through the balance method. Read more in the user guide . Warning The clustercentroids estimator is unavailable because of incompatibilities of the APIs. Parameters strategy: str or estimator, default=\"ADASYN\" Type of algorithm with which to balance the dataset. Choose from the name of any estimator in the imbalanced-learn package or provide a custom instance of such. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If <-1: Use number of cores - 1 - value. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState used by np.random . **kwargs Additional keyword arguments for the strategy estimator. Attributes [strategy]: imblearn estimator Object (lowercase strategy) used to balance the data, e.g. balancer.adasyn for the default strategy. mapping: dict Target values mapped to their respective encoded integer. See Also Encoder Perform encoding of categorical features. Imputer Handle missing values in the data. Pruner Prune outliers from the data. Example atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . train ) mean radius mean texture ... worst fractal dimension target 0 18.030 16.85 ... 0.08225 0 1 10.950 21.35 ... 0.09606 0 2 14.250 22.15 ... 0.11320 0 3 17.570 15.05 ... 0.07919 0 4 10.600 18.95 ... 0.07587 1 .. ... ... ... ... ... 451 8.888 14.64 ... 0.10840 1 452 21.090 26.57 ... 0.12840 0 453 16.160 21.54 ... 0.07619 0 454 11.260 19.83 ... 0.07613 1 455 12.000 15.65 ... 0.07924 1 [456 rows x 31 columns] >>> atom . balance ( strategy = \"smote\" , verbose = 2 ) Oversampling with SMOTE... --> Adding 116 samples to class 0. >>> # Note that the number of rows has increased >>> print ( atom . train ) mean radius mean texture ... worst fractal dimension target 0 11.420000 20.380000 ... 0.173000 0 1 9.876000 17.270000 ... 0.073800 1 2 13.470000 14.060000 ... 0.093260 1 3 16.300000 15.700000 ... 0.072300 1 4 12.250000 17.940000 ... 0.081320 1 .. ... ... ... ... ... 567 12.975558 20.580996 ... 0.118509 0 568 11.786135 17.120749 ... 0.091266 0 569 16.194544 19.737215 ... 0.106434 0 570 16.780524 21.261883 ... 0.086889 0 571 20.705316 22.635645 ... 0.085362 0 [572 rows x 31 columns] >>> from atom.data_cleaning import Balancer >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> print ( X ) mean radius mean texture ... worst symmetry worst fractal dimension 0 17.99 10.38 ... 0.4601 0.11890 1 20.57 17.77 ... 0.2750 0.08902 2 19.69 21.25 ... 0.3613 0.08758 3 11.42 20.38 ... 0.6638 0.17300 4 20.29 14.34 ... 0.2364 0.07678 .. ... ... ... ... ... 564 21.56 22.39 ... 0.2060 0.07115 565 20.13 28.25 ... 0.2572 0.06637 566 16.60 28.08 ... 0.2218 0.07820 567 20.60 29.33 ... 0.4087 0.12400 568 7.76 24.54 ... 0.2871 0.07039 [569 rows x 30 columns] >>> balancer = Balancer ( strategy = \"smote\" , verbose = 2 ) >>> X , y = balancer . transform ( X , y ) Oversampling with SMOTE... --> Adding 145 samples to class 0. >>> # Note that the number of rows has increased >>> print ( X ) mean radius mean texture ... worst symmetry worst fractal dimension 0 17.990000 10.380000 ... 0.460100 0.118900 1 20.570000 17.770000 ... 0.275000 0.089020 2 19.690000 21.250000 ... 0.361300 0.087580 3 11.420000 20.380000 ... 0.663800 0.173000 4 20.290000 14.340000 ... 0.236400 0.076780 .. ... ... ... ... ... 709 14.824550 17.497674 ... 0.345200 0.100678 710 20.170649 23.997572 ... 0.538881 0.099281 711 21.006050 22.305044 ... 0.277181 0.076740 712 20.791828 25.103989 ... 0.388202 0.122836 713 17.081185 23.560768 ... 0.342508 0.082558 [714 rows x 30 columns] Methods fit Does nothing. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Balance the data. method fit (X=None, y=None, **fit_params) [source] Does nothing. Implemented for continuity of the API. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=-1) [source] Balance the data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str or sequence, default=-1 Target column corresponding to X. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Balanced dataframe. pd.Series Transformed target column.","title":"Balancer"},{"location":"API/data_cleaning/balancer/#balancer","text":"class atom.data_cleaning. Balancer (strategy=\"ADASYN\", n_jobs=1, verbose=0, logger=None, random_state=None, **kwargs) [source] Balance the number of samples per class in the target column. When oversampling, the newly created samples have an increasing integer index for numerical indices, and an index of the form [estimator]_N for non-numerical indices, where N stands for the N-th sample in the data set. Use only for classification tasks. This class can be accessed from atom through the balance method. Read more in the user guide . Warning The clustercentroids estimator is unavailable because of incompatibilities of the APIs. Parameters strategy: str or estimator, default=\"ADASYN\" Type of algorithm with which to balance the dataset. Choose from the name of any estimator in the imbalanced-learn package or provide a custom instance of such. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If <-1: Use number of cores - 1 - value. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState used by np.random . **kwargs Additional keyword arguments for the strategy estimator. Attributes [strategy]: imblearn estimator Object (lowercase strategy) used to balance the data, e.g. balancer.adasyn for the default strategy. mapping: dict Target values mapped to their respective encoded integer. See Also Encoder Perform encoding of categorical features. Imputer Handle missing values in the data. Pruner Prune outliers from the data.","title":"Balancer"},{"location":"API/data_cleaning/balancer/#example","text":"atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . train ) mean radius mean texture ... worst fractal dimension target 0 18.030 16.85 ... 0.08225 0 1 10.950 21.35 ... 0.09606 0 2 14.250 22.15 ... 0.11320 0 3 17.570 15.05 ... 0.07919 0 4 10.600 18.95 ... 0.07587 1 .. ... ... ... ... ... 451 8.888 14.64 ... 0.10840 1 452 21.090 26.57 ... 0.12840 0 453 16.160 21.54 ... 0.07619 0 454 11.260 19.83 ... 0.07613 1 455 12.000 15.65 ... 0.07924 1 [456 rows x 31 columns] >>> atom . balance ( strategy = \"smote\" , verbose = 2 ) Oversampling with SMOTE... --> Adding 116 samples to class 0. >>> # Note that the number of rows has increased >>> print ( atom . train ) mean radius mean texture ... worst fractal dimension target 0 11.420000 20.380000 ... 0.173000 0 1 9.876000 17.270000 ... 0.073800 1 2 13.470000 14.060000 ... 0.093260 1 3 16.300000 15.700000 ... 0.072300 1 4 12.250000 17.940000 ... 0.081320 1 .. ... ... ... ... ... 567 12.975558 20.580996 ... 0.118509 0 568 11.786135 17.120749 ... 0.091266 0 569 16.194544 19.737215 ... 0.106434 0 570 16.780524 21.261883 ... 0.086889 0 571 20.705316 22.635645 ... 0.085362 0 [572 rows x 31 columns] >>> from atom.data_cleaning import Balancer >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> print ( X ) mean radius mean texture ... worst symmetry worst fractal dimension 0 17.99 10.38 ... 0.4601 0.11890 1 20.57 17.77 ... 0.2750 0.08902 2 19.69 21.25 ... 0.3613 0.08758 3 11.42 20.38 ... 0.6638 0.17300 4 20.29 14.34 ... 0.2364 0.07678 .. ... ... ... ... ... 564 21.56 22.39 ... 0.2060 0.07115 565 20.13 28.25 ... 0.2572 0.06637 566 16.60 28.08 ... 0.2218 0.07820 567 20.60 29.33 ... 0.4087 0.12400 568 7.76 24.54 ... 0.2871 0.07039 [569 rows x 30 columns] >>> balancer = Balancer ( strategy = \"smote\" , verbose = 2 ) >>> X , y = balancer . transform ( X , y ) Oversampling with SMOTE... --> Adding 145 samples to class 0. >>> # Note that the number of rows has increased >>> print ( X ) mean radius mean texture ... worst symmetry worst fractal dimension 0 17.990000 10.380000 ... 0.460100 0.118900 1 20.570000 17.770000 ... 0.275000 0.089020 2 19.690000 21.250000 ... 0.361300 0.087580 3 11.420000 20.380000 ... 0.663800 0.173000 4 20.290000 14.340000 ... 0.236400 0.076780 .. ... ... ... ... ... 709 14.824550 17.497674 ... 0.345200 0.100678 710 20.170649 23.997572 ... 0.538881 0.099281 711 21.006050 22.305044 ... 0.277181 0.076740 712 20.791828 25.103989 ... 0.388202 0.122836 713 17.081185 23.560768 ... 0.342508 0.082558 [714 rows x 30 columns]","title":"Example"},{"location":"API/data_cleaning/balancer/#methods","text":"fit Does nothing. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Balance the data. method fit (X=None, y=None, **fit_params) [source] Does nothing. Implemented for continuity of the API. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=-1) [source] Balance the data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str or sequence, default=-1 Target column corresponding to X. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Balanced dataframe. pd.Series Transformed target column.","title":"Methods"},{"location":"API/data_cleaning/cleaner/","text":"Cleaner class atom.data_cleaning. Cleaner (drop_types=None, strip_categorical=True, drop_duplicates=False, drop_missing_target=True, encode_target=True, device=\"cpu\", engine=\"sklearn\", verbose=0, logger=None) [source] Applies standard data cleaning steps on a dataset. Use the parameters to choose which transformations to perform. The available steps are: Drop columns with specific data types. Strip categorical features from white spaces. Drop duplicate rows. Drop rows with missing values in the target column. Encode the target column. This class can be accessed from atom through the clean method. Read more in the user guide . Parameters drop_types: str, sequence or None, default=None Columns with these data types are dropped from the dataset. strip_categorical: bool, default=True Whether to strip spaces from the categorical columns. drop_duplicates: bool, default=False Whether to drop duplicate rows. Only the first occurrence of every duplicated row is kept. drop_missing_target: bool, default=True Whether to drop rows with missing values in the target column. This transformation is ignored if y is not provided. encode_target: bool, default=True Whether to Label-encode the target column. This transformation is ignored if y is not provided. device: str, default=\"cpu\" Device on which to train the estimators. Use any string that follows the SYCL_DEVICE_FILTER filter selector, e.g. device=\"gpu\" to use the GPU. Read more in the user guide . engine: str, default=\"sklearn\" Execution engine to use for the estimators. Refer to the user guide for an explanation regarding every choice. Choose from: \"sklearn\" (only if device=\"cpu\") \"cuml\" (only if device=\"gpu\") verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. Attributes missing: list Values that are considered \"missing\". Default values are: \"\", \"?\", \"None\", \"NA\", \"nan\", \"NaN\" and \"inf\". Note that None , NaN , +inf and -inf are always considered missing since they are incompatible with sklearn estimators. mapping: dict Target values mapped to their respective encoded integer. Only available if encode_target=True. See Also Encoder Perform encoding of categorical features. Discretizer Bin continuous data into intervals. Scaler Scale the data. Example atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> y = [ \"a\" if i else \"b\" for i in y ] >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . y ) 0 b 1 b 2 b 3 b 4 a .. 995 b 996 a 997 a 998 b 999 b Name: target, Length: 1000, dtype: object >>> atom . clean ( verbose = 2 ) Fitting Cleaner... Cleaning the data... --> Label-encoding the target column. >>> print ( atom . y ) 0 1 1 1 2 1 3 1 4 0 .. 995 1 996 0 997 0 998 1 999 1 Name: target, Length: 1000, dtype: int32 >>> import numpy as np >>> from atom.data_cleaning import Cleaner >>> y = [ \"a\" if i else \"b\" for i in np . randint ( 100 )] >>> cleaner = Cleaner ( verbose = 2 ) >>> y = cleaner . fit_transform ( y = y ) Fitting Cleaner... Cleaning the data... --> Label-encoding the target column. >>> print ( y ) 0 0 1 0 2 1 3 0 4 0 .. 95 1 96 1 97 0 98 0 99 0 Name: target, Length: 100, dtype: int32 Methods fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Inversely transform the label encoding. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Apply the data cleaning steps to the data. method fit (X=None, y=None) [source] Fit to data. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns Cleaner Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Inversely transform the label encoding. This method only inversely transforms the label encoding. The rest of the transformations can't be inverted. If encode_target=False , the data is returned as is. Parameters X: dataframe-like or None, default=None Does nothing. Implemented for continuity of the API. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Unchanged feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X=None, y=None) [source] Apply the data cleaning steps to the data. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Cleaner"},{"location":"API/data_cleaning/cleaner/#cleaner","text":"class atom.data_cleaning. Cleaner (drop_types=None, strip_categorical=True, drop_duplicates=False, drop_missing_target=True, encode_target=True, device=\"cpu\", engine=\"sklearn\", verbose=0, logger=None) [source] Applies standard data cleaning steps on a dataset. Use the parameters to choose which transformations to perform. The available steps are: Drop columns with specific data types. Strip categorical features from white spaces. Drop duplicate rows. Drop rows with missing values in the target column. Encode the target column. This class can be accessed from atom through the clean method. Read more in the user guide . Parameters drop_types: str, sequence or None, default=None Columns with these data types are dropped from the dataset. strip_categorical: bool, default=True Whether to strip spaces from the categorical columns. drop_duplicates: bool, default=False Whether to drop duplicate rows. Only the first occurrence of every duplicated row is kept. drop_missing_target: bool, default=True Whether to drop rows with missing values in the target column. This transformation is ignored if y is not provided. encode_target: bool, default=True Whether to Label-encode the target column. This transformation is ignored if y is not provided. device: str, default=\"cpu\" Device on which to train the estimators. Use any string that follows the SYCL_DEVICE_FILTER filter selector, e.g. device=\"gpu\" to use the GPU. Read more in the user guide . engine: str, default=\"sklearn\" Execution engine to use for the estimators. Refer to the user guide for an explanation regarding every choice. Choose from: \"sklearn\" (only if device=\"cpu\") \"cuml\" (only if device=\"gpu\") verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. Attributes missing: list Values that are considered \"missing\". Default values are: \"\", \"?\", \"None\", \"NA\", \"nan\", \"NaN\" and \"inf\". Note that None , NaN , +inf and -inf are always considered missing since they are incompatible with sklearn estimators. mapping: dict Target values mapped to their respective encoded integer. Only available if encode_target=True. See Also Encoder Perform encoding of categorical features. Discretizer Bin continuous data into intervals. Scaler Scale the data.","title":"Cleaner"},{"location":"API/data_cleaning/cleaner/#example","text":"atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> y = [ \"a\" if i else \"b\" for i in y ] >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . y ) 0 b 1 b 2 b 3 b 4 a .. 995 b 996 a 997 a 998 b 999 b Name: target, Length: 1000, dtype: object >>> atom . clean ( verbose = 2 ) Fitting Cleaner... Cleaning the data... --> Label-encoding the target column. >>> print ( atom . y ) 0 1 1 1 2 1 3 1 4 0 .. 995 1 996 0 997 0 998 1 999 1 Name: target, Length: 1000, dtype: int32 >>> import numpy as np >>> from atom.data_cleaning import Cleaner >>> y = [ \"a\" if i else \"b\" for i in np . randint ( 100 )] >>> cleaner = Cleaner ( verbose = 2 ) >>> y = cleaner . fit_transform ( y = y ) Fitting Cleaner... Cleaning the data... --> Label-encoding the target column. >>> print ( y ) 0 0 1 0 2 1 3 0 4 0 .. 95 1 96 1 97 0 98 0 99 0 Name: target, Length: 100, dtype: int32","title":"Example"},{"location":"API/data_cleaning/cleaner/#methods","text":"fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Inversely transform the label encoding. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Apply the data cleaning steps to the data. method fit (X=None, y=None) [source] Fit to data. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns Cleaner Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Inversely transform the label encoding. This method only inversely transforms the label encoding. The rest of the transformations can't be inverted. If encode_target=False , the data is returned as is. Parameters X: dataframe-like or None, default=None Does nothing. Implemented for continuity of the API. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Unchanged feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X=None, y=None) [source] Apply the data cleaning steps to the data. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/data_cleaning/discretizer/","text":"Discretizer class atom.data_cleaning. Discretizer (strategy=\"quantile\", bins=5, labels=None, device=\"cpu\", engine=\"sklearn\", verbose=0, logger=None, random_state=None) [source] Bin continuous data into intervals. For each feature, the bin edges are computed during fit and, together with the number of bins, they define the intervals. Ignores categorical columns. This class can be accessed from atom through the discretize method. Read more in the user guide . Tip The transformation returns categorical columns. Use the Encoder class to convert them back to numerical types. Parameters strategy: str, default=\"quantile\" Strategy used to define the widths of the bins. Choose from: \"uniform\": All bins have identical widths. \"quantile\": All bins have the same number of points. \"kmeans\": Values in each bin have the same nearest center of a 1D k-means cluster. \"custom\": Use custom bin edges provided through bins . bins: int, sequence or dict, default=5 Bin number or bin edges in which to split every column. If int: Number of bins to produce for all columns. Only for strategy!=\"custom\". If sequence: For strategy!=\"custom\": Number of bins per column, allowing for non-uniform width. The n-th value corresponds to the n-th column that is transformed. Note that categorical columns are automatically ignored. For strategy=\"custom\": Bin edges with length=n_bins - 1. The outermost edges are always -inf and +inf , e.g. bins [1, 2] indicate (-inf, 1], (1, 2], (2, inf] . If dict: One of the aforementioned options per column, where the key is the column's name. labels: sequence, dict or None, default=None Label names with which to replace the binned intervals. If None: Use default labels of the form (min_edge, max_edge] . If sequence: Labels to use for all columns. If dict: Labels per column, where the key is the column's name. device: str, default=\"cpu\" Device on which to train the estimators. Use any string that follows the SYCL_DEVICE_FILTER filter selector, e.g. device=\"gpu\" to use the GPU. Read more in the user guide . engine: str, default=\"sklearn\" Execution engine to use for the estimators. Refer to the user guide for an explanation regarding every choice. Choose from: \"sklearn\" (only if device=\"cpu\") \"cuml\" (only if device=\"gpu\") verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState used by np.random . Only for strategy=\"quantile\". Attributes feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also Encoder Perform encoding of categorical features. Imputer Handle missing values in the data. Normalizer Transform the data to follow a Normal/Gaussian distribution. Example atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> atom = ATOMClassifier ( X , y ) >>> print ( atom [ \"mean radius\" ]) 0 17.99 1 20.57 2 19.69 3 11.42 4 20.29 ... 564 21.56 565 20.13 566 16.60 567 20.60 568 7.76 Name: mean radius, Length: 569, dtype: float64 >>> atom . discretize ( ... strategy = \"custom\" , ... bins = [ 13 , 18 ], ... labels = [ \"small\" , \"medium\" , \"large\" ], ... verbose = 2 , ... columns = \"mean radius\" , ... ) Fitting Discretizer... Binning the features... --> Discretizing feature mean radius in 3 bins. >>> print ( atom [ \"mean radius\" ]) 0 small 1 medium 2 medium 3 medium 4 small ... 564 large 565 small 566 large 567 small 568 small Name: mean radius, Length: 569, dtype: category Categories (3, object): ['small' < 'medium' < 'large'] >>> from atom.data_cleaning import Discretizer >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> print ( X [ \"mean radius\" ]) 0 17.99 1 20.57 2 19.69 3 11.42 4 20.29 ... 564 21.56 565 20.13 566 16.60 567 20.60 568 7.76 Name: mean radius, Length: 569, dtype: float64 >>> disc = Discretizer ( ... strategy = \"custom\" , ... bins = [ 13 , 18 ], ... labels = [ \"small\" , \"medium\" , \"large\" ], ... verbose = 2 , ... ) >>> X [ \"mean radius\" ] = disc . fit_transform ( X [[ \"mean radius\" ]])[ \"mean radius\" ] Fitting Discretizer... Binning the features... --> Discretizing feature mean radius in 3 bins. >>> print ( X [ \"mean radius\" ]) 0 small 1 medium 2 medium 3 medium 4 small ... 564 large 565 small 566 large 567 small 568 small Name: mean radius, Length: 569, dtype: category Categories (3, object): ['small' < 'medium' < 'large'] Methods fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Bin the data into intervals. method fit (X, y=None) [source] Fit to data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns Discretizer Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Bin the data into intervals. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed feature set.","title":"Discretizer"},{"location":"API/data_cleaning/discretizer/#discretizer","text":"class atom.data_cleaning. Discretizer (strategy=\"quantile\", bins=5, labels=None, device=\"cpu\", engine=\"sklearn\", verbose=0, logger=None, random_state=None) [source] Bin continuous data into intervals. For each feature, the bin edges are computed during fit and, together with the number of bins, they define the intervals. Ignores categorical columns. This class can be accessed from atom through the discretize method. Read more in the user guide . Tip The transformation returns categorical columns. Use the Encoder class to convert them back to numerical types. Parameters strategy: str, default=\"quantile\" Strategy used to define the widths of the bins. Choose from: \"uniform\": All bins have identical widths. \"quantile\": All bins have the same number of points. \"kmeans\": Values in each bin have the same nearest center of a 1D k-means cluster. \"custom\": Use custom bin edges provided through bins . bins: int, sequence or dict, default=5 Bin number or bin edges in which to split every column. If int: Number of bins to produce for all columns. Only for strategy!=\"custom\". If sequence: For strategy!=\"custom\": Number of bins per column, allowing for non-uniform width. The n-th value corresponds to the n-th column that is transformed. Note that categorical columns are automatically ignored. For strategy=\"custom\": Bin edges with length=n_bins - 1. The outermost edges are always -inf and +inf , e.g. bins [1, 2] indicate (-inf, 1], (1, 2], (2, inf] . If dict: One of the aforementioned options per column, where the key is the column's name. labels: sequence, dict or None, default=None Label names with which to replace the binned intervals. If None: Use default labels of the form (min_edge, max_edge] . If sequence: Labels to use for all columns. If dict: Labels per column, where the key is the column's name. device: str, default=\"cpu\" Device on which to train the estimators. Use any string that follows the SYCL_DEVICE_FILTER filter selector, e.g. device=\"gpu\" to use the GPU. Read more in the user guide . engine: str, default=\"sklearn\" Execution engine to use for the estimators. Refer to the user guide for an explanation regarding every choice. Choose from: \"sklearn\" (only if device=\"cpu\") \"cuml\" (only if device=\"gpu\") verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState used by np.random . Only for strategy=\"quantile\". Attributes feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also Encoder Perform encoding of categorical features. Imputer Handle missing values in the data. Normalizer Transform the data to follow a Normal/Gaussian distribution.","title":"Discretizer"},{"location":"API/data_cleaning/discretizer/#example","text":"atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> atom = ATOMClassifier ( X , y ) >>> print ( atom [ \"mean radius\" ]) 0 17.99 1 20.57 2 19.69 3 11.42 4 20.29 ... 564 21.56 565 20.13 566 16.60 567 20.60 568 7.76 Name: mean radius, Length: 569, dtype: float64 >>> atom . discretize ( ... strategy = \"custom\" , ... bins = [ 13 , 18 ], ... labels = [ \"small\" , \"medium\" , \"large\" ], ... verbose = 2 , ... columns = \"mean radius\" , ... ) Fitting Discretizer... Binning the features... --> Discretizing feature mean radius in 3 bins. >>> print ( atom [ \"mean radius\" ]) 0 small 1 medium 2 medium 3 medium 4 small ... 564 large 565 small 566 large 567 small 568 small Name: mean radius, Length: 569, dtype: category Categories (3, object): ['small' < 'medium' < 'large'] >>> from atom.data_cleaning import Discretizer >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> print ( X [ \"mean radius\" ]) 0 17.99 1 20.57 2 19.69 3 11.42 4 20.29 ... 564 21.56 565 20.13 566 16.60 567 20.60 568 7.76 Name: mean radius, Length: 569, dtype: float64 >>> disc = Discretizer ( ... strategy = \"custom\" , ... bins = [ 13 , 18 ], ... labels = [ \"small\" , \"medium\" , \"large\" ], ... verbose = 2 , ... ) >>> X [ \"mean radius\" ] = disc . fit_transform ( X [[ \"mean radius\" ]])[ \"mean radius\" ] Fitting Discretizer... Binning the features... --> Discretizing feature mean radius in 3 bins. >>> print ( X [ \"mean radius\" ]) 0 small 1 medium 2 medium 3 medium 4 small ... 564 large 565 small 566 large 567 small 568 small Name: mean radius, Length: 569, dtype: category Categories (3, object): ['small' < 'medium' < 'large']","title":"Example"},{"location":"API/data_cleaning/discretizer/#methods","text":"fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Bin the data into intervals. method fit (X, y=None) [source] Fit to data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns Discretizer Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Bin the data into intervals. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed feature set.","title":"Methods"},{"location":"API/data_cleaning/encoder/","text":"Encoder class atom.data_cleaning. Encoder (strategy=\"LeaveOneOut\", max_onehot=10, ordinal=None, rare_to_value=None, value=\"rare\", verbose=0, logger=None, **kwargs) [source] Perform encoding of categorical features. The encoding type depends on the number of classes in the column: If n_classes=2 or ordinal feature, use Ordinal-encoding. If 2 < n_classes <= max_onehot , use OneHot-encoding. If n_classes > max_onehot , use strategy -encoding. Missing values are propagated to the output column. Unknown classes encountered during transforming are imputed according to the selected strategy. Rare classes can be replaced with a value in order to prevent too high cardinality. This class can be accessed from atom through the encode method. Read more in the user guide . Warning Two category-encoders estimators are unavailable: OneHotEncoder : Use the max_onehot parameter. HashingEncoder : Incompatibility of APIs. Parameters strategy: str or estimator, default=\"LeaveOneOut\" Type of encoding to use for high cardinality features. Choose from any of the estimators in the category-encoders package or provide a custom one. max_onehot: int or None, default=10 Maximum number of unique values in a feature to perform one-hot encoding. If None, strategy -encoding is always used for columns with more than two classes. ordinal: dict or None, default=None Order of ordinal features, where the dict key is the feature's name and the value is the class order, e.g. {\"salary\": [\"low\", \"medium\", \"high\"]} . rare_to_value: int, float or None, default=None Replaces rare class occurrences in categorical columns with the string in parameter value . This transformation is done before the encoding of the column. If None: Skip this step. If int: Minimum number of occurrences in a class. If float: Minimum fraction of occurrences in a class. value: str, default=\"rare\" Value with which to replace rare classes. This parameter is ignored if rare_to_value=None . verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. **kwargs Additional keyword arguments for the strategy estimator. Attributes mapping: dict of dicts Encoded values and their respective mapping. The column name is the key to its mapping dictionary. Only for columns mapped to a single column (e.g. Ordinal, Leave-one-out, etc...). feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also Cleaner Applies standard data cleaning steps on a dataset. Imputer Handle missing values in the data. Pruner Prune outliers from the data. Example atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> from numpy.random import randint >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X [ \"cat_feature_1\" ] = [ f \"x { i } \" for i in randint ( 0 , 2 , len ( X ))] >>> X [ \"cat_feature_2\" ] = [ f \"x { i } \" for i in randint ( 0 , 3 , len ( X ))] >>> X [ \"cat_feature_3\" ] = [ f \"x { i } \" for i in randint ( 0 , 20 , len ( X ))] >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . X ) mean radius mean texture ... cat_feature_2 cat_feature_3 0 13.62 23.23 ... x0 x0 1 14.86 16.94 ... x0 x5 2 16.74 21.59 ... x2 x15 3 13.37 16.39 ... x1 x18 4 11.37 18.89 ... x0 x13 .. ... ... ... ... ... 564 14.06 17.18 ... x2 x1 565 11.29 13.04 ... x0 x10 566 14.26 19.65 ... x0 x5 567 12.05 14.63 ... x2 x14 568 18.81 19.98 ... x1 x13 [569 rows x 33 columns] >>> atom . encode ( strategy = \"leaveoneout\" , max_onehot = 10 , verbose = 2 ) Fitting Encoder... Encoding categorical columns... --> Ordinal-encoding feature cat_feature_1. Contains 2 classes. --> OneHot-encoding feature cat_feature_2. Contains 3 classes. --> LeaveOneOut-encoding feature cat_feature_3. Contains 20 classes. >>> # Note the one-hot encoded column with name [feature]_[class] >>> print ( atom . X ) mean radius mean texture ... cat_feature_2_x2 cat_feature_3 0 13.62 23.23 ... 0.0 0.714286 1 14.86 16.94 ... 0.0 0.555556 2 16.74 21.59 ... 1.0 0.681818 3 13.37 16.39 ... 0.0 0.739130 4 11.37 18.89 ... 0.0 0.521739 .. ... ... ... ... ... 564 14.06 17.18 ... 1.0 0.772727 565 11.29 13.04 ... 0.0 0.766667 566 14.26 19.65 ... 0.0 0.555556 567 12.05 14.63 ... 1.0 0.411765 568 18.81 19.98 ... 0.0 0.521739 [569 rows x 35 columns] >>> from atom.data_cleaning import Encoder >>> from sklearn.datasets import load_breast_cancer >>> from numpy.random import randint >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X [ \"cat_feature_1\" ] = [ f \"x { i } \" for i in randint ( 0 , 2 , len ( X ))] >>> X [ \"cat_feature_2\" ] = [ f \"x { i } \" for i in randint ( 0 , 3 , len ( X ))] >>> X [ \"cat_feature_3\" ] = [ f \"x { i } \" for i in randint ( 0 , 20 , len ( X ))] >>> print ( X ) mean radius mean texture ... cat_feature_2 cat_feature_3 0 13.62 23.23 ... x0 x0 1 14.86 16.94 ... x0 x5 2 16.74 21.59 ... x2 x15 3 13.37 16.39 ... x1 x18 4 11.37 18.89 ... x0 x13 .. ... ... ... ... ... 564 14.06 17.18 ... x2 x1 565 11.29 13.04 ... x0 x10 566 14.26 19.65 ... x0 x5 567 12.05 14.63 ... x2 x14 568 18.81 19.98 ... x1 x13 [569 rows x 33 columns] >>> encoder = Encoder ( strategy = \"leaveoneout\" , max_onehot = 10 , verbose = 2 ) >>> X = encoder . fit_transform ( X , y ) Fitting Encoder... Encoding categorical columns... --> Ordinal-encoding feature cat_feature_1. Contains 2 classes. --> OneHot-encoding feature cat_feature_2. Contains 3 classes. --> LeaveOneOut-encoding feature cat_feature_3. Contains 20 classes. >>> # Note the one-hot encoded column with name [feature]_[class] >>> print ( X ) mean radius mean texture ... cat_feature_2_x2 cat_feature_3 0 17.99 10.38 ... 1.0 0.379310 1 20.57 17.77 ... 1.0 0.714286 2 19.69 21.25 ... 0.0 0.586207 3 11.42 20.38 ... 0.0 0.678571 4 20.29 14.34 ... 0.0 0.714286 .. ... ... ... ... ... 564 21.56 22.39 ... 0.0 0.580645 565 20.13 28.25 ... 0.0 0.518519 566 16.60 28.08 ... 1.0 0.600000 567 20.60 29.33 ... 1.0 0.586207 568 7.76 24.54 ... 1.0 0.678571 [569 rows x 35 columns] Methods fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Encode the data. method fit (X, y=None) [source] Fit to data. Note that leaving y=None can lead to errors if the strategy encoder requires target values. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str or sequence Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns Encoder Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Encode the data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Encoded dataframe.","title":"Encoder"},{"location":"API/data_cleaning/encoder/#encoder","text":"class atom.data_cleaning. Encoder (strategy=\"LeaveOneOut\", max_onehot=10, ordinal=None, rare_to_value=None, value=\"rare\", verbose=0, logger=None, **kwargs) [source] Perform encoding of categorical features. The encoding type depends on the number of classes in the column: If n_classes=2 or ordinal feature, use Ordinal-encoding. If 2 < n_classes <= max_onehot , use OneHot-encoding. If n_classes > max_onehot , use strategy -encoding. Missing values are propagated to the output column. Unknown classes encountered during transforming are imputed according to the selected strategy. Rare classes can be replaced with a value in order to prevent too high cardinality. This class can be accessed from atom through the encode method. Read more in the user guide . Warning Two category-encoders estimators are unavailable: OneHotEncoder : Use the max_onehot parameter. HashingEncoder : Incompatibility of APIs. Parameters strategy: str or estimator, default=\"LeaveOneOut\" Type of encoding to use for high cardinality features. Choose from any of the estimators in the category-encoders package or provide a custom one. max_onehot: int or None, default=10 Maximum number of unique values in a feature to perform one-hot encoding. If None, strategy -encoding is always used for columns with more than two classes. ordinal: dict or None, default=None Order of ordinal features, where the dict key is the feature's name and the value is the class order, e.g. {\"salary\": [\"low\", \"medium\", \"high\"]} . rare_to_value: int, float or None, default=None Replaces rare class occurrences in categorical columns with the string in parameter value . This transformation is done before the encoding of the column. If None: Skip this step. If int: Minimum number of occurrences in a class. If float: Minimum fraction of occurrences in a class. value: str, default=\"rare\" Value with which to replace rare classes. This parameter is ignored if rare_to_value=None . verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. **kwargs Additional keyword arguments for the strategy estimator. Attributes mapping: dict of dicts Encoded values and their respective mapping. The column name is the key to its mapping dictionary. Only for columns mapped to a single column (e.g. Ordinal, Leave-one-out, etc...). feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also Cleaner Applies standard data cleaning steps on a dataset. Imputer Handle missing values in the data. Pruner Prune outliers from the data.","title":"Encoder"},{"location":"API/data_cleaning/encoder/#example","text":"atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> from numpy.random import randint >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X [ \"cat_feature_1\" ] = [ f \"x { i } \" for i in randint ( 0 , 2 , len ( X ))] >>> X [ \"cat_feature_2\" ] = [ f \"x { i } \" for i in randint ( 0 , 3 , len ( X ))] >>> X [ \"cat_feature_3\" ] = [ f \"x { i } \" for i in randint ( 0 , 20 , len ( X ))] >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . X ) mean radius mean texture ... cat_feature_2 cat_feature_3 0 13.62 23.23 ... x0 x0 1 14.86 16.94 ... x0 x5 2 16.74 21.59 ... x2 x15 3 13.37 16.39 ... x1 x18 4 11.37 18.89 ... x0 x13 .. ... ... ... ... ... 564 14.06 17.18 ... x2 x1 565 11.29 13.04 ... x0 x10 566 14.26 19.65 ... x0 x5 567 12.05 14.63 ... x2 x14 568 18.81 19.98 ... x1 x13 [569 rows x 33 columns] >>> atom . encode ( strategy = \"leaveoneout\" , max_onehot = 10 , verbose = 2 ) Fitting Encoder... Encoding categorical columns... --> Ordinal-encoding feature cat_feature_1. Contains 2 classes. --> OneHot-encoding feature cat_feature_2. Contains 3 classes. --> LeaveOneOut-encoding feature cat_feature_3. Contains 20 classes. >>> # Note the one-hot encoded column with name [feature]_[class] >>> print ( atom . X ) mean radius mean texture ... cat_feature_2_x2 cat_feature_3 0 13.62 23.23 ... 0.0 0.714286 1 14.86 16.94 ... 0.0 0.555556 2 16.74 21.59 ... 1.0 0.681818 3 13.37 16.39 ... 0.0 0.739130 4 11.37 18.89 ... 0.0 0.521739 .. ... ... ... ... ... 564 14.06 17.18 ... 1.0 0.772727 565 11.29 13.04 ... 0.0 0.766667 566 14.26 19.65 ... 0.0 0.555556 567 12.05 14.63 ... 1.0 0.411765 568 18.81 19.98 ... 0.0 0.521739 [569 rows x 35 columns] >>> from atom.data_cleaning import Encoder >>> from sklearn.datasets import load_breast_cancer >>> from numpy.random import randint >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X [ \"cat_feature_1\" ] = [ f \"x { i } \" for i in randint ( 0 , 2 , len ( X ))] >>> X [ \"cat_feature_2\" ] = [ f \"x { i } \" for i in randint ( 0 , 3 , len ( X ))] >>> X [ \"cat_feature_3\" ] = [ f \"x { i } \" for i in randint ( 0 , 20 , len ( X ))] >>> print ( X ) mean radius mean texture ... cat_feature_2 cat_feature_3 0 13.62 23.23 ... x0 x0 1 14.86 16.94 ... x0 x5 2 16.74 21.59 ... x2 x15 3 13.37 16.39 ... x1 x18 4 11.37 18.89 ... x0 x13 .. ... ... ... ... ... 564 14.06 17.18 ... x2 x1 565 11.29 13.04 ... x0 x10 566 14.26 19.65 ... x0 x5 567 12.05 14.63 ... x2 x14 568 18.81 19.98 ... x1 x13 [569 rows x 33 columns] >>> encoder = Encoder ( strategy = \"leaveoneout\" , max_onehot = 10 , verbose = 2 ) >>> X = encoder . fit_transform ( X , y ) Fitting Encoder... Encoding categorical columns... --> Ordinal-encoding feature cat_feature_1. Contains 2 classes. --> OneHot-encoding feature cat_feature_2. Contains 3 classes. --> LeaveOneOut-encoding feature cat_feature_3. Contains 20 classes. >>> # Note the one-hot encoded column with name [feature]_[class] >>> print ( X ) mean radius mean texture ... cat_feature_2_x2 cat_feature_3 0 17.99 10.38 ... 1.0 0.379310 1 20.57 17.77 ... 1.0 0.714286 2 19.69 21.25 ... 0.0 0.586207 3 11.42 20.38 ... 0.0 0.678571 4 20.29 14.34 ... 0.0 0.714286 .. ... ... ... ... ... 564 21.56 22.39 ... 0.0 0.580645 565 20.13 28.25 ... 0.0 0.518519 566 16.60 28.08 ... 1.0 0.600000 567 20.60 29.33 ... 1.0 0.586207 568 7.76 24.54 ... 1.0 0.678571 [569 rows x 35 columns]","title":"Example"},{"location":"API/data_cleaning/encoder/#methods","text":"fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Encode the data. method fit (X, y=None) [source] Fit to data. Note that leaving y=None can lead to errors if the strategy encoder requires target values. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str or sequence Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns Encoder Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Encode the data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Encoded dataframe.","title":"Methods"},{"location":"API/data_cleaning/imputer/","text":"Imputer class atom.data_cleaning. Imputer (strat_num=\"drop\", strat_cat=\"drop\", max_nan_rows=None, max_nan_cols=None, device=\"cpu\", engine=\"sklearn\", verbose=0, logger=None) [source] Handle missing values in the data. Impute or remove missing values according to the selected strategy. Also removes rows and columns with too many missing values. Use the missing attribute to customize what are considered \"missing values\". This class can be accessed from atom through the impute method. Read more in the user guide . Parameters strat_num: str, int or float, default=\"drop\" Imputing strategy for numerical columns. Choose from: \"drop\": Drop rows containing missing values. \"mean\": Impute with mean of column. \"median\": Impute with median of column. \"knn\": Impute using a K-Nearest Neighbors approach. \"most_frequent\": Impute with most frequent value. int or float: Impute with provided numerical value. strat_cat: str, default=\"drop\" Imputing strategy for categorical columns. Choose from: \"drop\": Drop rows containing missing values. \"most_frequent\": Impute with most frequent value. str: Impute with provided string. max_nan_rows: int, float or None, default=None Maximum number or fraction of missing values in a row (if more, the row is removed). If None, ignore this step. max_nan_cols: int, float or None, default=None Maximum number or fraction of missing values in a column (if more, the column is removed). If None, ignore this step. device: str, default=\"cpu\" Device on which to train the estimators. Use any string that follows the SYCL_DEVICE_FILTER filter selector, e.g. device=\"gpu\" to use the GPU. Read more in the user guide . engine: str, default=\"sklearn\" Execution engine to use for the estimators. Refer to the user guide for an explanation regarding every choice. Choose from: \"sklearn\" (only if device=\"cpu\") \"cuml\" (only if device=\"gpu\") verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. Attributes missing: list Values that are considered \"missing\". Default values are: \"\", \"?\", \"None\", \"NA\", \"nan\", \"NaN\" and \"inf\". Note that None , NaN , +inf and -inf are always considered missing since they are incompatible with sklearn estimators. feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also Balancer Balance the number of samples per class in the target column. Discretizer Bin continuous data into intervals. Encoder Perform encoding of categorical features. Example atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> from numpy.random import randint >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> # Add some random missing values to the data >>> for i , j in zip ( randint ( 0 , X . shape [ 0 ], 600 ), randint ( 0 , 4 , 600 ]) >>> X . iloc [ i , j ] = np . nan >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . nans ) mean radius 118 mean texture 134 mean perimeter 135 mean area 140 dtype: int64 >>> atom . impute ( strat_num = \"median\" , max_nan_rows = 0.1 , verbose = 2 ) Fitting Imputer... Imputing missing values... --> Dropping 3 samples for containing more than 3 missing values. --> Imputing 115 missing values with median (13.3) in feature mean radius. --> Imputing 131 missing values with median (18.8) in feature mean texture. --> Imputing 132 missing values with median (85.86) in feature mean perimeter. --> Imputing 137 missing values with median (561.3) in feature mean area. >>> print ( atom . n_nans ) 0 >>> from atom.data_cleaning import Imputer >>> from sklearn.datasets import load_breast_cancer >>> from numpy.random import randint >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> # Add some random missing values to the data >>> for i , j in zip ( randint ( 0 , X . shape [ 0 ], 600 ), randint ( 0 , 4 , 600 ]) >>> X . iloc [ i , j ] = np . nan mean radius mean texture ... worst symmetry worst fractal dimension 0 17.99 NaN ... 0.4601 0.11890 1 20.57 17.77 ... 0.2750 0.08902 2 19.69 21.25 ... 0.3613 0.08758 3 NaN 20.38 ... 0.6638 0.17300 4 NaN 14.34 ... 0.2364 0.07678 .. ... ... ... ... ... 564 NaN 22.39 ... 0.2060 0.07115 565 20.13 28.25 ... 0.2572 0.06637 566 NaN NaN ... 0.2218 0.07820 567 NaN 29.33 ... 0.4087 0.12400 568 NaN 24.54 ... 0.2871 0.07039 [569 rows x 30 columns] >>> imputer = Imputer ( strat_num = \"median\" , max_nan_rows = 0.1 , verbose = 2 ) >>> X , y = imputer . fit_transform ( X , y ) Fitting Imputer... Imputing missing values... --> Imputing 135 missing values with median (13.42) in feature mean radius. --> Imputing 133 missing values with median (18.81) in feature mean texture. --> Imputing 129 missing values with median (86.14) in feature mean perimeter. --> Imputing 120 missing values with median (537.9) in feature mean area. >>> print ( X ) mean radius mean texture ... worst symmetry worst fractal dimension 0 17.990 10.38 ... 0.4601 0.11890 1 13.415 17.77 ... 0.2750 0.08902 2 19.690 21.25 ... 0.3613 0.08758 3 11.420 20.38 ... 0.6638 0.17300 4 20.290 14.34 ... 0.2364 0.07678 .. ... ... ... ... ... 564 21.560 22.39 ... 0.2060 0.07115 565 20.130 28.25 ... 0.2572 0.06637 566 13.415 28.08 ... 0.2218 0.07820 567 13.415 18.81 ... 0.4087 0.12400 568 7.760 24.54 ... 0.2871 0.07039 [569 rows x 30 columns] Methods fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Impute the missing values. method fit (X, y=None) [source] Fit to data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns Imputer Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Impute the missing values. Note that leaving y=None can lead to inconsistencies in data length between X and y if rows are dropped during the transformation. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Imputed dataframe. pd.Series Transformed target column. Only returned if provided.","title":"Imputer"},{"location":"API/data_cleaning/imputer/#imputer","text":"class atom.data_cleaning. Imputer (strat_num=\"drop\", strat_cat=\"drop\", max_nan_rows=None, max_nan_cols=None, device=\"cpu\", engine=\"sklearn\", verbose=0, logger=None) [source] Handle missing values in the data. Impute or remove missing values according to the selected strategy. Also removes rows and columns with too many missing values. Use the missing attribute to customize what are considered \"missing values\". This class can be accessed from atom through the impute method. Read more in the user guide . Parameters strat_num: str, int or float, default=\"drop\" Imputing strategy for numerical columns. Choose from: \"drop\": Drop rows containing missing values. \"mean\": Impute with mean of column. \"median\": Impute with median of column. \"knn\": Impute using a K-Nearest Neighbors approach. \"most_frequent\": Impute with most frequent value. int or float: Impute with provided numerical value. strat_cat: str, default=\"drop\" Imputing strategy for categorical columns. Choose from: \"drop\": Drop rows containing missing values. \"most_frequent\": Impute with most frequent value. str: Impute with provided string. max_nan_rows: int, float or None, default=None Maximum number or fraction of missing values in a row (if more, the row is removed). If None, ignore this step. max_nan_cols: int, float or None, default=None Maximum number or fraction of missing values in a column (if more, the column is removed). If None, ignore this step. device: str, default=\"cpu\" Device on which to train the estimators. Use any string that follows the SYCL_DEVICE_FILTER filter selector, e.g. device=\"gpu\" to use the GPU. Read more in the user guide . engine: str, default=\"sklearn\" Execution engine to use for the estimators. Refer to the user guide for an explanation regarding every choice. Choose from: \"sklearn\" (only if device=\"cpu\") \"cuml\" (only if device=\"gpu\") verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. Attributes missing: list Values that are considered \"missing\". Default values are: \"\", \"?\", \"None\", \"NA\", \"nan\", \"NaN\" and \"inf\". Note that None , NaN , +inf and -inf are always considered missing since they are incompatible with sklearn estimators. feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also Balancer Balance the number of samples per class in the target column. Discretizer Bin continuous data into intervals. Encoder Perform encoding of categorical features.","title":"Imputer"},{"location":"API/data_cleaning/imputer/#example","text":"atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> from numpy.random import randint >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> # Add some random missing values to the data >>> for i , j in zip ( randint ( 0 , X . shape [ 0 ], 600 ), randint ( 0 , 4 , 600 ]) >>> X . iloc [ i , j ] = np . nan >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . nans ) mean radius 118 mean texture 134 mean perimeter 135 mean area 140 dtype: int64 >>> atom . impute ( strat_num = \"median\" , max_nan_rows = 0.1 , verbose = 2 ) Fitting Imputer... Imputing missing values... --> Dropping 3 samples for containing more than 3 missing values. --> Imputing 115 missing values with median (13.3) in feature mean radius. --> Imputing 131 missing values with median (18.8) in feature mean texture. --> Imputing 132 missing values with median (85.86) in feature mean perimeter. --> Imputing 137 missing values with median (561.3) in feature mean area. >>> print ( atom . n_nans ) 0 >>> from atom.data_cleaning import Imputer >>> from sklearn.datasets import load_breast_cancer >>> from numpy.random import randint >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> # Add some random missing values to the data >>> for i , j in zip ( randint ( 0 , X . shape [ 0 ], 600 ), randint ( 0 , 4 , 600 ]) >>> X . iloc [ i , j ] = np . nan mean radius mean texture ... worst symmetry worst fractal dimension 0 17.99 NaN ... 0.4601 0.11890 1 20.57 17.77 ... 0.2750 0.08902 2 19.69 21.25 ... 0.3613 0.08758 3 NaN 20.38 ... 0.6638 0.17300 4 NaN 14.34 ... 0.2364 0.07678 .. ... ... ... ... ... 564 NaN 22.39 ... 0.2060 0.07115 565 20.13 28.25 ... 0.2572 0.06637 566 NaN NaN ... 0.2218 0.07820 567 NaN 29.33 ... 0.4087 0.12400 568 NaN 24.54 ... 0.2871 0.07039 [569 rows x 30 columns] >>> imputer = Imputer ( strat_num = \"median\" , max_nan_rows = 0.1 , verbose = 2 ) >>> X , y = imputer . fit_transform ( X , y ) Fitting Imputer... Imputing missing values... --> Imputing 135 missing values with median (13.42) in feature mean radius. --> Imputing 133 missing values with median (18.81) in feature mean texture. --> Imputing 129 missing values with median (86.14) in feature mean perimeter. --> Imputing 120 missing values with median (537.9) in feature mean area. >>> print ( X ) mean radius mean texture ... worst symmetry worst fractal dimension 0 17.990 10.38 ... 0.4601 0.11890 1 13.415 17.77 ... 0.2750 0.08902 2 19.690 21.25 ... 0.3613 0.08758 3 11.420 20.38 ... 0.6638 0.17300 4 20.290 14.34 ... 0.2364 0.07678 .. ... ... ... ... ... 564 21.560 22.39 ... 0.2060 0.07115 565 20.130 28.25 ... 0.2572 0.06637 566 13.415 28.08 ... 0.2218 0.07820 567 13.415 18.81 ... 0.4087 0.12400 568 7.760 24.54 ... 0.2871 0.07039 [569 rows x 30 columns]","title":"Example"},{"location":"API/data_cleaning/imputer/#methods","text":"fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Impute the missing values. method fit (X, y=None) [source] Fit to data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns Imputer Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Impute the missing values. Note that leaving y=None can lead to inconsistencies in data length between X and y if rows are dropped during the transformation. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Imputed dataframe. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/data_cleaning/normalizer/","text":"Normalizer class atom.data_cleaning. Normalizer (strategy=\"yeojohnson\", verbose=0, logger=None, random_state=None, **kwargs) [source] Transform the data to follow a Normal/Gaussian distribution. This transformation is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Missing values are disregarded in fit and maintained in transform. Categorical columns are ignored. This class can be accessed from atom through the normalize method. Read more in the user guide . Warning The quantile strategy performs a non-linear transformation. This may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable. Note The yeojohnson and boxcox strategies scale the data after transforming. Use the kwargs to change this behaviour. Parameters strategy: str, default=\"yeojohnson\" The transforming strategy. Choose from: \" yeojohnson \" \" boxcox \" (only works with strictly positive values) \" quantile \": Transform features using quantiles information. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. random_state: int or None, default=None Seed used by the quantile strategy. If None, the random number generator is the RandomState used by np.random . **kwargs Additional keyword arguments for the strategy estimator. Attributes [strategy]: sklearn transformer Object with which the data is transformed. feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. NaN , +inf and -inf are always considered missing since they are incompatible with sklearn estimators. See Also Cleaner Applies standard data cleaning steps on a dataset. Pruner Prune outliers from the data. Scaler Scale the data. Example atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . dataset ) mean radius mean texture ... worst fractal dimension target 0 16.78 18.80 ... 0.07228 0 1 15.34 14.26 ... 0.09946 0 2 14.22 27.85 ... 0.07796 1 3 18.31 18.58 ... 0.06938 0 4 18.49 17.52 ... 0.09445 0 .. ... ... ... ... ... 564 13.44 21.58 ... 0.07146 0 565 20.47 20.67 ... 0.06386 0 566 12.98 19.35 ... 0.09166 1 567 14.61 15.69 ... 0.05695 1 568 23.27 22.04 ... 0.09187 0 [569 rows x 31 columns] >>> atom . plot_distribution ( columns = 0 ) >>> atom . normalize ( verbose = 2 ) Fitting Normalizer... Normalizing features... >>> print ( atom . dataset ) mean radius mean texture ... worst fractal dimension target 0 0.868700 0.010820 ... -0.684572 0 1 0.513904 -1.257343 ... 1.019875 0 2 0.200435 1.773390 ... -0.226619 1 3 1.197448 -0.042755 ... -0.945047 0 4 1.233326 -0.310726 ... 0.786014 0 .. ... ... ... ... ... 564 -0.041166 0.635293 ... -0.756291 0 565 1.595052 0.440855 ... -1.497202 0 566 -0.193933 0.141884 ... 0.642613 1 567 0.313768 -0.816597 ... -2.307746 1 568 2.022355 0.730259 ... 0.653756 0 [569 rows x 31 columns] >>> atom . plot_distribution ( columns = 0 ) >>> from atom.data_cleaning import Normalizer >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) mean radius mean texture ... worst symmetry worst fractal dimension 0 17.99 10.38 ... 0.4601 0.11890 1 20.57 17.77 ... 0.2750 0.08902 2 19.69 21.25 ... 0.3613 0.08758 3 11.42 20.38 ... 0.6638 0.17300 4 20.29 14.34 ... 0.2364 0.07678 .. ... ... ... ... ... 564 21.56 22.39 ... 0.2060 0.07115 565 20.13 28.25 ... 0.2572 0.06637 566 16.60 28.08 ... 0.2218 0.07820 567 20.60 29.33 ... 0.4087 0.12400 568 7.76 24.54 ... 0.2871 0.07039 [569 rows x 30 columns] >>> normalizer = Normalizer ( verbose = 2 ) >>> X = normalizer . fit_transform ( X ) Fitting Normalizer... Normalizing features... >>> print ( X ) mean radius mean texture ... worst symmetry worst fractal dimension 0 1.134881 -2.678666 ... 2.197206 1.723624 1 1.619346 -0.264377 ... -0.121997 0.537179 2 1.464796 0.547806 ... 1.218181 0.453955 3 -0.759262 0.357721 ... 3.250202 2.517606 4 1.571260 -1.233520 ... -0.943554 -0.279402 .. ... ... ... ... ... 564 1.781795 0.785604 ... -1.721528 -0.751459 565 1.543335 1.845150 ... -0.480093 -1.210527 566 0.828589 1.817618 ... -1.301164 -0.170872 567 1.624440 2.016299 ... 1.744693 1.850944 568 -2.699432 1.203224 ... 0.103122 -0.820663 [569 rows x 30 columns] Methods fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Apply the inverse transformation to the data. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Apply the transformations to the data. method fit (X, y=None) [source] Fit to data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns Normalizer Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X, y=None) [source] Apply the inverse transformation to the data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Original dataframe. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Apply the transformations to the data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Normalized dataframe.","title":"Normalizer"},{"location":"API/data_cleaning/normalizer/#normalizer","text":"class atom.data_cleaning. Normalizer (strategy=\"yeojohnson\", verbose=0, logger=None, random_state=None, **kwargs) [source] Transform the data to follow a Normal/Gaussian distribution. This transformation is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Missing values are disregarded in fit and maintained in transform. Categorical columns are ignored. This class can be accessed from atom through the normalize method. Read more in the user guide . Warning The quantile strategy performs a non-linear transformation. This may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable. Note The yeojohnson and boxcox strategies scale the data after transforming. Use the kwargs to change this behaviour. Parameters strategy: str, default=\"yeojohnson\" The transforming strategy. Choose from: \" yeojohnson \" \" boxcox \" (only works with strictly positive values) \" quantile \": Transform features using quantiles information. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. random_state: int or None, default=None Seed used by the quantile strategy. If None, the random number generator is the RandomState used by np.random . **kwargs Additional keyword arguments for the strategy estimator. Attributes [strategy]: sklearn transformer Object with which the data is transformed. feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. NaN , +inf and -inf are always considered missing since they are incompatible with sklearn estimators. See Also Cleaner Applies standard data cleaning steps on a dataset. Pruner Prune outliers from the data. Scaler Scale the data.","title":"Normalizer"},{"location":"API/data_cleaning/normalizer/#example","text":"atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . dataset ) mean radius mean texture ... worst fractal dimension target 0 16.78 18.80 ... 0.07228 0 1 15.34 14.26 ... 0.09946 0 2 14.22 27.85 ... 0.07796 1 3 18.31 18.58 ... 0.06938 0 4 18.49 17.52 ... 0.09445 0 .. ... ... ... ... ... 564 13.44 21.58 ... 0.07146 0 565 20.47 20.67 ... 0.06386 0 566 12.98 19.35 ... 0.09166 1 567 14.61 15.69 ... 0.05695 1 568 23.27 22.04 ... 0.09187 0 [569 rows x 31 columns] >>> atom . plot_distribution ( columns = 0 ) >>> atom . normalize ( verbose = 2 ) Fitting Normalizer... Normalizing features... >>> print ( atom . dataset ) mean radius mean texture ... worst fractal dimension target 0 0.868700 0.010820 ... -0.684572 0 1 0.513904 -1.257343 ... 1.019875 0 2 0.200435 1.773390 ... -0.226619 1 3 1.197448 -0.042755 ... -0.945047 0 4 1.233326 -0.310726 ... 0.786014 0 .. ... ... ... ... ... 564 -0.041166 0.635293 ... -0.756291 0 565 1.595052 0.440855 ... -1.497202 0 566 -0.193933 0.141884 ... 0.642613 1 567 0.313768 -0.816597 ... -2.307746 1 568 2.022355 0.730259 ... 0.653756 0 [569 rows x 31 columns] >>> atom . plot_distribution ( columns = 0 ) >>> from atom.data_cleaning import Normalizer >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) mean radius mean texture ... worst symmetry worst fractal dimension 0 17.99 10.38 ... 0.4601 0.11890 1 20.57 17.77 ... 0.2750 0.08902 2 19.69 21.25 ... 0.3613 0.08758 3 11.42 20.38 ... 0.6638 0.17300 4 20.29 14.34 ... 0.2364 0.07678 .. ... ... ... ... ... 564 21.56 22.39 ... 0.2060 0.07115 565 20.13 28.25 ... 0.2572 0.06637 566 16.60 28.08 ... 0.2218 0.07820 567 20.60 29.33 ... 0.4087 0.12400 568 7.76 24.54 ... 0.2871 0.07039 [569 rows x 30 columns] >>> normalizer = Normalizer ( verbose = 2 ) >>> X = normalizer . fit_transform ( X ) Fitting Normalizer... Normalizing features... >>> print ( X ) mean radius mean texture ... worst symmetry worst fractal dimension 0 1.134881 -2.678666 ... 2.197206 1.723624 1 1.619346 -0.264377 ... -0.121997 0.537179 2 1.464796 0.547806 ... 1.218181 0.453955 3 -0.759262 0.357721 ... 3.250202 2.517606 4 1.571260 -1.233520 ... -0.943554 -0.279402 .. ... ... ... ... ... 564 1.781795 0.785604 ... -1.721528 -0.751459 565 1.543335 1.845150 ... -0.480093 -1.210527 566 0.828589 1.817618 ... -1.301164 -0.170872 567 1.624440 2.016299 ... 1.744693 1.850944 568 -2.699432 1.203224 ... 0.103122 -0.820663 [569 rows x 30 columns]","title":"Example"},{"location":"API/data_cleaning/normalizer/#methods","text":"fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Apply the inverse transformation to the data. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Apply the transformations to the data. method fit (X, y=None) [source] Fit to data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns Normalizer Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X, y=None) [source] Apply the inverse transformation to the data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Original dataframe. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Apply the transformations to the data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Normalized dataframe.","title":"Methods"},{"location":"API/data_cleaning/pruner/","text":"Pruner class atom.data_cleaning. Pruner (strategy=\"zscore\", method=\"drop\", max_sigma=3, include_target=False, verbose=0, logger=None, **kwargs) [source] Replace or remove outliers. The definition of outlier depends on the selected strategy and can greatly differ from one another. Ignores categorical columns. This class can be accessed from atom through the prune method. Read more in the user guide . Parameters: strategy: str or sequence, default=\"zscore\" Strategy with which to select the outliers. If sequence of strategies, only samples marked as outliers by all chosen strategies are dropped. Choose from: \"zscore\": Z-score of each data value. \" iforest \": Isolation Forest. \" ee \": Elliptic Envelope. \" lof \": Local Outlier Factor. \" svm \": One-class SVM. \" dbscan \": Density-Based Spatial Clustering. \" optics \": DBSCAN-like clustering approach. method: int, float or str, default=\"drop\" Method to apply on the outliers. Only the zscore strategy accepts another method than \"drop\". Choose from: \"drop\": Drop any sample with outlier values. \"min_max\": Replace the outlier with the min or max of the column. Any numerical value with which to replace the outliers. max_sigma: int or float, default=3 Maximum allowed standard deviations from the mean of the column. If more, it is considered an outlier. Only if strategy=\"zscore\". include_target: bool, default=False Whether to include the target column in the search for outliers. This can be useful for regression tasks. Only if strategy=\"zscore\". verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. **kwargs Additional keyword arguments for the strategy estimator. If sequence of strategies, the params should be provided in a dict with the strategy's name as key. Tip Use atom's outliers attribute for an overview of the number of outlier values per column. Attributes Attributes: <strategy>: sklearn estimator Object used to prune the data, e.g. pruner.iforest for the isolation forest strategy. Methods fit_transform Same as transform. get_params Get parameters for this estimator. log Write information to the logger and print to stdout. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. method fit_transform (X, y=None) [source] Apply the outlier strategy to the data. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns: pd.DataFrame Transformed feature set. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method save (filename=\"auto\") [source] Save the instance to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: Pruner Estimator instance. method transform (X, y=None) [source] Apply the outlier strategy to the data. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns: pd.DataFrame Transformed feature set. X: pd.Series Transformed target column. Only returned if provided. Example atom stand-alone from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . prune ( strategy = \"zscore\" , max_sigma = 2 , include_target = True ) from atom.data_cleaning import Pruner pruner = Pruner ( strategy = \"zscore\" , max_sigma = 2 , include_target = True ) X_train , y_train = pruner . transform ( X_train , y_train )","title":"Pruner"},{"location":"API/data_cleaning/pruner/#pruner","text":"class atom.data_cleaning. Pruner (strategy=\"zscore\", method=\"drop\", max_sigma=3, include_target=False, verbose=0, logger=None, **kwargs) [source] Replace or remove outliers. The definition of outlier depends on the selected strategy and can greatly differ from one another. Ignores categorical columns. This class can be accessed from atom through the prune method. Read more in the user guide . Parameters: strategy: str or sequence, default=\"zscore\" Strategy with which to select the outliers. If sequence of strategies, only samples marked as outliers by all chosen strategies are dropped. Choose from: \"zscore\": Z-score of each data value. \" iforest \": Isolation Forest. \" ee \": Elliptic Envelope. \" lof \": Local Outlier Factor. \" svm \": One-class SVM. \" dbscan \": Density-Based Spatial Clustering. \" optics \": DBSCAN-like clustering approach. method: int, float or str, default=\"drop\" Method to apply on the outliers. Only the zscore strategy accepts another method than \"drop\". Choose from: \"drop\": Drop any sample with outlier values. \"min_max\": Replace the outlier with the min or max of the column. Any numerical value with which to replace the outliers. max_sigma: int or float, default=3 Maximum allowed standard deviations from the mean of the column. If more, it is considered an outlier. Only if strategy=\"zscore\". include_target: bool, default=False Whether to include the target column in the search for outliers. This can be useful for regression tasks. Only if strategy=\"zscore\". verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. **kwargs Additional keyword arguments for the strategy estimator. If sequence of strategies, the params should be provided in a dict with the strategy's name as key. Tip Use atom's outliers attribute for an overview of the number of outlier values per column.","title":"Pruner"},{"location":"API/data_cleaning/pruner/#attributes","text":"Attributes: <strategy>: sklearn estimator Object used to prune the data, e.g. pruner.iforest for the isolation forest strategy.","title":"Attributes"},{"location":"API/data_cleaning/pruner/#methods","text":"fit_transform Same as transform. get_params Get parameters for this estimator. log Write information to the logger and print to stdout. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. method fit_transform (X, y=None) [source] Apply the outlier strategy to the data. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns: pd.DataFrame Transformed feature set. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method save (filename=\"auto\") [source] Save the instance to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: Pruner Estimator instance. method transform (X, y=None) [source] Apply the outlier strategy to the data. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns: pd.DataFrame Transformed feature set. X: pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/data_cleaning/pruner/#example","text":"atom stand-alone from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . prune ( strategy = \"zscore\" , max_sigma = 2 , include_target = True ) from atom.data_cleaning import Pruner pruner = Pruner ( strategy = \"zscore\" , max_sigma = 2 , include_target = True ) X_train , y_train = pruner . transform ( X_train , y_train )","title":"Example"},{"location":"API/data_cleaning/scaler/","text":"Scaler class atom.data_cleaning. Scaler (strategy=\"standard\", gpu=False, verbose=0, logger=None, **kwargs) [source] Apply one of sklearn's scalers. Categorical columns are ignored. This class can be accessed from atom through the scale method. Read more in the user guide . Parameters: strategy: str, default=\"standard\" Strategy with which to scale the data. Choose from: \" standard \": Remove mean and scale to unit variance. \" minmax \": Scale features to a given range. \" maxabs \": Scale features by their maximum absolute value. \" robust \": Scale using statistics that are robust to outliers. gpu: bool or str, default=False Train strategy on GPU. If False: Always use CPU implementation. If True: Use GPU implementation if possible. If \"force\": Force GPU implementation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. **kwargs Additional keyword arguments for the strategy estimator. Tip Use atom's scaled attribute to check if the feature set is scaled. Attributes Attributes: estimator: sklearn transformer Object instance with which the data is scaled. feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. Methods fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. log Write information to the logger and print to stdout. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. method fit (X, y=None) [source] Compute the mean and std to be used for scaling. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns: Scaler Estimator instance. method fit_transform (X, y=None) [source] Fit to data, then transform it. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns: X: pd.DataFrame Scaled feature set. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method save (filename=\"auto\") [source] Save the instance to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: Scaler Estimator instance. method transform (X, y=None) [source] Perform standardization by centering and scaling. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns: X: pd.DataFrame Scaled feature set. Example atom stand-alone from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . scale () from atom.data_cleaning import Scaler scaler = Scaler () scaler . fit ( X_train ) X = scaler . transform ( X )","title":"Scaler"},{"location":"API/data_cleaning/scaler/#scaler","text":"class atom.data_cleaning. Scaler (strategy=\"standard\", gpu=False, verbose=0, logger=None, **kwargs) [source] Apply one of sklearn's scalers. Categorical columns are ignored. This class can be accessed from atom through the scale method. Read more in the user guide . Parameters: strategy: str, default=\"standard\" Strategy with which to scale the data. Choose from: \" standard \": Remove mean and scale to unit variance. \" minmax \": Scale features to a given range. \" maxabs \": Scale features by their maximum absolute value. \" robust \": Scale using statistics that are robust to outliers. gpu: bool or str, default=False Train strategy on GPU. If False: Always use CPU implementation. If True: Use GPU implementation if possible. If \"force\": Force GPU implementation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. **kwargs Additional keyword arguments for the strategy estimator. Tip Use atom's scaled attribute to check if the feature set is scaled.","title":"Scaler"},{"location":"API/data_cleaning/scaler/#attributes","text":"Attributes: estimator: sklearn transformer Object instance with which the data is scaled. feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit.","title":"Attributes"},{"location":"API/data_cleaning/scaler/#methods","text":"fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. log Write information to the logger and print to stdout. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. method fit (X, y=None) [source] Compute the mean and std to be used for scaling. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns: Scaler Estimator instance. method fit_transform (X, y=None) [source] Fit to data, then transform it. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns: X: pd.DataFrame Scaled feature set. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method save (filename=\"auto\") [source] Save the instance to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: Scaler Estimator instance. method transform (X, y=None) [source] Perform standardization by centering and scaling. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns: X: pd.DataFrame Scaled feature set.","title":"Methods"},{"location":"API/data_cleaning/scaler/#example","text":"atom stand-alone from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . scale () from atom.data_cleaning import Scaler scaler = Scaler () scaler . fit ( X_train ) X = scaler . transform ( X )","title":"Example"},{"location":"API/feature_engineering/featureextractor/","text":"FeatureExtractor class atom.feature_engineering. FeatureExtractor (features=['day', 'month', 'year'], fmt=None, encoding_type=\"ordinal\", drop_columns=True, verbose=0, logger=None) [source] Extract features from datetime columns. Create new features extracting datetime elements (day, month, year, etc...) from the provided columns. Columns of dtype datetime64 are used as is. Categorical columns that can be successfully converted to a datetime format (less than 30% NaT values after conversion) are also used. This class can be accessed from atom through the feature_extraction method. Read more in the user guide . Warning Decision trees based algorithms build their split rules according to one feature at a time. This means that they will fail to correctly process cyclic features since the sin/cos features should be considered one single coordinate system. Parameters features: str or sequence, default=[\"day\", \"month\", \"year\"] Features to create from the datetime columns. Note that created features with zero variance (e.g. the feature hour in a column that only contains dates) are ignored. Allowed values are datetime attributes from pandas.Series.dt . fmt: str, sequence or None, default=None Format ( strptime ) of the categorical columns that need to be converted to datetime. If sequence, the n-th format corresponds to the n-th categorical column that can be successfully converted. If None, the format is inferred automatically from the first non NaN value. Values that can not be converted are returned as NaT . encoding_type: str, default=\"ordinal\" Type of encoding to use. Choose from: \"ordinal\": Encode features in increasing order. \"cyclic\": Encode features using sine and cosine to capture their cyclic nature. This approach creates two columns for every feature. Non-cyclic features still use ordinal encoding. drop_columns: bool, default=True Whether to drop the original columns after transformation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. Attributes feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also FeatureGenerator Generate new features. FeatureGrouper Extract statistics from similar features. FeatureSelector Reduce the number of features in the data. Example atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X [ \"date\" ] = pd . date_range ( start = \"1/1/2018\" , periods = len ( X )) >>> atom = ATOMClassifier ( X , y ) >>> atom . feature_extraction ( features = [ \"day\" ], fmt = \" %d /%m/%Y\" , verbose = 2 ) Extracting datetime features... --> Extracting features from column date. --> Creating feature date_day. >>> # Note the date_day column >>> print ( atom . dataset ) mean radius mean texture ... date_day target 0 11.300 18.19 ... 31 1 1 16.460 20.11 ... 27 0 2 11.370 18.89 ... 17 1 3 8.598 20.98 ... 3 1 4 12.800 17.46 ... 2 1 .. ... ... ... ... ... 564 17.060 21.00 ... 2 0 565 11.940 20.76 ... 14 1 566 19.590 25.00 ... 28 0 567 12.360 18.54 ... 18 1 568 18.450 21.91 ... 15 0 [569 rows x 32 columns] >>> from atom.feature_engineering import FeatureExtractor >>> from sklearn.datasets import load_breast_cancer >>> X , _ = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X [ \"date\" ] = pd . date_range ( start = \"1/1/2018\" , periods = len ( X )) >>> fe = FeatureExtractor ( features = [ \"day\" ], fmt = \"%Y-%m- %d \" , verbose = 2 ) >>> X = fe . transform ( X ) Extracting datetime features... --> Extracting features from column date. --> Creating feature date_day. >>> # Note the date_day column >>> print ( X ) mean radius mean texture ... worst fractal dimension date_day 0 17.99 10.38 ... 0.11890 1 1 20.57 17.77 ... 0.08902 2 2 19.69 21.25 ... 0.08758 3 3 11.42 20.38 ... 0.17300 4 4 20.29 14.34 ... 0.07678 5 .. ... ... ... ... ... 564 21.56 22.39 ... 0.07115 19 565 20.13 28.25 ... 0.06637 20 566 16.60 28.08 ... 0.07820 21 567 20.60 29.33 ... 0.12400 22 568 7.76 24.54 ... 0.07039 23 [569 rows x 31 columns] Methods fit Does nothing. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Extract the new features. method fit (X=None, y=None, **fit_params) [source] Does nothing. Implemented for continuity of the API. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Extract the new features. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed feature set.","title":"FeatureExtractor"},{"location":"API/feature_engineering/featureextractor/#featureextractor","text":"class atom.feature_engineering. FeatureExtractor (features=['day', 'month', 'year'], fmt=None, encoding_type=\"ordinal\", drop_columns=True, verbose=0, logger=None) [source] Extract features from datetime columns. Create new features extracting datetime elements (day, month, year, etc...) from the provided columns. Columns of dtype datetime64 are used as is. Categorical columns that can be successfully converted to a datetime format (less than 30% NaT values after conversion) are also used. This class can be accessed from atom through the feature_extraction method. Read more in the user guide . Warning Decision trees based algorithms build their split rules according to one feature at a time. This means that they will fail to correctly process cyclic features since the sin/cos features should be considered one single coordinate system. Parameters features: str or sequence, default=[\"day\", \"month\", \"year\"] Features to create from the datetime columns. Note that created features with zero variance (e.g. the feature hour in a column that only contains dates) are ignored. Allowed values are datetime attributes from pandas.Series.dt . fmt: str, sequence or None, default=None Format ( strptime ) of the categorical columns that need to be converted to datetime. If sequence, the n-th format corresponds to the n-th categorical column that can be successfully converted. If None, the format is inferred automatically from the first non NaN value. Values that can not be converted are returned as NaT . encoding_type: str, default=\"ordinal\" Type of encoding to use. Choose from: \"ordinal\": Encode features in increasing order. \"cyclic\": Encode features using sine and cosine to capture their cyclic nature. This approach creates two columns for every feature. Non-cyclic features still use ordinal encoding. drop_columns: bool, default=True Whether to drop the original columns after transformation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. Attributes feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also FeatureGenerator Generate new features. FeatureGrouper Extract statistics from similar features. FeatureSelector Reduce the number of features in the data.","title":"FeatureExtractor"},{"location":"API/feature_engineering/featureextractor/#example","text":"atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X [ \"date\" ] = pd . date_range ( start = \"1/1/2018\" , periods = len ( X )) >>> atom = ATOMClassifier ( X , y ) >>> atom . feature_extraction ( features = [ \"day\" ], fmt = \" %d /%m/%Y\" , verbose = 2 ) Extracting datetime features... --> Extracting features from column date. --> Creating feature date_day. >>> # Note the date_day column >>> print ( atom . dataset ) mean radius mean texture ... date_day target 0 11.300 18.19 ... 31 1 1 16.460 20.11 ... 27 0 2 11.370 18.89 ... 17 1 3 8.598 20.98 ... 3 1 4 12.800 17.46 ... 2 1 .. ... ... ... ... ... 564 17.060 21.00 ... 2 0 565 11.940 20.76 ... 14 1 566 19.590 25.00 ... 28 0 567 12.360 18.54 ... 18 1 568 18.450 21.91 ... 15 0 [569 rows x 32 columns] >>> from atom.feature_engineering import FeatureExtractor >>> from sklearn.datasets import load_breast_cancer >>> X , _ = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X [ \"date\" ] = pd . date_range ( start = \"1/1/2018\" , periods = len ( X )) >>> fe = FeatureExtractor ( features = [ \"day\" ], fmt = \"%Y-%m- %d \" , verbose = 2 ) >>> X = fe . transform ( X ) Extracting datetime features... --> Extracting features from column date. --> Creating feature date_day. >>> # Note the date_day column >>> print ( X ) mean radius mean texture ... worst fractal dimension date_day 0 17.99 10.38 ... 0.11890 1 1 20.57 17.77 ... 0.08902 2 2 19.69 21.25 ... 0.08758 3 3 11.42 20.38 ... 0.17300 4 4 20.29 14.34 ... 0.07678 5 .. ... ... ... ... ... 564 21.56 22.39 ... 0.07115 19 565 20.13 28.25 ... 0.06637 20 566 16.60 28.08 ... 0.07820 21 567 20.60 29.33 ... 0.12400 22 568 7.76 24.54 ... 0.07039 23 [569 rows x 31 columns]","title":"Example"},{"location":"API/feature_engineering/featureextractor/#methods","text":"fit Does nothing. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Extract the new features. method fit (X=None, y=None, **fit_params) [source] Does nothing. Implemented for continuity of the API. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Extract the new features. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed feature set.","title":"Methods"},{"location":"API/feature_engineering/featuregenerator/","text":"FeatureGenerator class atom.feature_engineering. FeatureGenerator (strategy=\"dfs\", n_features=None, operators=None, n_jobs=1, verbose=0, logger=None, random_state=None, **kwargs) [source] Generate new features. Create new combinations of existing features to capture the non-linear relations between the original features. This class can be accessed from atom through the feature_generation method. Read more in the user guide . Warning Using the div , log or sqrt operators can return new features with inf or NaN values. Check the warnings that may pop up or use atom's nans attribute. When using dfs with n_jobs>1 , make sure to protect your code with if __name__ == \"__main__\" . Featuretools uses dask , which uses python multiprocessing for parallelization. The spawn method on multiprocessing starts a new python process, which requires it to import the __main__ module before it can do its task. gfg can be slow for very large populations. Tip dfs can create many new features and not all of them will be useful. Use the FeatureSelector class to reduce the number of features. Parameters strategy: str, default=\"dfs\" Strategy to crate new features. Choose from: \" dfs \": Deep Feature Synthesis. \" gfg \": Genetic Feature Generation. n_features: int or None, default=None Maximum number of newly generated features to add to the dataset. If None, select all created features. operators: str, sequence or None, default=None Mathematical operators to apply on the features. None to use all. Choose from: add , sub , mul , div , abs , sqrt , log , inv , sin , cos , tan . n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If <-1: Use number of cores - 1 + n_jobs . verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState used by np.random . **kwargs Additional keyword arguments for the SymbolicTransformer instance. Only for the gfg strategy. Attributes gfg: SymbolicTransformer Object used to calculate the genetic features. Only for the gfg strategy. genetic_features: pd.DataFrame Information on the newly created non-linear features. Only for the gfg strategy. Columns include: name: Name of the feature (generated automatically). description: Operators used to create this feature. fitness: Fitness score. feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also FeatureExtractor Extract features from datetime columns. FeatureGrouper Extract statistics from similar features. FeatureSelector Reduce the number of features in the data. Example atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> atom = ATOMClassifier ( X , y ) >>> atom . feature_generation ( strategy = \"dfs\" , n_features = 5 , verbose = 2 ) Fitting FeatureGenerator... Generating new features... --> 5 new features were added. >>> # Note the texture error / worst symmetry column >>> print ( atom . dataset ) mean radius mean texture ... texture error / worst symmetry target 0 15.75 19.22 ... 3.118963 0 1 12.10 17.72 ... 5.418170 1 2 20.16 19.66 ... 2.246481 0 3 12.88 18.22 ... 4.527498 1 4 13.03 18.42 ... 11.786613 1 .. ... ... ... ... ... 564 21.75 20.99 ... 4.772326 0 565 13.64 16.34 ... 3.936061 1 566 10.08 15.11 ... 4.323219 1 567 12.91 16.33 ... 3.004630 1 568 11.60 18.36 ... 2.385047 1 [569 rows x 36 columns] >>> from atom.feature_engineering import FeatureGenerator >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> fg = FeatureGenerator ( strategy = \"dfs\" , n_features = 5 , verbose = 2 ) >>> X = fg . fit_transform ( X , y ) Fitting FeatureGenerator... Generating new features... --> 5 new features were added. >>> # Note the radius error * worst smoothness column >>> print ( X ) mean radius ... radius error * worst smoothness 0 17.99 ... 0.177609 1 20.57 ... 0.067285 2 19.69 ... 0.107665 3 11.42 ... 0.103977 4 20.29 ... 0.104039 .. ... ... ... 564 21.56 ... 0.165816 565 20.13 ... 0.089257 566 16.60 ... 0.051984 567 20.60 ... 0.119790 568 7.76 ... 0.034698 [569 rows x 35 columns] Methods fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Generate new features. method fit (X, y) [source] Fit to data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str or sequence Target column corresponding to X. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Generate new features. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed feature set.","title":"FeatureGenerator"},{"location":"API/feature_engineering/featuregenerator/#featuregenerator","text":"class atom.feature_engineering. FeatureGenerator (strategy=\"dfs\", n_features=None, operators=None, n_jobs=1, verbose=0, logger=None, random_state=None, **kwargs) [source] Generate new features. Create new combinations of existing features to capture the non-linear relations between the original features. This class can be accessed from atom through the feature_generation method. Read more in the user guide . Warning Using the div , log or sqrt operators can return new features with inf or NaN values. Check the warnings that may pop up or use atom's nans attribute. When using dfs with n_jobs>1 , make sure to protect your code with if __name__ == \"__main__\" . Featuretools uses dask , which uses python multiprocessing for parallelization. The spawn method on multiprocessing starts a new python process, which requires it to import the __main__ module before it can do its task. gfg can be slow for very large populations. Tip dfs can create many new features and not all of them will be useful. Use the FeatureSelector class to reduce the number of features. Parameters strategy: str, default=\"dfs\" Strategy to crate new features. Choose from: \" dfs \": Deep Feature Synthesis. \" gfg \": Genetic Feature Generation. n_features: int or None, default=None Maximum number of newly generated features to add to the dataset. If None, select all created features. operators: str, sequence or None, default=None Mathematical operators to apply on the features. None to use all. Choose from: add , sub , mul , div , abs , sqrt , log , inv , sin , cos , tan . n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If <-1: Use number of cores - 1 + n_jobs . verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState used by np.random . **kwargs Additional keyword arguments for the SymbolicTransformer instance. Only for the gfg strategy. Attributes gfg: SymbolicTransformer Object used to calculate the genetic features. Only for the gfg strategy. genetic_features: pd.DataFrame Information on the newly created non-linear features. Only for the gfg strategy. Columns include: name: Name of the feature (generated automatically). description: Operators used to create this feature. fitness: Fitness score. feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also FeatureExtractor Extract features from datetime columns. FeatureGrouper Extract statistics from similar features. FeatureSelector Reduce the number of features in the data.","title":"FeatureGenerator"},{"location":"API/feature_engineering/featuregenerator/#example","text":"atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> atom = ATOMClassifier ( X , y ) >>> atom . feature_generation ( strategy = \"dfs\" , n_features = 5 , verbose = 2 ) Fitting FeatureGenerator... Generating new features... --> 5 new features were added. >>> # Note the texture error / worst symmetry column >>> print ( atom . dataset ) mean radius mean texture ... texture error / worst symmetry target 0 15.75 19.22 ... 3.118963 0 1 12.10 17.72 ... 5.418170 1 2 20.16 19.66 ... 2.246481 0 3 12.88 18.22 ... 4.527498 1 4 13.03 18.42 ... 11.786613 1 .. ... ... ... ... ... 564 21.75 20.99 ... 4.772326 0 565 13.64 16.34 ... 3.936061 1 566 10.08 15.11 ... 4.323219 1 567 12.91 16.33 ... 3.004630 1 568 11.60 18.36 ... 2.385047 1 [569 rows x 36 columns] >>> from atom.feature_engineering import FeatureGenerator >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> fg = FeatureGenerator ( strategy = \"dfs\" , n_features = 5 , verbose = 2 ) >>> X = fg . fit_transform ( X , y ) Fitting FeatureGenerator... Generating new features... --> 5 new features were added. >>> # Note the radius error * worst smoothness column >>> print ( X ) mean radius ... radius error * worst smoothness 0 17.99 ... 0.177609 1 20.57 ... 0.067285 2 19.69 ... 0.107665 3 11.42 ... 0.103977 4 20.29 ... 0.104039 .. ... ... ... 564 21.56 ... 0.165816 565 20.13 ... 0.089257 566 16.60 ... 0.051984 567 20.60 ... 0.119790 568 7.76 ... 0.034698 [569 rows x 35 columns]","title":"Example"},{"location":"API/feature_engineering/featuregenerator/#methods","text":"fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Generate new features. method fit (X, y) [source] Fit to data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str or sequence Target column corresponding to X. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Generate new features. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed feature set.","title":"Methods"},{"location":"API/feature_engineering/featuregrouper/","text":"FeatureGrouper class atom.feature_engineering. FeatureGrouper (group, name=None, operators=None, drop_columns=True, verbose=0, logger=None) [source] Extract statistics from similar features. Replace groups of features with related characteristics with new features that summarize statistical properties of te group. The statistical operators are calculated over every row of the group. The group names and features can be accessed through the groups method. This class can be accessed from atom through the feature_grouping method. Read more in the user guide . Tip Use a regex pattern with the groups parameter to select groups easier, e.g. atom.feature_generation(features=\"var_.+\") to select all features that start with var_ . Parameters group: str, slice or sequence Features that belong to a group. Select them by name, position or regex pattern. A feature can belong to multiple groups. Use a sequence of sequences to define multiple groups. name: str, sequence or None, default=None Name of the group. The new features are named combining the operator used and the group's name, e.g. mean(group_1) . If specfified, the length should match with the number of groups defined in features . If None, default group names of the form group1 , group2 , etc... are used. operators: str, sequence or None, default=None Statistical operators to apply on the groups. Any operator from numpy or scipy.stats (checked in that order) that is applied on an array can be used. If None, it uses: min , max , mean , median , mode and std . drop_columns: bool, default=True Whether to drop the columns in groups after transformation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. Attributes groups: dict Names and features of every created group. feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also FeatureExtractor Extract features from datetime columns. FeatureGenerator Generate new features. FeatureSelector Reduce the number of features in the data. Example atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> atom = ATOMClassifier ( X , y ) >>> atom . feature_grouping ( group = [ \"mean.+\" ], name = \"means\" , verbose = 2 ) Fitting FeatureGrouper... Grouping features... --> Group means successfully created. >>> # Note the mean features are gone and the new std(means) feature >>> print ( atom . dataset ) radius error texture error ... std(means) target 0 0.2949 1.6560 ... 137.553584 1 1 0.2351 2.0110 ... 79.830195 1 2 0.4302 2.8780 ... 80.330330 1 3 0.2345 1.2190 ... 151.858455 1 4 0.3511 0.9527 ... 145.769474 1 .. ... ... ... ... ... 564 0.4866 1.9050 ... 116.749243 1 565 0.5925 0.6863 ... 378.431333 0 566 0.2577 1.0950 ... 141.220243 1 567 0.4615 0.9197 ... 257.903846 0 568 0.5462 1.5110 ... 194.704033 1 [569 rows x 27 columns] >>> from atom.feature_engineering import FeatureGrouper >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> # Group all features that start with mean >>> fg = FeatureGrouper ( group = \"mean.+\" , name = \"means\" , verbose = 2 ) >>> X = fg . transform ( X ) Fitting FeatureGrouper... Grouping features... --> Group means successfully created. >>> # Note the mean features are gone and the new std(means) feature >>> print ( X ) radius error texture error ... mode(means) std(means) 0 1.0950 0.9053 ... 0.07871 297.404540 1 0.5435 0.7339 ... 0.05667 393.997131 2 0.7456 0.7869 ... 0.05999 357.203084 3 0.4956 1.1560 ... 0.09744 114.444620 4 0.7572 0.7813 ... 0.05883 385.450556 .. ... ... ... ... ... 564 1.1760 1.2560 ... 0.05623 439.441252 565 0.7655 2.4630 ... 0.05533 374.274845 566 0.4564 1.0750 ... 0.05302 254.320568 567 0.7260 1.5950 ... 0.07016 375.376476 568 0.3857 1.4280 ... 0.00000 53.739926 [569 rows x 26 columns] Methods fit Does nothing. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Group features. method fit (X=None, y=None, **fit_params) [source] Does nothing. Implemented for continuity of the API. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Group features. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed feature set.","title":"FeatureGrouper"},{"location":"API/feature_engineering/featuregrouper/#featuregrouper","text":"class atom.feature_engineering. FeatureGrouper (group, name=None, operators=None, drop_columns=True, verbose=0, logger=None) [source] Extract statistics from similar features. Replace groups of features with related characteristics with new features that summarize statistical properties of te group. The statistical operators are calculated over every row of the group. The group names and features can be accessed through the groups method. This class can be accessed from atom through the feature_grouping method. Read more in the user guide . Tip Use a regex pattern with the groups parameter to select groups easier, e.g. atom.feature_generation(features=\"var_.+\") to select all features that start with var_ . Parameters group: str, slice or sequence Features that belong to a group. Select them by name, position or regex pattern. A feature can belong to multiple groups. Use a sequence of sequences to define multiple groups. name: str, sequence or None, default=None Name of the group. The new features are named combining the operator used and the group's name, e.g. mean(group_1) . If specfified, the length should match with the number of groups defined in features . If None, default group names of the form group1 , group2 , etc... are used. operators: str, sequence or None, default=None Statistical operators to apply on the groups. Any operator from numpy or scipy.stats (checked in that order) that is applied on an array can be used. If None, it uses: min , max , mean , median , mode and std . drop_columns: bool, default=True Whether to drop the columns in groups after transformation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. Attributes groups: dict Names and features of every created group. feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also FeatureExtractor Extract features from datetime columns. FeatureGenerator Generate new features. FeatureSelector Reduce the number of features in the data.","title":"FeatureGrouper"},{"location":"API/feature_engineering/featuregrouper/#example","text":"atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> atom = ATOMClassifier ( X , y ) >>> atom . feature_grouping ( group = [ \"mean.+\" ], name = \"means\" , verbose = 2 ) Fitting FeatureGrouper... Grouping features... --> Group means successfully created. >>> # Note the mean features are gone and the new std(means) feature >>> print ( atom . dataset ) radius error texture error ... std(means) target 0 0.2949 1.6560 ... 137.553584 1 1 0.2351 2.0110 ... 79.830195 1 2 0.4302 2.8780 ... 80.330330 1 3 0.2345 1.2190 ... 151.858455 1 4 0.3511 0.9527 ... 145.769474 1 .. ... ... ... ... ... 564 0.4866 1.9050 ... 116.749243 1 565 0.5925 0.6863 ... 378.431333 0 566 0.2577 1.0950 ... 141.220243 1 567 0.4615 0.9197 ... 257.903846 0 568 0.5462 1.5110 ... 194.704033 1 [569 rows x 27 columns] >>> from atom.feature_engineering import FeatureGrouper >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> # Group all features that start with mean >>> fg = FeatureGrouper ( group = \"mean.+\" , name = \"means\" , verbose = 2 ) >>> X = fg . transform ( X ) Fitting FeatureGrouper... Grouping features... --> Group means successfully created. >>> # Note the mean features are gone and the new std(means) feature >>> print ( X ) radius error texture error ... mode(means) std(means) 0 1.0950 0.9053 ... 0.07871 297.404540 1 0.5435 0.7339 ... 0.05667 393.997131 2 0.7456 0.7869 ... 0.05999 357.203084 3 0.4956 1.1560 ... 0.09744 114.444620 4 0.7572 0.7813 ... 0.05883 385.450556 .. ... ... ... ... ... 564 1.1760 1.2560 ... 0.05623 439.441252 565 0.7655 2.4630 ... 0.05533 374.274845 566 0.4564 1.0750 ... 0.05302 254.320568 567 0.7260 1.5950 ... 0.07016 375.376476 568 0.3857 1.4280 ... 0.00000 53.739926 [569 rows x 26 columns]","title":"Example"},{"location":"API/feature_engineering/featuregrouper/#methods","text":"fit Does nothing. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Group features. method fit (X=None, y=None, **fit_params) [source] Does nothing. Implemented for continuity of the API. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Group features. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed feature set.","title":"Methods"},{"location":"API/feature_engineering/featureselector/","text":"FeatureSelector class atom.feature_engineering. FeatureSelector (strategy=None, solver=None, n_features=None, min_repeated=2, max_repeated=1.0, max_correlation=1.0, n_jobs=1, device=\"cpu\", engine=\"sklearn\", verbose=0, logger=None, random_state=None, **kwargs) [source] Reduce the number of features in the data. Apply feature selection or dimensionality reduction, either to improve the estimators' accuracy or to boost their performance on very high-dimensional datasets. Additionally, remove multicollinear and low variance features. This class can be accessed from atom through the feature_selection method. Read more in the user guide . Warning Ties between features with equal scores are broken in an unspecified way. For strategy=\"rfecv\", the n_features parameter is the minimum number of features to select, not the actual number of features that the transformer returns. It may very well be that it returns more! Info The \"sklearnex\" and \"cuml\" engines are only supported for strategy=\"pca\" with dense datasets. If strategy=\"pca\" and the data is dense and unscaled, it's scaled to mean=0 and std=1 before fitting the PCA transformer. If strategy=\"pca\" and the provided data is sparse, the used estimator is TruncatedSVD , which works more efficiently with sparse matrices. Tip Use the plot_feature_importance method to examine how much a specific feature contributes to the final predictions. If the model doesn't have a feature_importances_ attribute, use plot_permutation_importance instead. Parameters strategy: str or None, default=None Feature selection strategy to use. Choose from: None: Do not perform any feature selection strategy. \" univariate \": Univariate statistical F-test. \" pca \": Principal Component Analysis. \" sfm \": Select best features according to a model. \" sfs \": Sequential Feature Selection. \" rfe \": Recursive Feature Elimination. \" rfecv \": RFE with cross-validated selection. \" pso \": Particle Swarm Optimization. \" hho \": Harris Hawks Optimization. \" gwo \": Grey Wolf Optimization. \" dfo \": Dragonfly Optimization. \" go \": Genetic Optimization. solver: str, estimator or None, default=None Solver/estimator to use for the feature selection strategy. See the corresponding documentation for an extended description of the choices. If None, the default value is used (only if strategy=\"pca\"). Choose from: If strategy=\"univariate\": \" f_classif \" \" f_regression \" \" mutual_info_classif \" \" mutual_info_regression \" \" chi2 \" Any function with signature func(X, y) -> (scores, p-values) . If strategy=\"pca\": If data is dense: If engine=\"sklearn\": \"auto\" (default) \"full\" \"arpack\" \"randomized\" If engine=\"sklearnex\": \"full\" (default) If engine=\"cuml\": \"full\" (default) \"jacobi\" If data is sparse: \"randomized\" (default) \"arpack\" for the remaining strategies: The base estimator. For sfm, rfe and rfecv, it should have either a feature_importances_ or coef_ attribute after fitting. You can use one of the predefined models . Add _class or _reg after the model's name to specify a classification or regression task, e.g. solver=\"LGB_reg\" (not necessary if called from atom). No default option. n_features: int, float or None, default=None Number of features to select. If None: Select all features. If <1: Fraction of the total features to select. If >=1: Number of features to select. If strategy=\"sfm\" and the threshold parameter is not specified, the threshold is automatically set to -inf to select n_features number of features. If strategy=\"rfecv\", n_features is the minimum number of features to select. This parameter is ignored if any of the following strategies is selected: pso, hho, gwo, dfo, go. min_repeated: int, float or None, default=2 Remove categorical features if there isn't any repeated value in at least min_repeated rows. The default is to keep all features with non-maximum variance, i.e. remove the features which number of unique values is equal to the number of rows (usually the case for names, IDs, etc...). If None: No check for minimum repetition. If >1: Minimum repetition number. If <=1: Minimum repetition fraction. max_repeated: int, float or None, default=1. Remove categorical features with the same value in at least max_repeated rows. The default is to keep all features with non-zero variance, i.e. remove the features that have the same value in all samples. If None: No check for maximum repetition. If >1: Maximum number of repeated occurences. If <=1: Maximum fraction of repeated occurences. max_correlation: float or None, default=1. Minimum absolute Pearson correlation to identify correlated features. For each group, it removes all except the feature with the highest correlation to y (if provided, else it removes all but the first). The default value removes equal columns. If None, skip this step. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If <-1: Use number of cores - 1 + n_jobs . device: str, default=\"cpu\" Device on which to train the estimators. Use any string that follows the SYCL_DEVICE_FILTER filter selector, e.g. device=\"gpu\" to use the GPU. Read more in the user guide . engine: str, default=\"sklearn\" Execution engine to use for the estimators. Refer to the user guide for an explanation regarding every choice. Choose from: \"sklearn\" (only if device=\"cpu\") \"sklearnex\" \"cuml\" (only if device=\"gpu\") verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState used by np.random . **kwargs Any extra keyword argument for the strategy estimator. See the corresponding documentation for the available options. Attributes collinear: pd.DataFrame Information on the removed collinear features. Columns include: drop: Name of the dropped feature. corr_feature: Names of the correlated features. corr_value: Corresponding correlation coefficients. feature_importance: pd.Series Normalized importance scores calculated by the solver for the features kept by the transformer. The scores are extracted from the coef_ or feature_importances_ attribute, checked in that order. Only if strategy is one of univariate, sfm, rfe or rfecv. [strategy]: sklearn transformer Object used to transform the data, e.g. fs.pca for the pca strategy. feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also FeatureExtractor Extract features from datetime columns. FeatureGenerator Generate new features. FeatureGrouper Extract statistics from similar features. Example atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> atom = ATOMClassifier ( X , y ) >>> atom . feature_selection ( strategy = \"pca\" , n_features = 12 , verbose = 2 ) Fitting FeatureSelector... Performing feature selection ... --> Applying Principal Component Analysis... --> Scaling features... --> Keeping 12 components. --> Explained variance ratio: 0.97 >>> # Note that the column names changed >>> print ( atom . dataset ) pca0 pca1 pca2 ... pca10 pca11 target 0 -2.493723 3.082653 1.318595 ... -0.182142 -0.591784 1 1 4.596102 -0.876940 -0.380685 ... 0.224170 1.155544 0 2 0.955979 -2.141057 -1.677736 ... 0.306153 0.099138 0 3 3.221488 4.209911 -2.818757 ... 0.808883 -0.531868 0 4 1.038000 2.451758 -1.753683 ... -0.312883 0.862319 1 .. ... ... ... ... ... ... ... 564 3.414827 -3.757253 -1.012369 ... 0.387175 0.283633 0 565 -1.191561 -1.276069 -0.871712 ... 0.106362 -0.449361 1 566 -2.757000 0.411997 -1.321697 ... 0.185550 -0.025368 1 567 -3.252533 0.074827 0.549622 ... 0.693073 -0.058251 1 568 1.607258 -2.076465 -1.025986 ... -0.385542 0.103603 0 [569 rows x 13 columns] >>> atom . plot_pca () >>> from atom.feature_engineering import FeatureSelector >>> from sklearn.datasets import load_breast_cancer >>> X , _ = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> fs = FeatureSelector ( strategy = \"pca\" , n_features = 12 , verbose = 2 ) >>> X = fs . fit_transform ( X ) Fitting FeatureSelector... Performing feature selection ... --> Applying Principal Component Analysis... --> Scaling features... --> Keeping 12 components. --> Explained variance ratio: 0.97 >>> # Note that the column names changed >>> print ( X ) pca0 pca1 pca2 ... pca9 pca10 pca11 0 9.192837 1.948583 -1.123166 ... -0.877402 0.262955 -0.859014 1 2.387802 -3.768172 -0.529293 ... 1.106995 0.813120 0.157923 2 5.733896 -1.075174 -0.551748 ... 0.454275 -0.605604 0.124387 3 7.122953 10.275589 -3.232790 ... -1.116975 -1.151514 1.011316 4 3.935302 -1.948072 1.389767 ... 0.377704 0.651360 -0.110515 .. ... ... ... ... ... ... ... 564 6.439315 -3.576817 2.459487 ... 0.256989 -0.062651 0.123342 565 3.793382 -3.584048 2.088476 ... -0.108632 0.244804 0.222753 566 1.256179 -1.902297 0.562731 ... 0.520877 -0.840512 0.096473 567 10.374794 1.672010 -1.877029 ... -0.089296 -0.178628 -0.697461 568 -5.475243 -0.670637 1.490443 ... -0.047726 -0.144094 -0.179496 [569 rows x 12 columns] >>> fs . plot_pca () Methods fit Fit the feature selector to the data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. plot_components Plot the explained variance ratio per component. plot_pca Plot the explained variance ratio vs number of components. plot_rfecv Plot the rfecv results. reset_aesthetics Reset the plot aesthetics to their default values. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. method fit (X, y=None) [source] Fit the feature selector to the data. The univariate, sfm (when model is not fitted), sfs, rfe and rfecv strategies need a target column. Leaving it None raises an exception. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method plot_components (show=None, title=None, figsize=None, filename=None, display=True) [source] Plot the explained variance ratio per component. Parameters show: int or None, default=None Number of components to show. None to show all. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of components shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns matplotlib.figure.Figure Plot object. Only returned if display=None . method plot_pca (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the explained variance ratio vs number of components. If the underlying estimator is pca (for dense datasets), all possible components are plotted. If the underlying estimator is TruncatedSVD (for sparse datasets), it only shows the selected components. The blue star marks the number of components selected by the user. Parameters title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns matplotlib.figure.Figure Plot object. Only returned if display=None . method plot_rfecv (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the rfecv results. Plot the scores obtained by the estimator fitted on every subset of the dataset. Parameters title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=None Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns matplotlib.figure.Figure Plot object. Only returned if display=None . method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Transform the data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Only for continuity of the API. Returns pd.DataFrame Transformed feature set.","title":"FeatureSelector"},{"location":"API/feature_engineering/featureselector/#featureselector","text":"class atom.feature_engineering. FeatureSelector (strategy=None, solver=None, n_features=None, min_repeated=2, max_repeated=1.0, max_correlation=1.0, n_jobs=1, device=\"cpu\", engine=\"sklearn\", verbose=0, logger=None, random_state=None, **kwargs) [source] Reduce the number of features in the data. Apply feature selection or dimensionality reduction, either to improve the estimators' accuracy or to boost their performance on very high-dimensional datasets. Additionally, remove multicollinear and low variance features. This class can be accessed from atom through the feature_selection method. Read more in the user guide . Warning Ties between features with equal scores are broken in an unspecified way. For strategy=\"rfecv\", the n_features parameter is the minimum number of features to select, not the actual number of features that the transformer returns. It may very well be that it returns more! Info The \"sklearnex\" and \"cuml\" engines are only supported for strategy=\"pca\" with dense datasets. If strategy=\"pca\" and the data is dense and unscaled, it's scaled to mean=0 and std=1 before fitting the PCA transformer. If strategy=\"pca\" and the provided data is sparse, the used estimator is TruncatedSVD , which works more efficiently with sparse matrices. Tip Use the plot_feature_importance method to examine how much a specific feature contributes to the final predictions. If the model doesn't have a feature_importances_ attribute, use plot_permutation_importance instead. Parameters strategy: str or None, default=None Feature selection strategy to use. Choose from: None: Do not perform any feature selection strategy. \" univariate \": Univariate statistical F-test. \" pca \": Principal Component Analysis. \" sfm \": Select best features according to a model. \" sfs \": Sequential Feature Selection. \" rfe \": Recursive Feature Elimination. \" rfecv \": RFE with cross-validated selection. \" pso \": Particle Swarm Optimization. \" hho \": Harris Hawks Optimization. \" gwo \": Grey Wolf Optimization. \" dfo \": Dragonfly Optimization. \" go \": Genetic Optimization. solver: str, estimator or None, default=None Solver/estimator to use for the feature selection strategy. See the corresponding documentation for an extended description of the choices. If None, the default value is used (only if strategy=\"pca\"). Choose from: If strategy=\"univariate\": \" f_classif \" \" f_regression \" \" mutual_info_classif \" \" mutual_info_regression \" \" chi2 \" Any function with signature func(X, y) -> (scores, p-values) . If strategy=\"pca\": If data is dense: If engine=\"sklearn\": \"auto\" (default) \"full\" \"arpack\" \"randomized\" If engine=\"sklearnex\": \"full\" (default) If engine=\"cuml\": \"full\" (default) \"jacobi\" If data is sparse: \"randomized\" (default) \"arpack\" for the remaining strategies: The base estimator. For sfm, rfe and rfecv, it should have either a feature_importances_ or coef_ attribute after fitting. You can use one of the predefined models . Add _class or _reg after the model's name to specify a classification or regression task, e.g. solver=\"LGB_reg\" (not necessary if called from atom). No default option. n_features: int, float or None, default=None Number of features to select. If None: Select all features. If <1: Fraction of the total features to select. If >=1: Number of features to select. If strategy=\"sfm\" and the threshold parameter is not specified, the threshold is automatically set to -inf to select n_features number of features. If strategy=\"rfecv\", n_features is the minimum number of features to select. This parameter is ignored if any of the following strategies is selected: pso, hho, gwo, dfo, go. min_repeated: int, float or None, default=2 Remove categorical features if there isn't any repeated value in at least min_repeated rows. The default is to keep all features with non-maximum variance, i.e. remove the features which number of unique values is equal to the number of rows (usually the case for names, IDs, etc...). If None: No check for minimum repetition. If >1: Minimum repetition number. If <=1: Minimum repetition fraction. max_repeated: int, float or None, default=1. Remove categorical features with the same value in at least max_repeated rows. The default is to keep all features with non-zero variance, i.e. remove the features that have the same value in all samples. If None: No check for maximum repetition. If >1: Maximum number of repeated occurences. If <=1: Maximum fraction of repeated occurences. max_correlation: float or None, default=1. Minimum absolute Pearson correlation to identify correlated features. For each group, it removes all except the feature with the highest correlation to y (if provided, else it removes all but the first). The default value removes equal columns. If None, skip this step. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If <-1: Use number of cores - 1 + n_jobs . device: str, default=\"cpu\" Device on which to train the estimators. Use any string that follows the SYCL_DEVICE_FILTER filter selector, e.g. device=\"gpu\" to use the GPU. Read more in the user guide . engine: str, default=\"sklearn\" Execution engine to use for the estimators. Refer to the user guide for an explanation regarding every choice. Choose from: \"sklearn\" (only if device=\"cpu\") \"sklearnex\" \"cuml\" (only if device=\"gpu\") verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState used by np.random . **kwargs Any extra keyword argument for the strategy estimator. See the corresponding documentation for the available options. Attributes collinear: pd.DataFrame Information on the removed collinear features. Columns include: drop: Name of the dropped feature. corr_feature: Names of the correlated features. corr_value: Corresponding correlation coefficients. feature_importance: pd.Series Normalized importance scores calculated by the solver for the features kept by the transformer. The scores are extracted from the coef_ or feature_importances_ attribute, checked in that order. Only if strategy is one of univariate, sfm, rfe or rfecv. [strategy]: sklearn transformer Object used to transform the data, e.g. fs.pca for the pca strategy. feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also FeatureExtractor Extract features from datetime columns. FeatureGenerator Generate new features. FeatureGrouper Extract statistics from similar features.","title":"FeatureSelector"},{"location":"API/feature_engineering/featureselector/#example","text":"atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> atom = ATOMClassifier ( X , y ) >>> atom . feature_selection ( strategy = \"pca\" , n_features = 12 , verbose = 2 ) Fitting FeatureSelector... Performing feature selection ... --> Applying Principal Component Analysis... --> Scaling features... --> Keeping 12 components. --> Explained variance ratio: 0.97 >>> # Note that the column names changed >>> print ( atom . dataset ) pca0 pca1 pca2 ... pca10 pca11 target 0 -2.493723 3.082653 1.318595 ... -0.182142 -0.591784 1 1 4.596102 -0.876940 -0.380685 ... 0.224170 1.155544 0 2 0.955979 -2.141057 -1.677736 ... 0.306153 0.099138 0 3 3.221488 4.209911 -2.818757 ... 0.808883 -0.531868 0 4 1.038000 2.451758 -1.753683 ... -0.312883 0.862319 1 .. ... ... ... ... ... ... ... 564 3.414827 -3.757253 -1.012369 ... 0.387175 0.283633 0 565 -1.191561 -1.276069 -0.871712 ... 0.106362 -0.449361 1 566 -2.757000 0.411997 -1.321697 ... 0.185550 -0.025368 1 567 -3.252533 0.074827 0.549622 ... 0.693073 -0.058251 1 568 1.607258 -2.076465 -1.025986 ... -0.385542 0.103603 0 [569 rows x 13 columns] >>> atom . plot_pca () >>> from atom.feature_engineering import FeatureSelector >>> from sklearn.datasets import load_breast_cancer >>> X , _ = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> fs = FeatureSelector ( strategy = \"pca\" , n_features = 12 , verbose = 2 ) >>> X = fs . fit_transform ( X ) Fitting FeatureSelector... Performing feature selection ... --> Applying Principal Component Analysis... --> Scaling features... --> Keeping 12 components. --> Explained variance ratio: 0.97 >>> # Note that the column names changed >>> print ( X ) pca0 pca1 pca2 ... pca9 pca10 pca11 0 9.192837 1.948583 -1.123166 ... -0.877402 0.262955 -0.859014 1 2.387802 -3.768172 -0.529293 ... 1.106995 0.813120 0.157923 2 5.733896 -1.075174 -0.551748 ... 0.454275 -0.605604 0.124387 3 7.122953 10.275589 -3.232790 ... -1.116975 -1.151514 1.011316 4 3.935302 -1.948072 1.389767 ... 0.377704 0.651360 -0.110515 .. ... ... ... ... ... ... ... 564 6.439315 -3.576817 2.459487 ... 0.256989 -0.062651 0.123342 565 3.793382 -3.584048 2.088476 ... -0.108632 0.244804 0.222753 566 1.256179 -1.902297 0.562731 ... 0.520877 -0.840512 0.096473 567 10.374794 1.672010 -1.877029 ... -0.089296 -0.178628 -0.697461 568 -5.475243 -0.670637 1.490443 ... -0.047726 -0.144094 -0.179496 [569 rows x 12 columns] >>> fs . plot_pca ()","title":"Example"},{"location":"API/feature_engineering/featureselector/#methods","text":"fit Fit the feature selector to the data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. plot_components Plot the explained variance ratio per component. plot_pca Plot the explained variance ratio vs number of components. plot_rfecv Plot the rfecv results. reset_aesthetics Reset the plot aesthetics to their default values. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. method fit (X, y=None) [source] Fit the feature selector to the data. The univariate, sfm (when model is not fitted), sfs, rfe and rfecv strategies need a target column. Leaving it None raises an exception. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method plot_components (show=None, title=None, figsize=None, filename=None, display=True) [source] Plot the explained variance ratio per component. Parameters show: int or None, default=None Number of components to show. None to show all. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of components shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns matplotlib.figure.Figure Plot object. Only returned if display=None . method plot_pca (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the explained variance ratio vs number of components. If the underlying estimator is pca (for dense datasets), all possible components are plotted. If the underlying estimator is TruncatedSVD (for sparse datasets), it only shows the selected components. The blue star marks the number of components selected by the user. Parameters title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns matplotlib.figure.Figure Plot object. Only returned if display=None . method plot_rfecv (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the rfecv results. Plot the scores obtained by the estimator fitted on every subset of the dataset. Parameters title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=None Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns matplotlib.figure.Figure Plot object. Only returned if display=None . method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Transform the data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). y: int, str, dict, sequence or None, default=None Does nothing. Only for continuity of the API. Returns pd.DataFrame Transformed feature set.","title":"Methods"},{"location":"API/models/adab/","text":"AdaBoost accept sparse AdaBoost is a meta-estimator that begins by fitting a classifier/regressor on the original dataset and then fits additional copies of the algorithm on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. Corresponding estimators are: AdaBoostClassifier for classification tasks. AdaBoostRegressor for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The algorithm parameter is only used with AdaBoostClassifier. The loss parameter is only used with AdaBoostRegressor. The random_state parameter is set equal to that of the parent. Dimensions: n_estimators: int, default=50 Integer(10, 500, name=\"n_estimators\") learning_rate: float, default=1.0 Real(0.01, 10, \"log-uniform\", name=\"learning_rate\") algorithm: str, default=\"SAMME.R\" Categorical([\"SAMME.R\", \"SAMME\"], name=\"algorithm\") loss: str, default=\"linear\" Categorical([\"linear\", \"square\", \"exponential\"], name=\"loss\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set (only if classifier). decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.adab.plot_permutation_importance() or atom.adab.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (memory=None, verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"AdaB\" , metric = \"poisson\" , est_params = { \"algorithm\" : \"SAMME.R\" })","title":"AdaBoost"},{"location":"API/models/adab/#adaboost","text":"accept sparse AdaBoost is a meta-estimator that begins by fitting a classifier/regressor on the original dataset and then fits additional copies of the algorithm on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. Corresponding estimators are: AdaBoostClassifier for classification tasks. AdaBoostRegressor for regression tasks. Read more in sklearn's documentation .","title":"AdaBoost"},{"location":"API/models/adab/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The algorithm parameter is only used with AdaBoostClassifier. The loss parameter is only used with AdaBoostRegressor. The random_state parameter is set equal to that of the parent. Dimensions: n_estimators: int, default=50 Integer(10, 500, name=\"n_estimators\") learning_rate: float, default=1.0 Real(0.01, 10, \"log-uniform\", name=\"learning_rate\") algorithm: str, default=\"SAMME.R\" Categorical([\"SAMME.R\", \"SAMME\"], name=\"algorithm\") loss: str, default=\"linear\" Categorical([\"linear\", \"square\", \"exponential\"], name=\"loss\")","title":"Hyperparameters"},{"location":"API/models/adab/#attributes","text":"","title":"Attributes"},{"location":"API/models/adab/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/adab/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/adab/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set (only if classifier). decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/adab/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.adab.plot_permutation_importance() or atom.adab.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (memory=None, verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/adab/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"AdaB\" , metric = \"poisson\" , est_params = { \"algorithm\" : \"SAMME.R\" })","title":"Example"},{"location":"API/models/ard/","text":"Automatic Relevance Determination needs scaling Automatic Relevance Determination is very similar to Bayesian Ridge , but can lead to sparser coefficients. Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Corresponding estimators are: ARDRegression for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: n_iter: float, default=300 Integer(100, 1000, name=\"n_iter\") alpha_1: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"alpha_1\") alpha_2: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"alpha_2\") lambda_1: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"lambda_1\") lambda_2: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"lambda_2\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.ard.plot_permutation_importance() or atom.ard.predict(X) . The remaining utility methods can be found hereunder. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", sample_weight=None) [source] Get the model's score for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"ARD\" , n_calls = 20 , n_initial_points = 7 , n_bootstrap = 5 )","title":"Automated Relevance Determination"},{"location":"API/models/ard/#automatic-relevance-determination","text":"needs scaling Automatic Relevance Determination is very similar to Bayesian Ridge , but can lead to sparser coefficients. Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Corresponding estimators are: ARDRegression for regression tasks. Read more in sklearn's documentation .","title":"Automatic Relevance Determination"},{"location":"API/models/ard/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: n_iter: float, default=300 Integer(100, 1000, name=\"n_iter\") alpha_1: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"alpha_1\") alpha_2: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"alpha_2\") lambda_1: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"lambda_1\") lambda_2: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"lambda_2\")","title":"Hyperparameters"},{"location":"API/models/ard/#attributes","text":"","title":"Attributes"},{"location":"API/models/ard/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/ard/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/ard/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/ard/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.ard.plot_permutation_importance() or atom.ard.predict(X) . The remaining utility methods can be found hereunder. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", sample_weight=None) [source] Get the model's score for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/ard/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"ARD\" , n_calls = 20 , n_initial_points = 7 , n_bootstrap = 5 )","title":"Example"},{"location":"API/models/bag/","text":"Bagging accept sparse Bagging uses an ensemble meta-estimator that fits base classifiers/regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree ), by introducing randomization into its construction procedure and then making an ensemble out of it. Corresponding estimators are: BaggingClassifier for classification tasks. BaggingRegressor for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: n_estimators: int, default=10 Integer(10, 500, name=\"n_estimators\") max_samples: float, default=1.0 Categorical(np.linspace(0.5, 1.0, 6), name=\"max_samples\") max_features: float, default=1.0 Categorical(np.linspace(0.5, 1.0, 6), name=\"max_features\") bootstrap: bool, default=True Categorical([True, False], name=\"bootstrap\") bootstrap_features: bool, default=False Categorical([True, False], name=\"bootstrap_features\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.bag.plot_permutation_importance() or atom.bag.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"Bag\" )","title":"Bagging"},{"location":"API/models/bag/#bagging","text":"accept sparse Bagging uses an ensemble meta-estimator that fits base classifiers/regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree ), by introducing randomization into its construction procedure and then making an ensemble out of it. Corresponding estimators are: BaggingClassifier for classification tasks. BaggingRegressor for regression tasks. Read more in sklearn's documentation .","title":"Bagging"},{"location":"API/models/bag/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: n_estimators: int, default=10 Integer(10, 500, name=\"n_estimators\") max_samples: float, default=1.0 Categorical(np.linspace(0.5, 1.0, 6), name=\"max_samples\") max_features: float, default=1.0 Categorical(np.linspace(0.5, 1.0, 6), name=\"max_features\") bootstrap: bool, default=True Categorical([True, False], name=\"bootstrap\") bootstrap_features: bool, default=False Categorical([True, False], name=\"bootstrap_features\")","title":"Hyperparameters"},{"location":"API/models/bag/#attributes","text":"","title":"Attributes"},{"location":"API/models/bag/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/bag/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/bag/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/bag/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.bag.plot_permutation_importance() or atom.bag.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/bag/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"Bag\" )","title":"Example"},{"location":"API/models/bnb/","text":"Bernoulli Naive Bayes accept sparse supports_gpu Bernoulli Naive Bayes implements the Naive Bayes algorithm for multivariate Bernoulli models. Like Multinomial Naive bayes (MNB) , this classifier is suitable for discrete data. The difference is that while MNB works with occurrence counts, BNB is designed for binary/boolean features. Corresponding estimators are: BernoulliNB for classification tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: alpha: float, default=1.0 Real(0.01, 10, \"log-uniform\", name=\"alpha\") fit_prior: bool, default=True Categorical([True, False], name=\"fit_prior\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.bnb.plot_permutation_importance() or atom.bnb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"BNB\" , metric = \"precision\" )","title":"Bernoulli Naive Bayes"},{"location":"API/models/bnb/#bernoulli-naive-bayes","text":"accept sparse supports_gpu Bernoulli Naive Bayes implements the Naive Bayes algorithm for multivariate Bernoulli models. Like Multinomial Naive bayes (MNB) , this classifier is suitable for discrete data. The difference is that while MNB works with occurrence counts, BNB is designed for binary/boolean features. Corresponding estimators are: BernoulliNB for classification tasks. Read more in sklearn's documentation .","title":"Bernoulli Naive Bayes"},{"location":"API/models/bnb/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: alpha: float, default=1.0 Real(0.01, 10, \"log-uniform\", name=\"alpha\") fit_prior: bool, default=True Categorical([True, False], name=\"fit_prior\")","title":"Hyperparameters"},{"location":"API/models/bnb/#attributes","text":"","title":"Attributes"},{"location":"API/models/bnb/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/bnb/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/bnb/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/bnb/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.bnb.plot_permutation_importance() or atom.bnb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/bnb/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"BNB\" , metric = \"precision\" )","title":"Example"},{"location":"API/models/br/","text":"Bayesian Ridge needs scaling Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand. Corresponding estimators are: BayesianRidge for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: n_iter: float, default=300 Integer(100, 1000, name=\"n_iter\") alpha_1: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"alpha_1\") alpha_2: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"alpha_2\") lambda_1: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"lambda_1\") lambda_2: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"lambda_2\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.br.plot_permutation_importance() or atom.br.predict(X) . The remaining utility methods can be found hereunder. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", sample_weight=None) [source] Get the model's score for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"BR\" , n_calls = 20 , n_initial_points = 7 , n_bootstrap = 5 )","title":"Bayesian Ridge"},{"location":"API/models/br/#bayesian-ridge","text":"needs scaling Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand. Corresponding estimators are: BayesianRidge for regression tasks. Read more in sklearn's documentation .","title":"Bayesian Ridge"},{"location":"API/models/br/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: n_iter: float, default=300 Integer(100, 1000, name=\"n_iter\") alpha_1: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"alpha_1\") alpha_2: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"alpha_2\") lambda_1: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"lambda_1\") lambda_2: float, default=1e-6 Categorical([1e-6, 1e-4, 1e-2, 1e-1, 1], name=\"lambda_2\")","title":"Hyperparameters"},{"location":"API/models/br/#attributes","text":"","title":"Attributes"},{"location":"API/models/br/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/br/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/br/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/br/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.br.plot_permutation_importance() or atom.br.predict(X) . The remaining utility methods can be found hereunder. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", sample_weight=None) [source] Get the model's score for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/br/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"BR\" , n_calls = 20 , n_initial_points = 7 , n_bootstrap = 5 )","title":"Example"},{"location":"API/models/catb/","text":"CatBoost (CatB) needs scaling accept sparse supports_gpu CatBoost is a machine learning method based on gradient boosting over decision trees. Main advantages of CatBoost: Superior quality when compared with other GBDT models on many datasets. Best in class prediction speed. Corresponding estimators are: CatBoostClassifier for classification tasks. CatBoostRegressor for regression tasks. Read more in CatBoost's documentation . Info CatBoost allows early stopping to stop the training of unpromising models prematurely. Note ATOM uses CatBoost's n_estimators parameter instead of iterations to indicate the number of trees to fit. This is done to have consistent naming with the XGB and LGB models. Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The bootstrap_type parameter is set to \"Bernoulli\" to allow for the subsample parameter. The num_leaves and min_child_samples parameters are not available for the CPU implementation. The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: n_estimators: int, default=100 Integer(20, 500, name=\"n_estimators\") learning_rate: float, default=0.1 Real(0.01, 1.0, \"log-uniform\", name=\"learning_rate\") max_depth: int or None, default=None Categorical([None, *range(1, 17)], name=\"max_depth\") max_depth: int, default=1 Integer(1, 30, name=\"min_child_samples\") subsample: float, default=1.0 Categorical(np.linspace(0.5, 1.0, 6), name=\"subsample\") reg_lambda: int, default=0 Categorical([0, 0.01, 0.1, 1, 10, 100], name=\"reg_lambda\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.catb.plot_permutation_importance() or atom.catb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"CatB\" , n_calls = 50 , bo_params = { \"early_stopping\" : 0.1 })","title":"CatBoost"},{"location":"API/models/catb/#catboost-catb","text":"needs scaling accept sparse supports_gpu CatBoost is a machine learning method based on gradient boosting over decision trees. Main advantages of CatBoost: Superior quality when compared with other GBDT models on many datasets. Best in class prediction speed. Corresponding estimators are: CatBoostClassifier for classification tasks. CatBoostRegressor for regression tasks. Read more in CatBoost's documentation . Info CatBoost allows early stopping to stop the training of unpromising models prematurely. Note ATOM uses CatBoost's n_estimators parameter instead of iterations to indicate the number of trees to fit. This is done to have consistent naming with the XGB and LGB models.","title":"CatBoost (CatB)"},{"location":"API/models/catb/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The bootstrap_type parameter is set to \"Bernoulli\" to allow for the subsample parameter. The num_leaves and min_child_samples parameters are not available for the CPU implementation. The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: n_estimators: int, default=100 Integer(20, 500, name=\"n_estimators\") learning_rate: float, default=0.1 Real(0.01, 1.0, \"log-uniform\", name=\"learning_rate\") max_depth: int or None, default=None Categorical([None, *range(1, 17)], name=\"max_depth\") max_depth: int, default=1 Integer(1, 30, name=\"min_child_samples\") subsample: float, default=1.0 Categorical(np.linspace(0.5, 1.0, 6), name=\"subsample\") reg_lambda: int, default=0 Categorical([0, 0.01, 0.1, 1, 10, 100], name=\"reg_lambda\")","title":"Hyperparameters"},{"location":"API/models/catb/#attributes","text":"","title":"Attributes"},{"location":"API/models/catb/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/catb/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/catb/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/catb/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.catb.plot_permutation_importance() or atom.catb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/catb/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"CatB\" , n_calls = 50 , bo_params = { \"early_stopping\" : 0.1 })","title":"Example"},{"location":"API/models/catnb/","text":"Categorical Naive Bayes (CatNB) accept sparse supports_gpu Categorical Naive Bayes implements the Naive Bayes algorithm for categorical features. Corresponding estimators are: CategoricalNB for classification tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: alpha: float, default=1.0 Real(0.01, 10, \"log-uniform\", name=\"alpha\") fit_prior: bool, default=True Categorical([True, False], name=\"fit_prior\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.catnb.plot_permutation_importance() or atom.catnb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"CatNB\" )","title":"Categorical Naive Bayes"},{"location":"API/models/catnb/#categorical-naive-bayes-catnb","text":"accept sparse supports_gpu Categorical Naive Bayes implements the Naive Bayes algorithm for categorical features. Corresponding estimators are: CategoricalNB for classification tasks. Read more in sklearn's documentation .","title":"Categorical Naive Bayes (CatNB)"},{"location":"API/models/catnb/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: alpha: float, default=1.0 Real(0.01, 10, \"log-uniform\", name=\"alpha\") fit_prior: bool, default=True Categorical([True, False], name=\"fit_prior\")","title":"Hyperparameters"},{"location":"API/models/catnb/#attributes","text":"","title":"Attributes"},{"location":"API/models/catnb/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/catnb/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/catnb/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/catnb/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.catnb.plot_permutation_importance() or atom.catnb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/catnb/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"CatNB\" )","title":"Example"},{"location":"API/models/cnb/","text":"Complement Naive Bayes (CNB) needs scaling accept sparse The Complement Naive Bayes classifier was designed to correct the \u201csevere assumptions\u201d made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets. Corresponding estimators are: ComplementNB for classification tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: alpha: float, default=1.0 Real(0.01, 10, \"log-uniform\", name=\"alpha\") fit_prior: bool, default=True Categorical([True, False], name=\"fit_prior\") norm: bool, default=False Categorical([True, False], name=\"norm\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.cnb.plot_permutation_importance() or atom.cnb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"CNB\" )","title":"Complement Naive Bayes"},{"location":"API/models/cnb/#complement-naive-bayes-cnb","text":"needs scaling accept sparse The Complement Naive Bayes classifier was designed to correct the \u201csevere assumptions\u201d made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets. Corresponding estimators are: ComplementNB for classification tasks. Read more in sklearn's documentation .","title":"Complement Naive Bayes (CNB)"},{"location":"API/models/cnb/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: alpha: float, default=1.0 Real(0.01, 10, \"log-uniform\", name=\"alpha\") fit_prior: bool, default=True Categorical([True, False], name=\"fit_prior\") norm: bool, default=False Categorical([True, False], name=\"norm\")","title":"Hyperparameters"},{"location":"API/models/cnb/#attributes","text":"","title":"Attributes"},{"location":"API/models/cnb/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/cnb/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/cnb/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/cnb/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.cnb.plot_permutation_importance() or atom.cnb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/cnb/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"CNB\" )","title":"Example"},{"location":"API/models/dummy/","text":"Dummy Estimator (Dummy) When doing supervised learning, a simple sanity check consists of comparing one's estimator against simple rules of thumb. The prediction methods completely ignore the input data. Do not use this model for real problems. Use it only as a simple baseline to compare with other models. Corresponding estimators are: DummyClassifier for classification tasks. DummyRegressor for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The quantile parameter is only used with DummyRegressor. The random_state parameter is set equal to that of the parent. Dimensions: strategy: str classifier: default=\"prior\" Categorical([\"most_frequent\", \"prior\", \"stratified\", \"uniform\"], name=\"strategy\") regressor: default=\"mean\" Categorical([\"mean\", \"median\", \"quantile\"], name=\"strategy\") quantile: float or None, default=None Categorical([None, *np.linspace(0.0, 1.0, 11)], name=\"quantile\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.dummy.plot_roc() or atom.dummy.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = [ \"Dummy\" , \"Tree\" ]) # Compare baseline with a decision tree","title":"Dummy Estimator"},{"location":"API/models/dummy/#dummy-estimator-dummy","text":"When doing supervised learning, a simple sanity check consists of comparing one's estimator against simple rules of thumb. The prediction methods completely ignore the input data. Do not use this model for real problems. Use it only as a simple baseline to compare with other models. Corresponding estimators are: DummyClassifier for classification tasks. DummyRegressor for regression tasks. Read more in sklearn's documentation .","title":"Dummy Estimator (Dummy)"},{"location":"API/models/dummy/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The quantile parameter is only used with DummyRegressor. The random_state parameter is set equal to that of the parent. Dimensions: strategy: str classifier: default=\"prior\" Categorical([\"most_frequent\", \"prior\", \"stratified\", \"uniform\"], name=\"strategy\") regressor: default=\"mean\" Categorical([\"mean\", \"median\", \"quantile\"], name=\"strategy\") quantile: float or None, default=None Categorical([None, *np.linspace(0.0, 1.0, 11)], name=\"quantile\")","title":"Hyperparameters"},{"location":"API/models/dummy/#attributes","text":"","title":"Attributes"},{"location":"API/models/dummy/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/dummy/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/dummy/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/dummy/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.dummy.plot_roc() or atom.dummy.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/dummy/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = [ \"Dummy\" , \"Tree\" ]) # Compare baseline with a decision tree","title":"Example"},{"location":"API/models/en/","text":"Elastic Net (EN) needs scaling accept sparse supports_gpu Linear least squares with l1 and l2 regularization. Corresponding estimators are: ElasticNet for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The random_state parameter is set equal to that of the parent. Dimensions: alpha: float, default=1.0 Real(1e-3, 10, \"log-uniform\", name=\"alpha\") l1_ratio: float, default=0.5 Categorical(np.linspace(0.1, 0.9, 9), name=\"l1_ratio\") selection: str, default=\"cyclic\" Categorical([\"cyclic\", \"random\"], name=\"selection\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.en.plot_permutation_importance() or atom.en.predict(X) . The remaining utility methods can be found hereunder. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", sample_weight=None) [source] Get the model's score for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"EN\" , est_params = { \"l1_ratio\" : 0.75 })","title":"ElasticNet Regression"},{"location":"API/models/en/#elastic-net-en","text":"needs scaling accept sparse supports_gpu Linear least squares with l1 and l2 regularization. Corresponding estimators are: ElasticNet for regression tasks. Read more in sklearn's documentation .","title":"Elastic Net (EN)"},{"location":"API/models/en/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The random_state parameter is set equal to that of the parent. Dimensions: alpha: float, default=1.0 Real(1e-3, 10, \"log-uniform\", name=\"alpha\") l1_ratio: float, default=0.5 Categorical(np.linspace(0.1, 0.9, 9), name=\"l1_ratio\") selection: str, default=\"cyclic\" Categorical([\"cyclic\", \"random\"], name=\"selection\")","title":"Hyperparameters"},{"location":"API/models/en/#attributes","text":"","title":"Attributes"},{"location":"API/models/en/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/en/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/en/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/en/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.en.plot_permutation_importance() or atom.en.predict(X) . The remaining utility methods can be found hereunder. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", sample_weight=None) [source] Get the model's score for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/en/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"EN\" , est_params = { \"l1_ratio\" : 0.75 })","title":"Example"},{"location":"API/models/et/","text":"Extra-Trees (ET) accept sparse Extra-Trees use a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Corresponding estimators are: ExtraTreesClassifier for classification tasks. ExtraTreesRegressor for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The max_samples parameter is only used when bootstrap=True. The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: n_estimators: int, default=100 Integer(10, 500, name=\"n_estimators\") criterion: str classifier: default=\"gini\" Categorical([\"gini\", \"entropy\"], name=\"criterion\") regressor: default=\"squared_error\" Categorical([\"squared_error\", \"absolute_error\"], name=\"criterion\") max_depth: int or None, default=None Categorical([None, *range(1, 17)], name=\"max_depth\") min_samples_split: int, default=2 Integer(2, 20, name=\"min_samples_split\") min_samples_leaf: int, default=1 Integer(1, 20, name=\"min_samples_leaf\") max_features: str, float or None, default=\"auto\" Categorical([\"auto\", \"sqrt\", \"log2\", *np.linspace(0.5, 0.9, 5), None], name=\"max_features\") bootstrap: bool, default=False Categorical([True, False], name=\"bootstrap\") max_samples: float or None, default=None Categorical([None, *np.linspace(0.5, 0.9, 5)], name=\"max_samples\") ccp_alpha: float, default=0 Real(0, 0.035, name=\"ccp_alpha\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.et.plot_permutation_importance() or atom.et.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"ET\" , metric = \"MSE\" , n_calls = 5 , n_initial_points = 1 )","title":"Extra-Trees"},{"location":"API/models/et/#extra-trees-et","text":"accept sparse Extra-Trees use a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Corresponding estimators are: ExtraTreesClassifier for classification tasks. ExtraTreesRegressor for regression tasks. Read more in sklearn's documentation .","title":"Extra-Trees (ET)"},{"location":"API/models/et/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The max_samples parameter is only used when bootstrap=True. The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: n_estimators: int, default=100 Integer(10, 500, name=\"n_estimators\") criterion: str classifier: default=\"gini\" Categorical([\"gini\", \"entropy\"], name=\"criterion\") regressor: default=\"squared_error\" Categorical([\"squared_error\", \"absolute_error\"], name=\"criterion\") max_depth: int or None, default=None Categorical([None, *range(1, 17)], name=\"max_depth\") min_samples_split: int, default=2 Integer(2, 20, name=\"min_samples_split\") min_samples_leaf: int, default=1 Integer(1, 20, name=\"min_samples_leaf\") max_features: str, float or None, default=\"auto\" Categorical([\"auto\", \"sqrt\", \"log2\", *np.linspace(0.5, 0.9, 5), None], name=\"max_features\") bootstrap: bool, default=False Categorical([True, False], name=\"bootstrap\") max_samples: float or None, default=None Categorical([None, *np.linspace(0.5, 0.9, 5)], name=\"max_samples\") ccp_alpha: float, default=0 Real(0, 0.035, name=\"ccp_alpha\")","title":"Hyperparameters"},{"location":"API/models/et/#attributes","text":"","title":"Attributes"},{"location":"API/models/et/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/et/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/et/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/et/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.et.plot_permutation_importance() or atom.et.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/et/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"ET\" , metric = \"MSE\" , n_calls = 5 , n_initial_points = 1 )","title":"Example"},{"location":"API/models/gbm/","text":"Gradient Boosting Machine (GBM) accept sparse A Gradient Boosting Machine builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced. Corresponding estimators are: GradientBoostingClassifier for classification tasks. GradientBoostingRegressor for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. For multiclass classification tasks, the loss parameter is always set to \"deviance\". The alpha parameter is only used when loss=\"huber\" or \"quantile\". The random_state parameter is set equal to that of the parent. Dimensions: loss: str binary classifier: default=\"deviance\" Categorical([\"deviance\", \"exponential\"], name=\"loss\") regressor: default=\"squared_error\" Categorical([\"squared_error\", \"absolute_error\", \"huber\", \"quantile\"], name=\"loss\") learning_rate: float, default=0.1 Real(0.01, 1.0, \"log-uniform\", name=\"learning_rate\") n_estimators: int, default=100 Integer(10, 500, name=\"n_estimators\") subsample: float, default=1.0 Categorical(np.linspace(0.5, 1.0, 6), name=\"subsample\") criterion: str, default=\"friedman_mse\" Categorical([\"friedman_mse\", \"mse\"], name=\"criterion\") min_samples_split: int, default=2 Integer(2, 20, name=\"min_samples_split\") min_samples_leaf: int, default=1 Integer(1, 20, name=\"min_samples_leaf\") max_depth: int, default=3 Integer(1, 21, name=\"max_depth\") max_features: str, float or None, default=\"auto\" Categorical([\"auto\", \"sqrt\", \"log2\", *np.linspace(0.5, 0.9, 5), None], name=\"max_features\") ccp_alpha: float, default=0.0 Real(0, 0.035, name=\"ccp_alpha\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set (only if classifier). decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.gbm.plot_permutation_importance() or atom.gbm.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"GBM\" )","title":"Gradient Boosting Machine"},{"location":"API/models/gbm/#gradient-boosting-machine-gbm","text":"accept sparse A Gradient Boosting Machine builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced. Corresponding estimators are: GradientBoostingClassifier for classification tasks. GradientBoostingRegressor for regression tasks. Read more in sklearn's documentation .","title":"Gradient Boosting Machine (GBM)"},{"location":"API/models/gbm/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. For multiclass classification tasks, the loss parameter is always set to \"deviance\". The alpha parameter is only used when loss=\"huber\" or \"quantile\". The random_state parameter is set equal to that of the parent. Dimensions: loss: str binary classifier: default=\"deviance\" Categorical([\"deviance\", \"exponential\"], name=\"loss\") regressor: default=\"squared_error\" Categorical([\"squared_error\", \"absolute_error\", \"huber\", \"quantile\"], name=\"loss\") learning_rate: float, default=0.1 Real(0.01, 1.0, \"log-uniform\", name=\"learning_rate\") n_estimators: int, default=100 Integer(10, 500, name=\"n_estimators\") subsample: float, default=1.0 Categorical(np.linspace(0.5, 1.0, 6), name=\"subsample\") criterion: str, default=\"friedman_mse\" Categorical([\"friedman_mse\", \"mse\"], name=\"criterion\") min_samples_split: int, default=2 Integer(2, 20, name=\"min_samples_split\") min_samples_leaf: int, default=1 Integer(1, 20, name=\"min_samples_leaf\") max_depth: int, default=3 Integer(1, 21, name=\"max_depth\") max_features: str, float or None, default=\"auto\" Categorical([\"auto\", \"sqrt\", \"log2\", *np.linspace(0.5, 0.9, 5), None], name=\"max_features\") ccp_alpha: float, default=0.0 Real(0, 0.035, name=\"ccp_alpha\")","title":"Hyperparameters"},{"location":"API/models/gbm/#attributes","text":"","title":"Attributes"},{"location":"API/models/gbm/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/gbm/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/gbm/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set (only if classifier). decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/gbm/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.gbm.plot_permutation_importance() or atom.gbm.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/gbm/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"GBM\" )","title":"Example"},{"location":"API/models/gnb/","text":"Gaussian Naive bayes (GNB) supports_gpu Gaussian Naive Bayes implements the Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian. Corresponding estimators are: GaussianNB for classification tasks. Read more in sklearn's documentation . Tip The features in the dataset can be transformed to follow a Gaussian distribution using the gauss method. Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. GNB has no parameters to tune with the BO. Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.gnb.plot_permutation_importance() or atom.gnb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"GNB\" )","title":"Gaussian Naive Bayes"},{"location":"API/models/gnb/#gaussian-naive-bayes-gnb","text":"supports_gpu Gaussian Naive Bayes implements the Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian. Corresponding estimators are: GaussianNB for classification tasks. Read more in sklearn's documentation . Tip The features in the dataset can be transformed to follow a Gaussian distribution using the gauss method.","title":"Gaussian Naive bayes (GNB)"},{"location":"API/models/gnb/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. GNB has no parameters to tune with the BO.","title":"Hyperparameters"},{"location":"API/models/gnb/#attributes","text":"","title":"Attributes"},{"location":"API/models/gnb/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/gnb/#utility-attributes","text":"Attributes: estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/gnb/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/gnb/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.gnb.plot_permutation_importance() or atom.gnb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/gnb/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"GNB\" )","title":"Example"},{"location":"API/models/gp/","text":"Gaussian Process (GP) Gaussian Processes are a generic supervised learning method designed to solve regression and probabilistic classification problems. The advantages of Gaussian processes are: The prediction interpolates the observations. The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and decide based on those if one should refit (online fitting, adaptive fitting) the prediction in some region of interest. The disadvantages of Gaussian processes include: They are not sparse, i.e. they use the whole samples/features information to perform the prediction. They lose efficiency in high dimensional spaces, namely when the number of features exceeds a few dozens. Corresponding estimators are: GaussianProcessClassifier for classification tasks. GaussianProcessClassifier for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. GP has no parameters to tune with the BO. Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.gp.plot_permutation_importance() or atom.gp.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"GP\" , metric = \"medae\" )","title":"Gaussian Process"},{"location":"API/models/gp/#gaussian-process-gp","text":"Gaussian Processes are a generic supervised learning method designed to solve regression and probabilistic classification problems. The advantages of Gaussian processes are: The prediction interpolates the observations. The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and decide based on those if one should refit (online fitting, adaptive fitting) the prediction in some region of interest. The disadvantages of Gaussian processes include: They are not sparse, i.e. they use the whole samples/features information to perform the prediction. They lose efficiency in high dimensional spaces, namely when the number of features exceeds a few dozens. Corresponding estimators are: GaussianProcessClassifier for classification tasks. GaussianProcessClassifier for regression tasks. Read more in sklearn's documentation .","title":"Gaussian Process (GP)"},{"location":"API/models/gp/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. GP has no parameters to tune with the BO.","title":"Hyperparameters"},{"location":"API/models/gp/#attributes","text":"","title":"Attributes"},{"location":"API/models/gp/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/gp/#utility-attributes","text":"Attributes: estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/gp/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/gp/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.gp.plot_permutation_importance() or atom.gp.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/gp/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"GP\" , metric = \"medae\" )","title":"Example"},{"location":"API/models/hgbm/","text":"HistGBM (hGBM) This Histogram-based Gradient Boosting Machine is much faster than the standard GBM for big datasets (n_samples >= 10 000). This variation first bins the input samples into integer-valued bins which tremendously reduces the number of splitting points to consider, and allows the algorithm to leverage integer-based data structures (histograms) instead of relying on sorted continuous values when building the trees. Corresponding estimators are: HistGradientBoostingClassifier for classification tasks. HistGradientBoostingRegressor for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. For binary classification tasks, the loss parameter is always set to \"binary_crossentropy\". For multiclass classification tasks, the loss parameter is always set to \"categorical_crossentropy\". The random_state parameter is set equal to that of the parent. Dimensions: loss: str, default=\"squared_error\" Categorical([\"squared_error\", \"absolute_error\", \"poisson\"], name=\"loss\") learning_rate: float, default=0.1 Real(0.01, 1.0, \"log-uniform\", name=\"learning_rate\") max_iter: int, default=100 Integer(10, 500, name=\"n_estimators\") max_leaf_nodes: int, default=31 Integer(10, 50, name=\"max_leaf_nodes\") max_depth: int or None, default=None Categorical([None, *range(1, 21], name=\"max_depth\") min_samples_leaf: int, default=20 Integer(10, 30, name=\"min_samples_leaf\") l2_regularization: float, default=0.0 Categorical(np.linspace(0.0, 1.0, 11), name=\"l2_regularization\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set (only if classifier). decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.hgbm.plot_permutation_importance() or atom.hgbm.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"hGBM\" )","title":"HistGBM"},{"location":"API/models/hgbm/#histgbm-hgbm","text":"This Histogram-based Gradient Boosting Machine is much faster than the standard GBM for big datasets (n_samples >= 10 000). This variation first bins the input samples into integer-valued bins which tremendously reduces the number of splitting points to consider, and allows the algorithm to leverage integer-based data structures (histograms) instead of relying on sorted continuous values when building the trees. Corresponding estimators are: HistGradientBoostingClassifier for classification tasks. HistGradientBoostingRegressor for regression tasks. Read more in sklearn's documentation .","title":"HistGBM (hGBM)"},{"location":"API/models/hgbm/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. For binary classification tasks, the loss parameter is always set to \"binary_crossentropy\". For multiclass classification tasks, the loss parameter is always set to \"categorical_crossentropy\". The random_state parameter is set equal to that of the parent. Dimensions: loss: str, default=\"squared_error\" Categorical([\"squared_error\", \"absolute_error\", \"poisson\"], name=\"loss\") learning_rate: float, default=0.1 Real(0.01, 1.0, \"log-uniform\", name=\"learning_rate\") max_iter: int, default=100 Integer(10, 500, name=\"n_estimators\") max_leaf_nodes: int, default=31 Integer(10, 50, name=\"max_leaf_nodes\") max_depth: int or None, default=None Categorical([None, *range(1, 21], name=\"max_depth\") min_samples_leaf: int, default=20 Integer(10, 30, name=\"min_samples_leaf\") l2_regularization: float, default=0.0 Categorical(np.linspace(0.0, 1.0, 11), name=\"l2_regularization\")","title":"Hyperparameters"},{"location":"API/models/hgbm/#attributes","text":"","title":"Attributes"},{"location":"API/models/hgbm/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/hgbm/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/hgbm/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set (only if classifier). decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/hgbm/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.hgbm.plot_permutation_importance() or atom.hgbm.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/hgbm/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"hGBM\" )","title":"Example"},{"location":"API/models/huber/","text":"Huber Regression (Huber) needs scaling Huber is a linear regression model that is robust to outliers. It makes sure that the loss function is not heavily influenced by the outliers while not completely ignoring their effect. Corresponding estimators are: HuberRegressor for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: epsilon: float, default=1.35 Real(1, 10, \"log-uniform\", name=\"epsilon\") max_iter: int, default=100 Integer(50, 500, name=\"max_iter\") alpha: float, default=1e-4 Categorical([1e-4, 1e-3, 1e-2, 1e-1, 1], name=\"alpha\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.hub.plot_residuals() or atom.hub.predict(X) . The remaining utility methods can be found hereunder. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", sample_weight=None) [source] Get the model's score for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"Huber\" )","title":"Huber Regression"},{"location":"API/models/huber/#huber-regression-huber","text":"needs scaling Huber is a linear regression model that is robust to outliers. It makes sure that the loss function is not heavily influenced by the outliers while not completely ignoring their effect. Corresponding estimators are: HuberRegressor for regression tasks. Read more in sklearn's documentation .","title":"Huber Regression (Huber)"},{"location":"API/models/huber/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: epsilon: float, default=1.35 Real(1, 10, \"log-uniform\", name=\"epsilon\") max_iter: int, default=100 Integer(50, 500, name=\"max_iter\") alpha: float, default=1e-4 Categorical([1e-4, 1e-3, 1e-2, 1e-1, 1], name=\"alpha\")","title":"Hyperparameters"},{"location":"API/models/huber/#attributes","text":"","title":"Attributes"},{"location":"API/models/huber/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/huber/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/huber/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/huber/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.hub.plot_residuals() or atom.hub.predict(X) . The remaining utility methods can be found hereunder. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", sample_weight=None) [source] Get the model's score for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/huber/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"Huber\" )","title":"Example"},{"location":"API/models/knn/","text":"K-Nearest Neighbors (KNN) needs scaling accept sparse supports_gpu K-Nearest Neighbors, as the name clearly indicates, implements the k-nearest neighbors vote. For regression, the target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set. Corresponding estimators are: KNeighborsClassifier for classification tasks. KNeighborsRegressor for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The n_jobs parameter is set equal to that of the parent. Dimensions: n_neighbors: int, default=5 Integer(1, 100, name=\"n_neighbors\") weights: str, default=\"uniform\" Categorical([\"uniform\", \"distance\"], name=\"weights\") algorithm: str, default=\"auto\" Categorical([\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"], name=\"algorithm\") leaf_size: int, default=30 Integer(20, 40, name=\"leaf_size\") p: int, default=2 Integer(1, 2, name=\"p\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.knn.plot_permutation_importance() or atom.knn.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"KNN\" , metric = \"ME\" , n_calls = 20 , bo_params = { \"max_time\" : 1000 })","title":"K-Nearest Neighbors"},{"location":"API/models/knn/#k-nearest-neighbors-knn","text":"needs scaling accept sparse supports_gpu K-Nearest Neighbors, as the name clearly indicates, implements the k-nearest neighbors vote. For regression, the target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set. Corresponding estimators are: KNeighborsClassifier for classification tasks. KNeighborsRegressor for regression tasks. Read more in sklearn's documentation .","title":"K-Nearest Neighbors (KNN)"},{"location":"API/models/knn/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The n_jobs parameter is set equal to that of the parent. Dimensions: n_neighbors: int, default=5 Integer(1, 100, name=\"n_neighbors\") weights: str, default=\"uniform\" Categorical([\"uniform\", \"distance\"], name=\"weights\") algorithm: str, default=\"auto\" Categorical([\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"], name=\"algorithm\") leaf_size: int, default=30 Integer(20, 40, name=\"leaf_size\") p: int, default=2 Integer(1, 2, name=\"p\")","title":"Hyperparameters"},{"location":"API/models/knn/#attributes","text":"","title":"Attributes"},{"location":"API/models/knn/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/knn/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/knn/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/knn/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.knn.plot_permutation_importance() or atom.knn.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/knn/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"KNN\" , metric = \"ME\" , n_calls = 20 , bo_params = { \"max_time\" : 1000 })","title":"Example"},{"location":"API/models/ksvm/","text":"Kernel SVM (kSVM) needs scaling accept sparse supports_gpu The implementation of the Kernel (non-linear) Support Vector Machine is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using a Linear Support Vector Machine or a Stochastic Gradient descent model instead. The multiclass support is handled according to a one-vs-one scheme. Corresponding estimators are: SVC for classification tasks. SVR for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The degree parameter is only used when kernel=\"poly\". The gamma parameter is always set to \"scale\" when kernel=\"poly\". The coef0 parameter is only used when kernel=\"rbf\". The random_state parameter is set equal to that of the parent. Dimensions: C: float, default=1.0 Real(1e-3, 100, \"log-uniform\", name=\"C\") kernel: str, default=\"rbf\" Categorical([\"linear\", \"poly\", \"rbf\", \"sigmoid\"], name=\"kernel\") degree: int, default=3 Integer(2, 5, name=\"degree\") gamma: str, default=\"scale\" Categorical([\"scale\", \"auto\"], name=\"gamma\") coef0: float, default=0 Real(-1.0, 1.0, name=\"coef0\") epsilon: float, default=0.1 Real(1e-3, 100, \"log-uniform\", name=\"epsilon\") shrinking: bool, default=True Categorical([True, False], name=\"shrinking\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set (only if classifier). decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.ksvm.plot_permutation_importance() or atom.ksvm.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"kSVM\" , metric = \"r2\" , est_params = { \"kernel\" : \"rbf\" })","title":"Kernel SVM"},{"location":"API/models/ksvm/#kernel-svm-ksvm","text":"needs scaling accept sparse supports_gpu The implementation of the Kernel (non-linear) Support Vector Machine is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using a Linear Support Vector Machine or a Stochastic Gradient descent model instead. The multiclass support is handled according to a one-vs-one scheme. Corresponding estimators are: SVC for classification tasks. SVR for regression tasks. Read more in sklearn's documentation .","title":"Kernel SVM (kSVM)"},{"location":"API/models/ksvm/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The degree parameter is only used when kernel=\"poly\". The gamma parameter is always set to \"scale\" when kernel=\"poly\". The coef0 parameter is only used when kernel=\"rbf\". The random_state parameter is set equal to that of the parent. Dimensions: C: float, default=1.0 Real(1e-3, 100, \"log-uniform\", name=\"C\") kernel: str, default=\"rbf\" Categorical([\"linear\", \"poly\", \"rbf\", \"sigmoid\"], name=\"kernel\") degree: int, default=3 Integer(2, 5, name=\"degree\") gamma: str, default=\"scale\" Categorical([\"scale\", \"auto\"], name=\"gamma\") coef0: float, default=0 Real(-1.0, 1.0, name=\"coef0\") epsilon: float, default=0.1 Real(1e-3, 100, \"log-uniform\", name=\"epsilon\") shrinking: bool, default=True Categorical([True, False], name=\"shrinking\")","title":"Hyperparameters"},{"location":"API/models/ksvm/#attributes","text":"","title":"Attributes"},{"location":"API/models/ksvm/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/ksvm/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/ksvm/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set (only if classifier). decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/ksvm/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.ksvm.plot_permutation_importance() or atom.ksvm.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/ksvm/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"kSVM\" , metric = \"r2\" , est_params = { \"kernel\" : \"rbf\" })","title":"Example"},{"location":"API/models/lars/","text":"Least Angle Regression (Lars) needs scaling supports_gpu Least-angle regression is a regression algorithm for high-dimensional data. Lars is similar to forward stepwise regression. At each step, it finds the feature most correlated with the target. When there are multiple features having equal correlation, instead of continuing along the same feature, it proceeds in a direction equiangular between the features. Corresponding estimators are: Lars for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The n_jobs parameter is set equal to that of the parent. Lars has no parameters to tune with the BO. Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.lars.plot_errors() or atom.lars.predict(X) . The remaining utility methods can be found hereunder. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", sample_weight=None) [source] Get the model's score for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"Lars\" )","title":"Least Angle Regression"},{"location":"API/models/lars/#least-angle-regression-lars","text":"needs scaling supports_gpu Least-angle regression is a regression algorithm for high-dimensional data. Lars is similar to forward stepwise regression. At each step, it finds the feature most correlated with the target. When there are multiple features having equal correlation, instead of continuing along the same feature, it proceeds in a direction equiangular between the features. Corresponding estimators are: Lars for regression tasks. Read more in sklearn's documentation .","title":"Least Angle Regression (Lars)"},{"location":"API/models/lars/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The n_jobs parameter is set equal to that of the parent. Lars has no parameters to tune with the BO.","title":"Hyperparameters"},{"location":"API/models/lars/#attributes","text":"","title":"Attributes"},{"location":"API/models/lars/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/lars/#utility-attributes","text":"Attributes: estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/lars/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/lars/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.lars.plot_errors() or atom.lars.predict(X) . The remaining utility methods can be found hereunder. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", sample_weight=None) [source] Get the model's score for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/lars/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"Lars\" )","title":"Example"},{"location":"API/models/lasso/","text":"Lasso Regression (Lasso) needs scaling accept sparse supports_gpu Linear least squares with l1 regularization. Corresponding estimators are: Lasso for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The random_state parameter is set equal to that of the parent. Dimensions: alpha: float, default=1.0 Real(1e-3, 10, \"log-uniform\", name=\"alpha\") selection: str, default=\"cyclic\" Categorical([\"cyclic\", \"random\"], name=\"selection\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.lasso.plot_permutation_importance() or atom.lasso.predict(X) . The remaining utility methods can be found hereunder. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", sample_weight=None) [source] Get the model's score for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"Lasso\" )","title":"Lasso Regression"},{"location":"API/models/lasso/#lasso-regression-lasso","text":"needs scaling accept sparse supports_gpu Linear least squares with l1 regularization. Corresponding estimators are: Lasso for regression tasks. Read more in sklearn's documentation .","title":"Lasso Regression (Lasso)"},{"location":"API/models/lasso/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The random_state parameter is set equal to that of the parent. Dimensions: alpha: float, default=1.0 Real(1e-3, 10, \"log-uniform\", name=\"alpha\") selection: str, default=\"cyclic\" Categorical([\"cyclic\", \"random\"], name=\"selection\")","title":"Hyperparameters"},{"location":"API/models/lasso/#attributes","text":"","title":"Attributes"},{"location":"API/models/lasso/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/lasso/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/lasso/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/lasso/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.lasso.plot_permutation_importance() or atom.lasso.predict(X) . The remaining utility methods can be found hereunder. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", sample_weight=None) [source] Get the model's score for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/lasso/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"Lasso\" )","title":"Example"},{"location":"API/models/lda/","text":"Linear Discriminant Analysis (LDA) Linear Discriminant Analysis is a classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. Corresponding estimators are: LinearDiscriminantAnalysis for classification tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The shrinkage parameter is not used when solver=\"svd\". Dimensions: solver: str, default=\"svd\" Categorical([\"svd\", \"lsqr\", \"eigen\"], name=\"solver\") shrinkage: float, default=\"auto\" Categorical(np.linspace(0.1, 1.0, 10), name=\"shrinkage\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set. decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.lda.plot_permutation_importance() or atom.lda.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"LDA\" )","title":"Linear Discriminant Analysis"},{"location":"API/models/lda/#linear-discriminant-analysis-lda","text":"Linear Discriminant Analysis is a classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. Corresponding estimators are: LinearDiscriminantAnalysis for classification tasks. Read more in sklearn's documentation .","title":"Linear Discriminant Analysis (LDA)"},{"location":"API/models/lda/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The shrinkage parameter is not used when solver=\"svd\". Dimensions: solver: str, default=\"svd\" Categorical([\"svd\", \"lsqr\", \"eigen\"], name=\"solver\") shrinkage: float, default=\"auto\" Categorical(np.linspace(0.1, 1.0, 10), name=\"shrinkage\")","title":"Hyperparameters"},{"location":"API/models/lda/#attributes","text":"","title":"Attributes"},{"location":"API/models/lda/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/lda/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/lda/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set. decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/lda/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.lda.plot_permutation_importance() or atom.lda.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/lda/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"LDA\" )","title":"Example"},{"location":"API/models/lgb/","text":"LightGBM (LGB) needs scaling accept sparse supports_gpu LightGBM is a gradient boosting model that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages: Faster training speed and higher efficiency. Lower memory usage. Better accuracy. Capable of handling large-scale data. Corresponding estimators are: LGBMClassifier for classification tasks. LGBMRegressor for regression tasks. Read more in LightGBM's documentation . Info LightGBM allows early stopping to stop the training of unpromising models prematurely. Info Using LightGBM's GPU implementation requires extra installations . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: n_estimators: int, default=100 Integer(20, 500, name=\"n_estimators\") learning_rate: float, default=0.1 Real(0.01, 1.0, \"log-uniform\", name=\"learning_rate\") max_depth: int, default=-1 Categorical([-1, *range(1, 17)], name=\"max_depth\") num_leaves: int, default=31 Integer(20, 40, name=\"num_leaves\") min_child_weight: float, default=0.001 Categorical([1e-4, 1e-3, 0.01, 0.1, 1, 10, 100], name=\"min_child_weight\"), min_child_samples: int, default=20 Integer(1, 30, name=\"min_child_samples\") subsample: float, default=1.0 Categorical(np.linspace(0.5, 1.0, 6), name=\"subsample\") colsample_bytree: float, default=1.0 Categorical(np.linspace(0.4, 1.0, 7), name=\"colsample_by_level\") reg_alpha: float, default=0.0 Categorical([0, 0.01, 0.1, 1, 10, 100], name=\"reg_alpha\") reg_lambda: float, default=0.0 Categorical([0, 0.01, 0.1, 1, 10, 100], name=\"reg_lambda\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.lgb.plot_permutation_importance() or atom.lgb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"LGB\" , n_calls = 50 , bo_params = { \"base_estimator\" : \"ET\" })","title":"LightGBM"},{"location":"API/models/lgb/#lightgbm-lgb","text":"needs scaling accept sparse supports_gpu LightGBM is a gradient boosting model that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages: Faster training speed and higher efficiency. Lower memory usage. Better accuracy. Capable of handling large-scale data. Corresponding estimators are: LGBMClassifier for classification tasks. LGBMRegressor for regression tasks. Read more in LightGBM's documentation . Info LightGBM allows early stopping to stop the training of unpromising models prematurely. Info Using LightGBM's GPU implementation requires extra installations .","title":"LightGBM (LGB)"},{"location":"API/models/lgb/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: n_estimators: int, default=100 Integer(20, 500, name=\"n_estimators\") learning_rate: float, default=0.1 Real(0.01, 1.0, \"log-uniform\", name=\"learning_rate\") max_depth: int, default=-1 Categorical([-1, *range(1, 17)], name=\"max_depth\") num_leaves: int, default=31 Integer(20, 40, name=\"num_leaves\") min_child_weight: float, default=0.001 Categorical([1e-4, 1e-3, 0.01, 0.1, 1, 10, 100], name=\"min_child_weight\"), min_child_samples: int, default=20 Integer(1, 30, name=\"min_child_samples\") subsample: float, default=1.0 Categorical(np.linspace(0.5, 1.0, 6), name=\"subsample\") colsample_bytree: float, default=1.0 Categorical(np.linspace(0.4, 1.0, 7), name=\"colsample_by_level\") reg_alpha: float, default=0.0 Categorical([0, 0.01, 0.1, 1, 10, 100], name=\"reg_alpha\") reg_lambda: float, default=0.0 Categorical([0, 0.01, 0.1, 1, 10, 100], name=\"reg_lambda\")","title":"Hyperparameters"},{"location":"API/models/lgb/#attributes","text":"","title":"Attributes"},{"location":"API/models/lgb/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/lgb/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/lgb/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/lgb/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.lgb.plot_permutation_importance() or atom.lgb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/lgb/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"LGB\" , n_calls = 50 , bo_params = { \"base_estimator\" : \"ET\" })","title":"Example"},{"location":"API/models/lr/","text":"Logistic regression (LR) needs scaling accept sparse supports_gpu Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function. Corresponding estimators are: LogisticRegression for classification tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The penalty parameter is always set to \"l2\" when penalty=\"none\" and solver=\"liblinear\". The penalty parameter is always set to \"l2\" when penalty=\"l1\" and solver!=\"liblinear\" or \"saga\". The penalty parameter is always set to \"l2\" when penalty=\"elasticnet\" and solver!=\"saga\". The C parameter is not used when penalty=\"none\". The l1_ratio parameter is only used when penalty=\"elasticnet\". The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: penalty: str, default=\"l2\" Categorical([\"l1\", \"l2\", \"elasticnet\", \"none\"], name=\"penalty\") C: float, default=1.0 Real(1e-3, 100, \"log-uniform\", name=\"C\") solver: str, default=\"lbfgs\" Categorical([\"lbfgs\", \"newton-cg\", \"liblinear\", \"sag\", \"saga\"], name=\"solver\") max_iter: int, default=100 Integer(100, 1000, name=\"max_iter\") l1_ratio: float, default=None Categorical([None, *np.linspace(0.1, 0.9, 9)], name=\"l1_ratio\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set. decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.lr.plot_permutation_importance() or atom.lr.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"LR\" )","title":"Logistic Regression"},{"location":"API/models/lr/#logistic-regression-lr","text":"needs scaling accept sparse supports_gpu Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function. Corresponding estimators are: LogisticRegression for classification tasks. Read more in sklearn's documentation .","title":"Logistic regression (LR)"},{"location":"API/models/lr/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The penalty parameter is always set to \"l2\" when penalty=\"none\" and solver=\"liblinear\". The penalty parameter is always set to \"l2\" when penalty=\"l1\" and solver!=\"liblinear\" or \"saga\". The penalty parameter is always set to \"l2\" when penalty=\"elasticnet\" and solver!=\"saga\". The C parameter is not used when penalty=\"none\". The l1_ratio parameter is only used when penalty=\"elasticnet\". The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: penalty: str, default=\"l2\" Categorical([\"l1\", \"l2\", \"elasticnet\", \"none\"], name=\"penalty\") C: float, default=1.0 Real(1e-3, 100, \"log-uniform\", name=\"C\") solver: str, default=\"lbfgs\" Categorical([\"lbfgs\", \"newton-cg\", \"liblinear\", \"sag\", \"saga\"], name=\"solver\") max_iter: int, default=100 Integer(100, 1000, name=\"max_iter\") l1_ratio: float, default=None Categorical([None, *np.linspace(0.1, 0.9, 9)], name=\"l1_ratio\")","title":"Hyperparameters"},{"location":"API/models/lr/#attributes","text":"","title":"Attributes"},{"location":"API/models/lr/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/lr/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/lr/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set. decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/lr/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.lr.plot_permutation_importance() or atom.lr.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/lr/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"LR\" )","title":"Example"},{"location":"API/models/lsvm/","text":"Linear SVM (lSVM) needs scaling accept sparse supports_gpu Similar to Kernel SVM but with a linear kernel. Implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. The multiclass support is handled according to a one-vs-rest scheme. Corresponding estimators are: LinearSVC for classification tasks. LinearSVR for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The penalty parameter is only used with LinearSVC. The penalty parameter is always set to \"l2\" when loss=\"hinge\". The dual parameter is always set to False when penalty=\"l1\" and loss=\"squared_hinge\". The dual parameter is always set to False when penalty=\"l2\" and loss=\"hinge\". The dual parameter is always set to True when loss=\"epsilon_insensitive\". The random_state parameter is set equal to that of the training instance. Dimensions: loss: str classifier: default=\"squared_hinge\" Categorical([\"hinge\", \"squared_hinge\"], name=\"loss\") regressor: default=\"epsilon_insensitive\" Categorical([\"epsilon_insensitive\", \"squared_epsilon_insensitive\"], name=\"loss\") C: float, default=1.0 Real(1e-3, 100, \"log-uniform\", name=\"C\") penalty: str, default=\"l2\" Categorical([\"l1\", \"l2\"], name=\"penalty\"). Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set (only if classifier). decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.lsvm.plot_permutation_importance() or atom.lsvm.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"lSVM\" , metric = \"accuracy\" , n_calls = 10 )","title":"Linear SVM"},{"location":"API/models/lsvm/#linear-svm-lsvm","text":"needs scaling accept sparse supports_gpu Similar to Kernel SVM but with a linear kernel. Implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. The multiclass support is handled according to a one-vs-rest scheme. Corresponding estimators are: LinearSVC for classification tasks. LinearSVR for regression tasks. Read more in sklearn's documentation .","title":"Linear SVM (lSVM)"},{"location":"API/models/lsvm/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The penalty parameter is only used with LinearSVC. The penalty parameter is always set to \"l2\" when loss=\"hinge\". The dual parameter is always set to False when penalty=\"l1\" and loss=\"squared_hinge\". The dual parameter is always set to False when penalty=\"l2\" and loss=\"hinge\". The dual parameter is always set to True when loss=\"epsilon_insensitive\". The random_state parameter is set equal to that of the training instance. Dimensions: loss: str classifier: default=\"squared_hinge\" Categorical([\"hinge\", \"squared_hinge\"], name=\"loss\") regressor: default=\"epsilon_insensitive\" Categorical([\"epsilon_insensitive\", \"squared_epsilon_insensitive\"], name=\"loss\") C: float, default=1.0 Real(1e-3, 100, \"log-uniform\", name=\"C\") penalty: str, default=\"l2\" Categorical([\"l1\", \"l2\"], name=\"penalty\").","title":"Hyperparameters"},{"location":"API/models/lsvm/#attributes","text":"","title":"Attributes"},{"location":"API/models/lsvm/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/lsvm/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/lsvm/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set (only if classifier). decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/lsvm/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.lsvm.plot_permutation_importance() or atom.lsvm.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/lsvm/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"lSVM\" , metric = \"accuracy\" , n_calls = 10 )","title":"Example"},{"location":"API/models/mlp/","text":"Multi-layer Perceptron (MLP) needs scaling accept sparse Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function by training on a dataset. Given a set of features and a target, it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. Corresponding estimators are: MLPClassifier for classification tasks. MLPRegressor for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The MLP optimizes three hidden layers by default, but any number of hidden layers can be tuned adding hidden_layer_n to the custom dimensions . The learning_rate and power_t parameters are only used when solver=\"lbfgs\". The learning_rate_init parameter is only used when solver!=\"lbfgs\". The random_state parameter is set equal to that of the parent. Dimensions: hidden_layer_[n]: tuple, default=(100,) Integer(10, 100, name=\"hidden_layer_1\") Integer(0, 100, name=\"hidden_layer_2\") Integer(0, 100, name=\"hidden_layer_3\") activation: str, default=\"relu\" Categorical([\"identity\", \"logistic\", \"tanh\", \"relu\"], name=\"activation\") solver: str, default=\"adam\" Categorical([\"lbfgs\", \"sgd\", \"adam\"], name=\"solver\") alpha: float, default=1e-4 Real(1e-4, 0.1, \"log-uniform\", name=\"alpha\") batch_size: str or int, default=\"auto\" [\"auto\", 8, 16, 32, 64, 128, 256] learning_rate: str, default=\"constant\" Categorical([\"constant\", \"invscaling\", \"adaptive\"], name=\"learning_rate\"). learning_rate_init: float, default=1e-3 Real(1e-3, 0.1, \"log-uniform\", name=\"learning_rate_init\"). power_t: float, default=0.5 Categorical(np.linspace(0.1, 0.9, 9), name=\"power_t\"). max_iter: int, default=200 Integer(50, 500, name=\"max_iter\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.mlp.plot_permutation_importance() or atom.mlp.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"MLP\" , est_params = { \"solver\" : \"sgd\" , \"activation\" : \"relu\" })","title":"Multi-layer Perceptron"},{"location":"API/models/mlp/#multi-layer-perceptron-mlp","text":"needs scaling accept sparse Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function by training on a dataset. Given a set of features and a target, it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. Corresponding estimators are: MLPClassifier for classification tasks. MLPRegressor for regression tasks. Read more in sklearn's documentation .","title":"Multi-layer Perceptron (MLP)"},{"location":"API/models/mlp/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The MLP optimizes three hidden layers by default, but any number of hidden layers can be tuned adding hidden_layer_n to the custom dimensions . The learning_rate and power_t parameters are only used when solver=\"lbfgs\". The learning_rate_init parameter is only used when solver!=\"lbfgs\". The random_state parameter is set equal to that of the parent. Dimensions: hidden_layer_[n]: tuple, default=(100,) Integer(10, 100, name=\"hidden_layer_1\") Integer(0, 100, name=\"hidden_layer_2\") Integer(0, 100, name=\"hidden_layer_3\") activation: str, default=\"relu\" Categorical([\"identity\", \"logistic\", \"tanh\", \"relu\"], name=\"activation\") solver: str, default=\"adam\" Categorical([\"lbfgs\", \"sgd\", \"adam\"], name=\"solver\") alpha: float, default=1e-4 Real(1e-4, 0.1, \"log-uniform\", name=\"alpha\") batch_size: str or int, default=\"auto\" [\"auto\", 8, 16, 32, 64, 128, 256] learning_rate: str, default=\"constant\" Categorical([\"constant\", \"invscaling\", \"adaptive\"], name=\"learning_rate\"). learning_rate_init: float, default=1e-3 Real(1e-3, 0.1, \"log-uniform\", name=\"learning_rate_init\"). power_t: float, default=0.5 Categorical(np.linspace(0.1, 0.9, 9), name=\"power_t\"). max_iter: int, default=200 Integer(50, 500, name=\"max_iter\")","title":"Hyperparameters"},{"location":"API/models/mlp/#attributes","text":"","title":"Attributes"},{"location":"API/models/mlp/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/mlp/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/mlp/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/mlp/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.mlp.plot_permutation_importance() or atom.mlp.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/mlp/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"MLP\" , est_params = { \"solver\" : \"sgd\" , \"activation\" : \"relu\" })","title":"Example"},{"location":"API/models/mnb/","text":"Multinomial Naive Bayes (MNB) accept sparse supports_gpu Multinomial Naive Bayes implements the Naive Bayes algorithm for multinomially distributed data, and is one of the two classic Naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). Corresponding estimators are: MultinomialNB for classification tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: alpha: float, default=1.0 Real(0.01, 10, \"log-uniform\", name=\"alpha\") fit_prior: bool, default=True Categorical([True, False], name=\"fit_prior\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.mnb.plot_permutation_importance() or atom.mnb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"MNB\" , metric = \"precision\" )","title":"Multinomial Naive Bayes"},{"location":"API/models/mnb/#multinomial-naive-bayes-mnb","text":"accept sparse supports_gpu Multinomial Naive Bayes implements the Naive Bayes algorithm for multinomially distributed data, and is one of the two classic Naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). Corresponding estimators are: MultinomialNB for classification tasks. Read more in sklearn's documentation .","title":"Multinomial Naive Bayes (MNB)"},{"location":"API/models/mnb/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: alpha: float, default=1.0 Real(0.01, 10, \"log-uniform\", name=\"alpha\") fit_prior: bool, default=True Categorical([True, False], name=\"fit_prior\")","title":"Hyperparameters"},{"location":"API/models/mnb/#attributes","text":"","title":"Attributes"},{"location":"API/models/mnb/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/mnb/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/mnb/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/mnb/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.mnb.plot_permutation_importance() or atom.mnb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/mnb/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"MNB\" , metric = \"precision\" )","title":"Example"},{"location":"API/models/ols/","text":"Ordinary Least Squares (OLS) needs scaling accept sparse supports_gpu Ordinary Least Squares is just linear regression without any regularization. It fits a linear model with coefficients w=(w1, \u2026, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Corresponding estimators are: LinearRegression for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The n_jobs parameter is set equal to that of the parent. OLS has no parameters to tune with the BO. Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.ols.plot_permutation_importance() or atom.ols.predict(X) . The remaining utility methods can be found hereunder. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", sample_weight=None) [source] Get the model's score for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"OLS\" )","title":"Ordinary Least Squares"},{"location":"API/models/ols/#ordinary-least-squares-ols","text":"needs scaling accept sparse supports_gpu Ordinary Least Squares is just linear regression without any regularization. It fits a linear model with coefficients w=(w1, \u2026, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Corresponding estimators are: LinearRegression for regression tasks. Read more in sklearn's documentation .","title":"Ordinary Least Squares (OLS)"},{"location":"API/models/ols/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The n_jobs parameter is set equal to that of the parent. OLS has no parameters to tune with the BO.","title":"Hyperparameters"},{"location":"API/models/ols/#attributes","text":"","title":"Attributes"},{"location":"API/models/ols/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/ols/#utility-attributes","text":"Attributes: estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/ols/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/ols/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.ols.plot_permutation_importance() or atom.ols.predict(X) . The remaining utility methods can be found hereunder. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", sample_weight=None) [source] Get the model's score for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/ols/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"OLS\" )","title":"Example"},{"location":"API/models/pa/","text":"Passive Aggressive (PA) needs scaling accept sparse The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter C. Corresponding estimators are: PassiveAggressiveClassifier for classification tasks. PassiveAggressiveRegressor for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: C: float, default=1.0 Real(1e-3, 100, \"log-uniform\", name=\"C\") loss: str classifier: default=\"hinge\" Categorical([\"hinge\", \"squared_hinge\"], name=\"loss\") regressor: default=\"epsilon_insensitive\" Categorical([\"epsilon_insensitive\", \"squared_epsilon_insensitive\"], name=\"loss\") average: float, default=False Categorical([True, False], name=\"average\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set (only if classifier). decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.pa.plot_permutation_importance() or atom.pa.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"PA\" , metric = \"f1\" )","title":"Passive Aggressive"},{"location":"API/models/pa/#passive-aggressive-pa","text":"needs scaling accept sparse The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter C. Corresponding estimators are: PassiveAggressiveClassifier for classification tasks. PassiveAggressiveRegressor for regression tasks. Read more in sklearn's documentation .","title":"Passive Aggressive (PA)"},{"location":"API/models/pa/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: C: float, default=1.0 Real(1e-3, 100, \"log-uniform\", name=\"C\") loss: str classifier: default=\"hinge\" Categorical([\"hinge\", \"squared_hinge\"], name=\"loss\") regressor: default=\"epsilon_insensitive\" Categorical([\"epsilon_insensitive\", \"squared_epsilon_insensitive\"], name=\"loss\") average: float, default=False Categorical([True, False], name=\"average\")","title":"Hyperparameters"},{"location":"API/models/pa/#attributes","text":"","title":"Attributes"},{"location":"API/models/pa/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/pa/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/pa/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set (only if classifier). decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/pa/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.pa.plot_permutation_importance() or atom.pa.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/pa/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"PA\" , metric = \"f1\" )","title":"Example"},{"location":"API/models/perc/","text":"Perceptron (Perc) needs scaling The Perceptron is a simple classification algorithm suitable for large scale learning. By default: It does not require a learning rate. It is not regularized (penalized). It updates its model only on mistakes. The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser. Corresponding estimators are: Perceptron for classification tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The l1_ratio parameter is only used when penalty=\"elasticnet\". The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: penalty: str or None, default=None Categorical([None, \"l1\", \"l2\", \"elasticnet\"], name=\"penalty\") alpha: float, default=1e-4 Categorical([1e-4, 1e-3, 1e-2, 0.1, 1, 10], name=\"alpha\") l1_ratio: float, default=0.15 Categorical([0.05, 0.15, 0.30, 0.45, 0.60, 0.75, 0.90], name=\"l1_ratio\") max_iter: int, default=1000 Integer(500, 1500, name=\"max_iter\") eta0: float, default=1.0 Real(1e-2, 10, \"log-uniform\", name=\"eta0\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set. decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.perc.plot_roc() or atom.perc.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"Perc\" )","title":"Perceptron"},{"location":"API/models/perc/#perceptron-perc","text":"needs scaling The Perceptron is a simple classification algorithm suitable for large scale learning. By default: It does not require a learning rate. It is not regularized (penalized). It updates its model only on mistakes. The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser. Corresponding estimators are: Perceptron for classification tasks. Read more in sklearn's documentation .","title":"Perceptron (Perc)"},{"location":"API/models/perc/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The l1_ratio parameter is only used when penalty=\"elasticnet\". The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: penalty: str or None, default=None Categorical([None, \"l1\", \"l2\", \"elasticnet\"], name=\"penalty\") alpha: float, default=1e-4 Categorical([1e-4, 1e-3, 1e-2, 0.1, 1, 10], name=\"alpha\") l1_ratio: float, default=0.15 Categorical([0.05, 0.15, 0.30, 0.45, 0.60, 0.75, 0.90], name=\"l1_ratio\") max_iter: int, default=1000 Integer(500, 1500, name=\"max_iter\") eta0: float, default=1.0 Real(1e-2, 10, \"log-uniform\", name=\"eta0\")","title":"Hyperparameters"},{"location":"API/models/perc/#attributes","text":"","title":"Attributes"},{"location":"API/models/perc/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/perc/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/perc/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set. decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/perc/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.perc.plot_roc() or atom.perc.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/perc/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"Perc\" )","title":"Example"},{"location":"API/models/qda/","text":"Quadratic Discriminant Analysis (QDA) Linear Discriminant Analysis is a classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. Corresponding estimators are: QuadraticDiscriminantAnalysis for classification tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: reg_param: float, default=0 Categorical(np.linspace(0.0, 1.0, 11), name=\"reg_param\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set. decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.qda.plot_permutation_importance() or atom.qda.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"QDA\" )","title":"Quadratic Discriminant Analysis"},{"location":"API/models/qda/#quadratic-discriminant-analysis-qda","text":"Linear Discriminant Analysis is a classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. Corresponding estimators are: QuadraticDiscriminantAnalysis for classification tasks. Read more in sklearn's documentation .","title":"Quadratic Discriminant Analysis (QDA)"},{"location":"API/models/qda/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. Dimensions: reg_param: float, default=0 Categorical(np.linspace(0.0, 1.0, 11), name=\"reg_param\")","title":"Hyperparameters"},{"location":"API/models/qda/#attributes","text":"","title":"Attributes"},{"location":"API/models/qda/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/qda/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/qda/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set. predict_proba_test: pd.DataFrame Predicted class probabilities on the test set. predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set. predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set. decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set. score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/qda/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.qda.plot_permutation_importance() or atom.qda.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the estimator. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute and is logged to any active mlflow experiment. Since the estimator changed, all the model's prediction attributes are reset. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/qda/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"QDA\" )","title":"Example"},{"location":"API/models/rf/","text":"Random Forest (RF) accept sparse supports_gpu Random forests are an ensemble learning method that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random forests correct for decision trees' habit of overfitting to their training set. Corresponding estimators are: RandomForestClassifier for classification tasks. RandomForestRegressor for regression tasks. Read more in sklearn's documentation . Warning The GPU implementation of RandomForestClassifier only supports predictions on dtype float32 . Convert all dtypes before calling the run method to avoid exceptions. Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The max_samples parameter is only used when bootstrap=True. The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: n_estimators: int, default=100 Integer(10, 500, name=\"n_estimators\") criterion: str classifier: default=\"gini\" Categorical([\"gini\", \"entropy\"], name=\"criterion\") regressor: default=\"squared_error\" Categorical([\"squared_error\", \"absolute_error\", \"poisson\"], name=\"criterion\") max_depth: int or None, default=None Categorical([None, *range(1, 17)], name=\"max_depth\") min_samples_split: int, default=2 Integer(2, 20, name=\"min_samples_split\") min_samples_leaf: int, default=1 Integer(1, 20, name=\"min_samples_leaf\") max_features: str, float or None, default=\"auto\" Categorical([\"auto\", \"sqrt\", \"log2\", *np.linspace(0.5, 0.9, 5), None], name=\"max_features\") bootstrap: bool, default=False Categorical([True, False], name=\"bootstrap\") max_samples: float or None, default=None Categorical([None, *np.linspace(0.5, 0.9, 5)], name=\"max_samples\") ccp_alpha: float, default=0 Real(0, 0.035, name=\"ccp_alpha\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.rf.plot_permutation_importance() or atom.rf.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"RF\" , metric = \"mae\" , n_calls = 20 , n_initial_points = 10 )","title":"Random Forest"},{"location":"API/models/rf/#random-forest-rf","text":"accept sparse supports_gpu Random forests are an ensemble learning method that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random forests correct for decision trees' habit of overfitting to their training set. Corresponding estimators are: RandomForestClassifier for classification tasks. RandomForestRegressor for regression tasks. Read more in sklearn's documentation . Warning The GPU implementation of RandomForestClassifier only supports predictions on dtype float32 . Convert all dtypes before calling the run method to avoid exceptions.","title":"Random Forest (RF)"},{"location":"API/models/rf/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The max_samples parameter is only used when bootstrap=True. The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: n_estimators: int, default=100 Integer(10, 500, name=\"n_estimators\") criterion: str classifier: default=\"gini\" Categorical([\"gini\", \"entropy\"], name=\"criterion\") regressor: default=\"squared_error\" Categorical([\"squared_error\", \"absolute_error\", \"poisson\"], name=\"criterion\") max_depth: int or None, default=None Categorical([None, *range(1, 17)], name=\"max_depth\") min_samples_split: int, default=2 Integer(2, 20, name=\"min_samples_split\") min_samples_leaf: int, default=1 Integer(1, 20, name=\"min_samples_leaf\") max_features: str, float or None, default=\"auto\" Categorical([\"auto\", \"sqrt\", \"log2\", *np.linspace(0.5, 0.9, 5), None], name=\"max_features\") bootstrap: bool, default=False Categorical([True, False], name=\"bootstrap\") max_samples: float or None, default=None Categorical([None, *np.linspace(0.5, 0.9, 5)], name=\"max_samples\") ccp_alpha: float, default=0 Real(0, 0.035, name=\"ccp_alpha\")","title":"Hyperparameters"},{"location":"API/models/rf/#attributes","text":"","title":"Attributes"},{"location":"API/models/rf/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/rf/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/rf/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/rf/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.rf.plot_permutation_importance() or atom.rf.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/rf/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"RF\" , metric = \"mae\" , n_calls = 20 , n_initial_points = 10 )","title":"Example"},{"location":"API/models/ridge/","text":"Ridge Estimator (Ridge) needs scaling accept sparse supports_gpu Linear least squares with l2 regularization. If classifier, it first converts the target values into {-1, 1} and then treats the problem as a regression task. Corresponding estimators are: RidgeClassifier for classification tasks. Ridge for regression tasks. Read more in sklearn's documentation . Warning The gpu implementation is only available for regression tasks. Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The random_state parameter is set equal to that of the parent. Dimensions: alpha: float, default=1.0 Real(1e-3, 10, \"log-uniform\", name=\"alpha\") solver: str, default=\"auto\" Categorical([\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"], name=\"solver\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the model, e.g. atom.ridge.plot_permutation_importance() or atom.ridge.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"Ridge\" )","title":"Ridge Estimator"},{"location":"API/models/ridge/#ridge-estimator-ridge","text":"needs scaling accept sparse supports_gpu Linear least squares with l2 regularization. If classifier, it first converts the target values into {-1, 1} and then treats the problem as a regression task. Corresponding estimators are: RidgeClassifier for classification tasks. Ridge for regression tasks. Read more in sklearn's documentation . Warning The gpu implementation is only available for regression tasks.","title":"Ridge Estimator (Ridge)"},{"location":"API/models/ridge/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The random_state parameter is set equal to that of the parent. Dimensions: alpha: float, default=1.0 Real(1e-3, 10, \"log-uniform\", name=\"alpha\") solver: str, default=\"auto\" Categorical([\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"], name=\"solver\")","title":"Hyperparameters"},{"location":"API/models/ridge/#attributes","text":"","title":"Attributes"},{"location":"API/models/ridge/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/ridge/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/ridge/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/ridge/#methods","text":"The majority of the plots and prediction methods can be called directly from the model, e.g. atom.ridge.plot_permutation_importance() or atom.ridge.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/ridge/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"Ridge\" )","title":"Example"},{"location":"API/models/rnn/","text":"Radius Nearest Neighbors (RNN) needs scaling accept sparse Radius Nearest Neighbors implements the nearest neighbors vote, where the neighbors are selected from within a given radius. For regression, the target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set. Corresponding estimators are: RadiusNeighborsClassifier for classification tasks. RadiusNeighborsRegressor for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The outlier_label parameter is set by default to \"most_frequent\" to avoid errors when encountering outliers. The n_jobs parameter is set equal to that of the parent. Dimensions: radius: float, default=mean(distances) Real(min(distances), max(distances), name=\"radius\") weights: str, default=\"uniform\" Categorical([\"uniform\", \"distance\"], name=\"weights\") algorithm: str, default=\"auto\" Categorical([\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"], name=\"algorithm\") leaf_size: int, default=30 Integer(20, 40, name=\"leaf_size\") p: int, default=2 Integer(1, 2, name=\"p\") Info Since the optimal radius depends hugely on the data, ATOM's RNN implementation doesn't use sklearn's default radius of 1, but instead calculates the euclidean distance between the maximum of 100 or 1% of random samples in the training set and uses the mean of those distances as default radius. The lower and upper bounds of the radius' dimensions for the BO are given by the minimum and maximum value of the calculated distances. Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.rnn.plot_permutation_importance() or atom.rnn.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"RNN\" , metric = \"precision\" , est_params = { \"radius\" : 3.5 })","title":"Radius Nearest Neighbors"},{"location":"API/models/rnn/#radius-nearest-neighbors-rnn","text":"needs scaling accept sparse Radius Nearest Neighbors implements the nearest neighbors vote, where the neighbors are selected from within a given radius. For regression, the target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set. Corresponding estimators are: RadiusNeighborsClassifier for classification tasks. RadiusNeighborsRegressor for regression tasks. Read more in sklearn's documentation .","title":"Radius Nearest Neighbors (RNN)"},{"location":"API/models/rnn/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The outlier_label parameter is set by default to \"most_frequent\" to avoid errors when encountering outliers. The n_jobs parameter is set equal to that of the parent. Dimensions: radius: float, default=mean(distances) Real(min(distances), max(distances), name=\"radius\") weights: str, default=\"uniform\" Categorical([\"uniform\", \"distance\"], name=\"weights\") algorithm: str, default=\"auto\" Categorical([\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"], name=\"algorithm\") leaf_size: int, default=30 Integer(20, 40, name=\"leaf_size\") p: int, default=2 Integer(1, 2, name=\"p\") Info Since the optimal radius depends hugely on the data, ATOM's RNN implementation doesn't use sklearn's default radius of 1, but instead calculates the euclidean distance between the maximum of 100 or 1% of random samples in the training set and uses the mean of those distances as default radius. The lower and upper bounds of the radius' dimensions for the BO are given by the minimum and maximum value of the calculated distances.","title":"Hyperparameters"},{"location":"API/models/rnn/#attributes","text":"","title":"Attributes"},{"location":"API/models/rnn/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/rnn/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/rnn/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/rnn/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.rnn.plot_permutation_importance() or atom.rnn.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/rnn/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"RNN\" , metric = \"precision\" , est_params = { \"radius\" : 3.5 })","title":"Example"},{"location":"API/models/sgd/","text":"Stochastic Gradient Descent (SGD) needs scaling accept sparse Stochastic Gradient Descent is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning. Corresponding estimators are: SGDClassifier for classification tasks. SGDRegressor for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The l1_ratio parameter is only used when penalty=\"elasticnet\". The eta0 parameter is only used when learning_rate!=\"optimal\". The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: loss: str classifier: default=\"hinge\" Categorical([\"hinge\", \"log\", \"modified_huber\", \"squared_hinge\", \"perceptron\"], name=\"loss\") regressor: default=\"squared_error\" Categorical([\"squared_error\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"], name=\"loss\") penalty: str, default=\"l2\" Categorical([\"none\", \"l1\", \"l2\", \"elasticnet\"], name=\"penalty\") alpha: float, default=1e-4 Real(1e-5, 1.0, \"log-uniform\", name=\"alpha\") l1_ratio: float, default=0.15 Categorical([0.05, 0.15, 0.30, 0.45, 0.60, 0.75, 0.90], name=\"l1_ratio\"). epsilon: float, default=0.1 Real(1e-4, 1.0, \"log-uniform\", name=\"epsilon\") learning_rate: str, default=\"optimal\" Categorical([\"constant\", \"invscaling\", \"optimal\", \"adaptive\"], name=\"learning_rate\") eta0: float, default=1e-2 Real(1e-2, 10, \"log-uniform\", name=\"eta0\"). power_t: float, default=0.5 Categorical(np.linspace(0.1, 0.9, 9), name=\"power_t\") average: bool, default=False Categorical([True, False], name=\"average\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set (only if classifier). decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.sgd.plot_permutation_importance() or atom.sgd.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"SGD\" , metric = \"recall\" , bo_params = { \"cv\" : 3 })","title":"Stochastic Gradient Descent"},{"location":"API/models/sgd/#stochastic-gradient-descent-sgd","text":"needs scaling accept sparse Stochastic Gradient Descent is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning. Corresponding estimators are: SGDClassifier for classification tasks. SGDRegressor for regression tasks. Read more in sklearn's documentation .","title":"Stochastic Gradient Descent (SGD)"},{"location":"API/models/sgd/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The l1_ratio parameter is only used when penalty=\"elasticnet\". The eta0 parameter is only used when learning_rate!=\"optimal\". The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: loss: str classifier: default=\"hinge\" Categorical([\"hinge\", \"log\", \"modified_huber\", \"squared_hinge\", \"perceptron\"], name=\"loss\") regressor: default=\"squared_error\" Categorical([\"squared_error\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"], name=\"loss\") penalty: str, default=\"l2\" Categorical([\"none\", \"l1\", \"l2\", \"elasticnet\"], name=\"penalty\") alpha: float, default=1e-4 Real(1e-5, 1.0, \"log-uniform\", name=\"alpha\") l1_ratio: float, default=0.15 Categorical([0.05, 0.15, 0.30, 0.45, 0.60, 0.75, 0.90], name=\"l1_ratio\"). epsilon: float, default=0.1 Real(1e-4, 1.0, \"log-uniform\", name=\"epsilon\") learning_rate: str, default=\"optimal\" Categorical([\"constant\", \"invscaling\", \"optimal\", \"adaptive\"], name=\"learning_rate\") eta0: float, default=1e-2 Real(1e-2, 10, \"log-uniform\", name=\"eta0\"). power_t: float, default=0.5 Categorical(np.linspace(0.1, 0.9, 9), name=\"power_t\") average: bool, default=False Categorical([True, False], name=\"average\")","title":"Hyperparameters"},{"location":"API/models/sgd/#attributes","text":"","title":"Attributes"},{"location":"API/models/sgd/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/sgd/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/sgd/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. decision_function_train: pd.Series or pd.DataFrame Predicted confidence scores on the training set (only if classifier). decision_function_test: pd.Series or pd.DataFrame Predicted confidence scores on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/sgd/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.sgd.plot_permutation_importance() or atom.sgd.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/sgd/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( models = \"SGD\" , metric = \"recall\" , bo_params = { \"cv\" : 3 })","title":"Example"},{"location":"API/models/tree/","text":"Decision Tree (Tree) accept sparse A single decision tree classifier/regressor. Corresponding estimators are: DecisionTreeClassifier for classification tasks. DecisionTreeRegressor for regression tasks. Read more in sklearn's documentation . Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The random_state parameter is set equal to that of the parent. Dimensions: criterion: str classifier: default=\"gini\" Categorical([\"gini\", \"entropy\"], name=\"criterion\") regressor: default=\"squared_error\" Categorical([\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"], name=\"criterion\") splitter: str, default=\"best\" Categorical([\"best\", \"random\"], name=\"splitter\") max_depth: int or None, default=None Categorical([None, *range(1, 17)], name=\"max_depth\") min_samples_split: int, default=2 Integer(2, 20, name=\"min_samples_split\") min_samples_leaf: int, default=1 Integer(1, 20, name=\"min_samples_leaf\") max_features: str, float or None, default=None Categorical([\"auto\", \"sqrt\", \"log2\", *np.linspace(0.5, 0.9, 5), None], name=\"max_features\") ccp_alpha: float, default=0.0 Real(0, 0.035, name=\"ccp_alpha\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.tree.plot_permutation_importance() or atom.tree.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"Tree\" , metric = \"MSLE\" )","title":"Decision Tree"},{"location":"API/models/tree/#decision-tree-tree","text":"accept sparse A single decision tree classifier/regressor. Corresponding estimators are: DecisionTreeClassifier for classification tasks. DecisionTreeRegressor for regression tasks. Read more in sklearn's documentation .","title":"Decision Tree (Tree)"},{"location":"API/models/tree/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The random_state parameter is set equal to that of the parent. Dimensions: criterion: str classifier: default=\"gini\" Categorical([\"gini\", \"entropy\"], name=\"criterion\") regressor: default=\"squared_error\" Categorical([\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"], name=\"criterion\") splitter: str, default=\"best\" Categorical([\"best\", \"random\"], name=\"splitter\") max_depth: int or None, default=None Categorical([None, *range(1, 17)], name=\"max_depth\") min_samples_split: int, default=2 Integer(2, 20, name=\"min_samples_split\") min_samples_leaf: int, default=1 Integer(1, 20, name=\"min_samples_leaf\") max_features: str, float or None, default=None Categorical([\"auto\", \"sqrt\", \"log2\", *np.linspace(0.5, 0.9, 5), None], name=\"max_features\") ccp_alpha: float, default=0.0 Real(0, 0.035, name=\"ccp_alpha\")","title":"Hyperparameters"},{"location":"API/models/tree/#attributes","text":"","title":"Attributes"},{"location":"API/models/tree/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/tree/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/tree/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/tree/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.tree.plot_permutation_importance() or atom.tree.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/tree/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"Tree\" , metric = \"MSLE\" )","title":"Example"},{"location":"API/models/xgb/","text":"XGBoost (XGB) needs scaling accept sparse supports_gpu XGBoost is an optimized distributed gradient boosting model designed to be highly efficient, flexible and portable. XGBoost provides a parallel tree boosting that solve many data science problems in a fast and accurate way. Corresponding estimators are: XGBClassifier for classification tasks. XGBRegressor for regression tasks. Read more in XGBoost's documentation . Info XGBoost allows early stopping to stop the training of unpromising models prematurely. Hyperparameters By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: n_estimators: int, default=100 Integer(20, 500, name=\"n_estimators\") learning_rate: float, default=0.1 Real(0.01, 1.0, \"log-uniform\", name=\"learning_rate\") max_depth: int, default=6 Integer(1, 20, name=\"max_depth\") gamma: float, default=0.0 Real(0, 1.0, name=\"gamma\") min_child_weight: int, default=1 Integer(1, 10, name=\"min_child_weight\") subsample: float, default=1.0 Categorical(np.linspace(0.5, 1.0, 6), name=\"subsample\") colsample_by_tree: float, default=1.0 Categorical(np.linspace(0.4, 1.0, 7), name=\"colsample_by_tree\") reg_alpha: float, default=0.0 Categorical([0, 0.01, 0.1, 1, 10, 100], name=\"reg_alpha\") reg_lambda: float, default=1.0 Categorical([0, 0.01, 0.1, 1, 10, 100], name=\"reg_lambda\") Attributes Data attributes Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Prediction attributes The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set. Methods The majority of the plots and prediction methods can be called directly from the models, e.g. atom.xgb.plot_permutation_importance() or atom.xgb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"XGB\" , metric = \"me\" , n_calls = 25 , bo_params = { \"cv\" : 1 })","title":"XGBoost"},{"location":"API/models/xgb/#xgboost-xgb","text":"needs scaling accept sparse supports_gpu XGBoost is an optimized distributed gradient boosting model designed to be highly efficient, flexible and portable. XGBoost provides a parallel tree boosting that solve many data science problems in a fast and accurate way. Corresponding estimators are: XGBClassifier for classification tasks. XGBRegressor for regression tasks. Read more in XGBoost's documentation . Info XGBoost allows early stopping to stop the training of unpromising models prematurely.","title":"XGBoost (XGB)"},{"location":"API/models/xgb/#hyperparameters","text":"By default, the estimator adopts the default parameters provided by its package. See the user guide on how to customize them. The n_jobs and random_state parameters are set equal to those of the parent. Dimensions: n_estimators: int, default=100 Integer(20, 500, name=\"n_estimators\") learning_rate: float, default=0.1 Real(0.01, 1.0, \"log-uniform\", name=\"learning_rate\") max_depth: int, default=6 Integer(1, 20, name=\"max_depth\") gamma: float, default=0.0 Real(0, 1.0, name=\"gamma\") min_child_weight: int, default=1 Integer(1, 10, name=\"min_child_weight\") subsample: float, default=1.0 Categorical(np.linspace(0.5, 1.0, 6), name=\"subsample\") colsample_by_tree: float, default=1.0 Categorical(np.linspace(0.4, 1.0, 7), name=\"colsample_by_tree\") reg_alpha: float, default=0.0 Categorical([0, 0.01, 0.1, 1, 10, 100], name=\"reg_alpha\") reg_lambda: float, default=1.0 Categorical([0, 0.01, 0.1, 1, 10, 100], name=\"reg_lambda\")","title":"Hyperparameters"},{"location":"API/models/xgb/#attributes","text":"","title":"Attributes"},{"location":"API/models/xgb/#data-attributes","text":"Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/models/xgb/#utility-attributes","text":"Attributes: bo: pd.DataFrame Information of every step taken by the BO. Columns include: call : Name of the call. params : Parameters used in the model. estimator : Estimator used for this iteration (fitted on last cross-validation). score : Score of the chosen metric. List of scores for multi-metric. time : Time spent on this iteration. total_time : Total time spent since the start of the BO. best_call: str Name of the best call in the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. estimator: class Estimator instance with the best combination of hyperparameters fitted on the complete training set. time_bo: str Time it took to run the bayesian optimization algorithm. metric_bo: float or list Best metric score(s) on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric(s) on the test set. metric_train: float or list Metric score(s) on the training set. metric_test: float or list Metric score(s) on the test set. metric_bootstrap: np.array Bootstrap results with shape=(n_bootstrap,) for single-metric runs and shape=(metric, n_bootstrap) for multi-metric runs. mean_bootstrap: float or list Mean of the bootstrap results. List of values for multi-metric runs. std_bootstrap: float or list Standard deviation of the bootstrap results. List of values for multi-metric runs. results: pd.Series Training results. Columns include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/models/xgb/#prediction-attributes","text":"The prediction attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to calculate attributes that are never used, saving time and memory. Prediction attributes: predict_train: pd.Series Class predictions on the training set. predict_test: pd.Series Class predictions on the test set. predict_proba_train: pd.DataFrame Predicted class probabilities on the training set (only if classifier). predict_proba_test: pd.DataFrame Predicted class probabilities on the test set (only if classifier). predict_log_proba_train: pd.DataFrame Predicted class log-probabilities on the training set (only if classifier). predict_log_proba_test: pd.DataFrame Predicted class log-probabilities on the test set (only if classifier). score_train: float Model's score on the training set. score_test: float Model's score on the test set.","title":"Prediction attributes"},{"location":"API/models/xgb/#methods","text":"The majority of the plots and prediction methods can be called directly from the models, e.g. atom.xgb.plot_permutation_importance() or atom.xgb.predict(X) . The remaining utility methods can be found hereunder. calibrate Calibrate the model. clear Clear attributes from the model. cross_validate Evaluate the model using cross-validation. delete Delete the model. dashboard Create an interactive dashboard to analyze the model. evaluate Get the model's scores for the provided metrics. export_pipeline Export the model's pipeline to a sklearn-like Pipeline object. full_train Train the estimator on the complete dataset. inverse_transform Inversely transform new data through the pipeline. rename Change the model's tag. save_estimator Save the estimator to a pickle file. transform Transform new data through the pipeline. method calibrate (**kwargs) [source] Applies probability calibration on the model. The estimator is trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the estimator attribute. If there is an active mlflow experiment, a new run is started using the name [model_name]_calibrate . Since the estimator changed, the model is cleared . Only if classifier. Parameters: **kwargs Additional keyword arguments for sklearn's CalibratedClassifierCV . Using cv=\"prefit\" will use the trained model and fit the calibrator on the test set. Use this only if you have another, independent set for testing. method clear () [source] Reset attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method cross_validate (**kwargs) [source] Evaluate the model using cross-validation. This method cross-validates the whole pipeline on the complete dataset. Use it to assess the robustness of the solution's performance. The return of sklearn's cross_validate function is stored under the cv attribute. Parameters: **kwargs Additional keyword arguments for sklearn's cross_validate function. If the scoring method is not specified, it uses atom's metric. Returns: pd.DataFrame Overview of the results. method delete () [source] If it's the last model in atom, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. The model is not removed from any active mlflow experiment. method dashboard (dataset=\"test\", filename=None, **kwargs) [source] Create an interactive dashboard to analyze the model. The dashboard allows you to investigate SHAP values, permutation importances, interaction effects, partial dependence plots, all kinds of performance plots, and even individual decision trees. By default, the dashboard opens in an external dash app. The created dashboard instance can be accessed through the explainer_dashboard attribute. Read more in the user guide . Parameters: dataset: str, default=\"test\" Data set to get the report from. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". filename: str or None, default=None Name to save the file with (as .html). None to not save anything. **kwargs Additional keyword arguments for the ExplainerDashboard instance. Returns: ExplainerDashboard Created dashboard object. method evaluate (metric=None, dataset=\"test\", threshold=0.5, sample_weight=None) [source] Get the model's scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. sample_weight: sequence or None, default=None Sample weights corresponding to y in dataset . Returns: pd.Series Scores of the model. method export_pipeline (verbose=None) [source] Export the model's pipeline to a sklearn-like Pipeline object. If the model used automated feature scaling , the scaler is added to the pipeline. The returned pipeline is already fitted on the training set. Info ATOM's Pipeline class behaves the same as a sklearn Pipeline , and additionally: Accepts transformers that change the target column. Accepts transformers that drop rows. Accepts transformers that only are fitted on a subset of the provided dataset. Always outputs pandas objects. Uses transformers that are only applied on the training set (see the balance or prune methods) to fit the pipeline, not to make predictions on new data. Parameters: memory: bool, str, Memory or None, default=None Used to cache the fitted transformers of the pipeline. If None or False: No caching is performed. If True: A default temp directory is used. If str: Path to the caching directory. If Memory: Object with the joblib.Memory interface. verbose: int or None, default=None Verbosity level of the transformers in the pipeline. If None, it leaves them to their original verbosity. Note that this is not the pipeline's own verbose parameter. To change that, use the set_params method. Returns: Pipeline Current branch as a sklearn-like Pipeline object. method full_train (include_holdout=False) [source] In some cases it might be desirable to use all available data to train a final model. Note that doing this means that the estimator can no longer be evaluated on the test set. The newly retrained estimator will replace the estimator attribute. If there is an active mlflow experiment, a new run is started with the name [model_name]_full_train . Since the estimator changed, the model is cleared . Warning Although the model is trained on the complete dataset, the pipeline is not! To also get the fully trained pipeline, use: pipeline = atom .export_pipeline().fit(X, y) . Parameters: include_holdout: bool, default=False Whether to include the holdout data set (if available) in the training of the estimator. Note that if True, it means the model can't be evaluated. method inverse_transform (X=None, y=None, verbose=None) [source] Inversely transform new data through the pipeline. Transformers that are only applied on the training set are skipped. The rest should all implement a inverse_transform method. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, inversely transform only the target column. If called from a model that used automated feature scaling, the scaling is inversed as well. Parameters: X: dataframe-like or None, default=None Transformed feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Original feature set. Only returned if provided. pd.Series Original target column. Only returned if provided. method rename (name=None) [source] Change the model's tag. The acronym always stays at the beginning of the model's name. If the model is being tracked by mlflow, the name of the corresponding run is also changed. Parameters: name: str or None, default=None New tag for the model. If None, the tag is removed. method save_estimator (filename=\"auto\") [source] Save the estimator to a pickle file. Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. method transform (X, y=None, verbose=None) [source] Transform new data through the pipeline. Transformers that are only applied on the training set are skipped. If only X or only y is provided, it ignores transformers that require the other parameter. This can be of use to, for example, transform only the target column. If called from a model that used automated feature scaling, the data is scaled as well. Parameters: X: dataframe-like Feature set with shape=(n_samples, n_features). If None, X is ignored in the transformers. y: int, str, dict, sequence or None, default=None If None: y is ignored in the transformers. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns: pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/models/xgb/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( models = \"XGB\" , metric = \"me\" , n_calls = 25 , bo_params = { \"cv\" : 1 })","title":"Example"},{"location":"API/nlp/textcleaner/","text":"TextCleaner class atom.nlp. TextCleaner (decode=True, lower_case=True, drop_email=True, regex_email=None, drop_url=True, regex_url=None, drop_html=True, regex_html=None, drop_emoji=True, regex_emoji=None, drop_number=True, regex_number=None, drop_punctuation=True, verbose=0, logger=None) [source] Applies standard text cleaning to the corpus. Transformations include normalizing characters and dropping noise from the text (emails, HTML tags, URLs, etc...). The transformations are applied on the column named corpus , in the same order the parameters are presented. If there is no column with that name, an exception is raised. This class can be accessed from atom through the textclean method. Read more in the user guide . Parameters decode: bool, default=True Whether to decode unicode characters to their ascii representations. lower_case: bool, default=True Whether to convert all characters to lower case. drop_email: bool, default=True Whether to drop email addresses from the text. regex_email: str, default=None Regex used to search for email addresses. If None, it uses r\"[\\w.-]+@[\\w-]+\\.[\\w.-]+\" . drop_url: bool, default=True Whether to drop URL links from the text. regex_url: str, default=None Regex used to search for URLs. If None, it uses r\"https?://\\S+|www\\.\\S+\" . drop_html: bool, default=True Whether to drop HTML tags from the text. This option is particularly useful if the data was scraped from a website. regex_html: str, default=None Regex used to search for html tags. If None, it uses r\"<.*?>\" . drop_emoji: bool, default=True Whether to drop emojis from the text. regex_emoji: str, default=None Regex used to search for emojis. If None, it uses r\":[a-z_]+:\" . drop_number: bool, default=False Whether to drop numbers from the text. regex_number: str, default=None Regex used to search for numbers. If None, it uses r\"\b\\d+\b\". Note that numbers adjacent to letters are not removed. drop_punctuation: bool, default=True Whether to drop punctuations from the text. Characters considered punctuation are !\"#$%&'()*+,-./:;<=>?@[\\]^_ ~`. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. Attributes drops: pd.DataFrame Encountered regex matches. The row indices correspond to the document index from which the occurrence was dropped. See Also TextNormalizer Normalize the corpus. Tokenizer Tokenize the corpus. Vectorizer Vectorize text data. Example atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import fetch_20newsgroups >>> X , y = fetch_20newsgroups ( ... return_X_y = True , ... categories = [ ... 'alt.atheism' , ... 'sci.med' , ... 'comp.windows.x' , ... ], ... shuffle = True , ... random_state = 1 , ... ) >>> X = np . array ( X ) . reshape ( - 1 , 1 ) >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . dataset ) 0 From: thssjxy@iitmax.iit.edu (Smile) Subject:... 2 1 From: nancyo@fraser.sfu.ca (Nancy Patricia O'C... 0 2 From: beck@irzr17.inf.tu-dresden.de (Andre Bec... 1 3 From: keith@cco.caltech.edu (Keith Allan Schne... 0 4 From: strom@Watson.Ibm.Com (Rob Strom) Subjec... 0 ... ... 2841 From: dreitman@oregon.uoregon.edu (Daniel R. R... 3 2842 From: ethan@cs.columbia.edu (Ethan Solomita) ... 1 2843 From: r0506048@cml3 (Chun-Hung Lin) Subject: ... 1 2844 From: eshneken@ux4.cso.uiuc.edu (Edward A Shne... 2 2845 From: ibeshir@nyx.cs.du.edu (Ibrahim) Subject... 2 [2846 rows x 2 columns] >>> atom . textclean ( verbose = 2 ) Fitting TextCleaner... Cleaning the corpus... --> Decoding unicode characters to ascii. --> Converting text to lower case. --> Dropping 10012 emails from 2830 documents. --> Dropping 0 URL links from 0 documents. --> Dropping 2214 HTML tags from 1304 documents. --> Dropping 2 emojis from 1 documents. --> Dropping 31222 numbers from 2843 documents. --> Dropping punctuation from the text. >>> print ( atom . dataset ) corpus target 0 from smile subject forsale used guitar amp... 2 1 from nancy patricia oconnor subject re amusi... 0 2 from andre beck subject re animation with xp... 1 3 from keith allan schneider subject re moralt... 0 4 from rob strom subject re socmotss et al pri... 0 ... ... 2841 from daniel r reitman attorney to be subject... 3 2842 from ethan solomita subject forcing a window... 1 2843 from r0506048cml3 chunhung lin subject re xma... 1 2844 from edward a shnekendorf subject airline ti... 2 2845 from ibrahim subject terminal for sale orga... 2 [2846 rows x 2 columns] >>> from atom.nlp import TextCleaner >>> from sklearn.datasets import fetch_20newsgroups >>> X , y = fetch_20newsgroups ( ... return_X_y = True , ... categories = [ ... 'alt.atheism' , ... 'sci.med' , ... 'comp.windows.x' , ... ], ... shuffle = True , ... random_state = 1 , ... ) >>> X = np . array ( X ) . reshape ( - 1 , 1 ) >>> textcleaner = TextCleaner ( verbose = 2 ) >>> X = textcleaner . transform ( X ) Cleaning the corpus... --> Decoding unicode characters to ascii. --> Converting text to lower case. --> Dropping 10012 emails from 2830 documents. --> Dropping 0 URL links from 0 documents. --> Dropping 2214 HTML tags from 1304 documents. --> Dropping 2 emojis from 1 documents. --> Dropping 31222 numbers from 2843 documents. --> Dropping punctuation from the text. >>> print ( X ) corpus 0 from donald mackie subject re barbecued food... 1 from david stockton subject re krillean phot... 2 from julia miller subject posix message cata... 3 from subject re yet more rushdie re islamic... 4 from joseph a muller subject jfk autograph f... ... 2841 from joel reymont subject motif maling list\\... 2842 from daniel paul checkman subject re is msg ... 2843 from ad absurdum per aspera subject re its a... 2844 from ralf subject items for sale organizati... 2845 from walter g seefeld subject klipsch kg1 sp... [2846 rows x 1 columns] Methods fit Does nothing. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Apply the transformations to the data. method fit (X=None, y=None, **fit_params) [source] Does nothing. Implemented for continuity of the API. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Apply the transformations to the data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). If X is not a pd.DataFrame, it should be composed of a single feature containing the text documents. y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed corpus.","title":"TextCleaner"},{"location":"API/nlp/textcleaner/#textcleaner","text":"class atom.nlp. TextCleaner (decode=True, lower_case=True, drop_email=True, regex_email=None, drop_url=True, regex_url=None, drop_html=True, regex_html=None, drop_emoji=True, regex_emoji=None, drop_number=True, regex_number=None, drop_punctuation=True, verbose=0, logger=None) [source] Applies standard text cleaning to the corpus. Transformations include normalizing characters and dropping noise from the text (emails, HTML tags, URLs, etc...). The transformations are applied on the column named corpus , in the same order the parameters are presented. If there is no column with that name, an exception is raised. This class can be accessed from atom through the textclean method. Read more in the user guide . Parameters decode: bool, default=True Whether to decode unicode characters to their ascii representations. lower_case: bool, default=True Whether to convert all characters to lower case. drop_email: bool, default=True Whether to drop email addresses from the text. regex_email: str, default=None Regex used to search for email addresses. If None, it uses r\"[\\w.-]+@[\\w-]+\\.[\\w.-]+\" . drop_url: bool, default=True Whether to drop URL links from the text. regex_url: str, default=None Regex used to search for URLs. If None, it uses r\"https?://\\S+|www\\.\\S+\" . drop_html: bool, default=True Whether to drop HTML tags from the text. This option is particularly useful if the data was scraped from a website. regex_html: str, default=None Regex used to search for html tags. If None, it uses r\"<.*?>\" . drop_emoji: bool, default=True Whether to drop emojis from the text. regex_emoji: str, default=None Regex used to search for emojis. If None, it uses r\":[a-z_]+:\" . drop_number: bool, default=False Whether to drop numbers from the text. regex_number: str, default=None Regex used to search for numbers. If None, it uses r\"\b\\d+\b\". Note that numbers adjacent to letters are not removed. drop_punctuation: bool, default=True Whether to drop punctuations from the text. Characters considered punctuation are !\"#$%&'()*+,-./:;<=>?@[\\]^_ ~`. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. Attributes drops: pd.DataFrame Encountered regex matches. The row indices correspond to the document index from which the occurrence was dropped. See Also TextNormalizer Normalize the corpus. Tokenizer Tokenize the corpus. Vectorizer Vectorize text data.","title":"TextCleaner"},{"location":"API/nlp/textcleaner/#example","text":"atom stand-alone >>> from atom import ATOMClassifier >>> from sklearn.datasets import fetch_20newsgroups >>> X , y = fetch_20newsgroups ( ... return_X_y = True , ... categories = [ ... 'alt.atheism' , ... 'sci.med' , ... 'comp.windows.x' , ... ], ... shuffle = True , ... random_state = 1 , ... ) >>> X = np . array ( X ) . reshape ( - 1 , 1 ) >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . dataset ) 0 From: thssjxy@iitmax.iit.edu (Smile) Subject:... 2 1 From: nancyo@fraser.sfu.ca (Nancy Patricia O'C... 0 2 From: beck@irzr17.inf.tu-dresden.de (Andre Bec... 1 3 From: keith@cco.caltech.edu (Keith Allan Schne... 0 4 From: strom@Watson.Ibm.Com (Rob Strom) Subjec... 0 ... ... 2841 From: dreitman@oregon.uoregon.edu (Daniel R. R... 3 2842 From: ethan@cs.columbia.edu (Ethan Solomita) ... 1 2843 From: r0506048@cml3 (Chun-Hung Lin) Subject: ... 1 2844 From: eshneken@ux4.cso.uiuc.edu (Edward A Shne... 2 2845 From: ibeshir@nyx.cs.du.edu (Ibrahim) Subject... 2 [2846 rows x 2 columns] >>> atom . textclean ( verbose = 2 ) Fitting TextCleaner... Cleaning the corpus... --> Decoding unicode characters to ascii. --> Converting text to lower case. --> Dropping 10012 emails from 2830 documents. --> Dropping 0 URL links from 0 documents. --> Dropping 2214 HTML tags from 1304 documents. --> Dropping 2 emojis from 1 documents. --> Dropping 31222 numbers from 2843 documents. --> Dropping punctuation from the text. >>> print ( atom . dataset ) corpus target 0 from smile subject forsale used guitar amp... 2 1 from nancy patricia oconnor subject re amusi... 0 2 from andre beck subject re animation with xp... 1 3 from keith allan schneider subject re moralt... 0 4 from rob strom subject re socmotss et al pri... 0 ... ... 2841 from daniel r reitman attorney to be subject... 3 2842 from ethan solomita subject forcing a window... 1 2843 from r0506048cml3 chunhung lin subject re xma... 1 2844 from edward a shnekendorf subject airline ti... 2 2845 from ibrahim subject terminal for sale orga... 2 [2846 rows x 2 columns] >>> from atom.nlp import TextCleaner >>> from sklearn.datasets import fetch_20newsgroups >>> X , y = fetch_20newsgroups ( ... return_X_y = True , ... categories = [ ... 'alt.atheism' , ... 'sci.med' , ... 'comp.windows.x' , ... ], ... shuffle = True , ... random_state = 1 , ... ) >>> X = np . array ( X ) . reshape ( - 1 , 1 ) >>> textcleaner = TextCleaner ( verbose = 2 ) >>> X = textcleaner . transform ( X ) Cleaning the corpus... --> Decoding unicode characters to ascii. --> Converting text to lower case. --> Dropping 10012 emails from 2830 documents. --> Dropping 0 URL links from 0 documents. --> Dropping 2214 HTML tags from 1304 documents. --> Dropping 2 emojis from 1 documents. --> Dropping 31222 numbers from 2843 documents. --> Dropping punctuation from the text. >>> print ( X ) corpus 0 from donald mackie subject re barbecued food... 1 from david stockton subject re krillean phot... 2 from julia miller subject posix message cata... 3 from subject re yet more rushdie re islamic... 4 from joseph a muller subject jfk autograph f... ... 2841 from joel reymont subject motif maling list\\... 2842 from daniel paul checkman subject re is msg ... 2843 from ad absurdum per aspera subject re its a... 2844 from ralf subject items for sale organizati... 2845 from walter g seefeld subject klipsch kg1 sp... [2846 rows x 1 columns]","title":"Example"},{"location":"API/nlp/textcleaner/#methods","text":"fit Does nothing. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Apply the transformations to the data. method fit (X=None, y=None, **fit_params) [source] Does nothing. Implemented for continuity of the API. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Apply the transformations to the data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). If X is not a pd.DataFrame, it should be composed of a single feature containing the text documents. y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed corpus.","title":"Methods"},{"location":"API/nlp/textnormalizer/","text":"TextNormalizer class atom.nlp. TextNormalizer (stopwords=True, custom_stopwords=None, stem=False, lemmatize=True, verbose=0, logger=None) [source] Normalize the corpus. Convert words to a more uniform standard. The transformations are applied on the column named corpus , in the same order the parameters are presented. If there is no column with that name, an exception is raised. If the provided documents are strings, words are separated by spaces. This class can be accessed from atom through the textnormalize method. Read more in the user guide . Parameters stopwords: bool or str, default=True Whether to remove a predefined dictionary of stopwords. If False: Don't remove any predefined stopwords. If True: Drop predefined english stopwords from the text. If str: Language from nltk.corpus.stopwords.words . custom_stopwords: sequence or None, default=None Custom stopwords to remove from the text. stem: bool or str, default=False Whether to apply stemming using SnowballStemmer . If False: Don't apply stemming. If True: Apply stemmer based on the english language. If str: Language from SnowballStemmer.languages . lemmatize: bool, default=True Whether to apply lemmatization using WordNetLemmatizer. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. See Also TextCleaner Applies standard text cleaning to the corpus. Tokenizer Tokenize the corpus. Vectorizer Vectorize text data. Example atom stand-alone >>> from atom import ATOMClassifier >>> X = [ ... [ \"I \u00e0m in ne'w york\" ], ... [ \"New york is nice\" ], ... [ \"new york\" ], ... [ \"hi there this is a test!\" ], ... [ \"another line...\" ], ... [ \"new york is larger than washington\" ], ... [ \"running the test\" ], ... [ \"this is a test\" ], ... ] >>> y = [ 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 ] >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . dataset ) corpus target 0 running the test 0 1 hi there this is a test! 1 2 this is a test 0 3 new york is larger than washington 1 4 New york is nice 0 5 I \u00e0m in ne'w york 1 6 another line... 1 7 new york 0 >>> atom . textnormalize ( stopwords = \"english\" , lemmatize = True , verbose = 2 ) Fitting TextNormalizer... Normalizing the corpus... --> Dropping stopwords. --> Applying lemmatization. >>> print ( atom . dataset ) corpus target 0 [run, test] 0 1 [hi, test!] 1 2 [test] 0 3 [new, york, large, washington] 1 4 [New, york, nice] 0 5 [I, \u00e0m, ne'w, york] 1 6 [another, line...] 1 7 [new, york] 0 >>> from atom.nlp import TextNormalizer >>> X = [ ... [ \"I \u00e0m in ne'w york\" ], ... [ \"New york is nice\" ], ... [ \"new york\" ], ... [ \"hi there this is a test!\" ], ... [ \"another line...\" ], ... [ \"new york is larger than washington\" ], ... [ \"running the test\" ], ... [ \"this is a test\" ], ... ] >>> y = [ 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 ] >>> textnormalizer = TextNormalizer ( ... stopwords = \"english\" , ... lemmatize = True , ... verbose = 2 , ... ) >>> X = textcleaner . transform ( X ) Fitting TextNormalizer... Normalizing the corpus... --> Dropping stopwords. --> Applying lemmatization. >>> print ( X ) corpus 0 [I, \u00e0m, ne'w, york] 1 [New, york, nice] 2 [new, york] 3 [hi, test!] 4 [another, line...] 5 [new, york, large, washington] 6 [run, test] 7 [test] Methods fit Does nothing. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Normalize the text. method fit (X=None, y=None, **fit_params) [source] Does nothing. Implemented for continuity of the API. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Normalize the text. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). If X is not a pd.DataFrame, it should be composed of a single feature containing the text documents. y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed corpus.","title":"TextNormalizer"},{"location":"API/nlp/textnormalizer/#textnormalizer","text":"class atom.nlp. TextNormalizer (stopwords=True, custom_stopwords=None, stem=False, lemmatize=True, verbose=0, logger=None) [source] Normalize the corpus. Convert words to a more uniform standard. The transformations are applied on the column named corpus , in the same order the parameters are presented. If there is no column with that name, an exception is raised. If the provided documents are strings, words are separated by spaces. This class can be accessed from atom through the textnormalize method. Read more in the user guide . Parameters stopwords: bool or str, default=True Whether to remove a predefined dictionary of stopwords. If False: Don't remove any predefined stopwords. If True: Drop predefined english stopwords from the text. If str: Language from nltk.corpus.stopwords.words . custom_stopwords: sequence or None, default=None Custom stopwords to remove from the text. stem: bool or str, default=False Whether to apply stemming using SnowballStemmer . If False: Don't apply stemming. If True: Apply stemmer based on the english language. If str: Language from SnowballStemmer.languages . lemmatize: bool, default=True Whether to apply lemmatization using WordNetLemmatizer. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. See Also TextCleaner Applies standard text cleaning to the corpus. Tokenizer Tokenize the corpus. Vectorizer Vectorize text data.","title":"TextNormalizer"},{"location":"API/nlp/textnormalizer/#example","text":"atom stand-alone >>> from atom import ATOMClassifier >>> X = [ ... [ \"I \u00e0m in ne'w york\" ], ... [ \"New york is nice\" ], ... [ \"new york\" ], ... [ \"hi there this is a test!\" ], ... [ \"another line...\" ], ... [ \"new york is larger than washington\" ], ... [ \"running the test\" ], ... [ \"this is a test\" ], ... ] >>> y = [ 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 ] >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . dataset ) corpus target 0 running the test 0 1 hi there this is a test! 1 2 this is a test 0 3 new york is larger than washington 1 4 New york is nice 0 5 I \u00e0m in ne'w york 1 6 another line... 1 7 new york 0 >>> atom . textnormalize ( stopwords = \"english\" , lemmatize = True , verbose = 2 ) Fitting TextNormalizer... Normalizing the corpus... --> Dropping stopwords. --> Applying lemmatization. >>> print ( atom . dataset ) corpus target 0 [run, test] 0 1 [hi, test!] 1 2 [test] 0 3 [new, york, large, washington] 1 4 [New, york, nice] 0 5 [I, \u00e0m, ne'w, york] 1 6 [another, line...] 1 7 [new, york] 0 >>> from atom.nlp import TextNormalizer >>> X = [ ... [ \"I \u00e0m in ne'w york\" ], ... [ \"New york is nice\" ], ... [ \"new york\" ], ... [ \"hi there this is a test!\" ], ... [ \"another line...\" ], ... [ \"new york is larger than washington\" ], ... [ \"running the test\" ], ... [ \"this is a test\" ], ... ] >>> y = [ 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 ] >>> textnormalizer = TextNormalizer ( ... stopwords = \"english\" , ... lemmatize = True , ... verbose = 2 , ... ) >>> X = textcleaner . transform ( X ) Fitting TextNormalizer... Normalizing the corpus... --> Dropping stopwords. --> Applying lemmatization. >>> print ( X ) corpus 0 [I, \u00e0m, ne'w, york] 1 [New, york, nice] 2 [new, york] 3 [hi, test!] 4 [another, line...] 5 [new, york, large, washington] 6 [run, test] 7 [test]","title":"Example"},{"location":"API/nlp/textnormalizer/#methods","text":"fit Does nothing. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Normalize the text. method fit (X=None, y=None, **fit_params) [source] Does nothing. Implemented for continuity of the API. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Normalize the text. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). If X is not a pd.DataFrame, it should be composed of a single feature containing the text documents. y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed corpus.","title":"Methods"},{"location":"API/nlp/tokenizer/","text":"Tokenizer class atom.nlp. Tokenizer (bigram_freq=None, trigram_freq=None, quadgram_freq=None, verbose=0, logger=None) [source] Tokenize the corpus. Convert documents into sequences of words. Additionally, create n-grams (represented by words united with underscores, e.g. \"New_York\") based on their frequency in the corpus. The transformations are applied on the column named corpus . If there is no column with that name, an exception is raised. This class can be accessed from atom through the tokenize method. Read more in the user guide . Parameters bigram_freq: int, float or None, default=None Frequency threshold for bigram creation. If None: Don't create any bigrams. If int: Minimum number of occurrences to make a bigram. If float: Minimum frequency fraction to make a bigram. trigram_freq: int, float or None, default=None Frequency threshold for trigram creation. If None: Don't create any trigrams. If int: Minimum number of occurrences to make a trigram. If float: Minimum frequency fraction to make a trigram. quadgram_freq: int, float or None, default=None Frequency threshold for quadgram creation. If None: Don't create any quadgrams. If int: Minimum number of occurrences to make a quadgram. If float: Minimum frequency fraction to make a quadgram. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. Attributes bigrams: pd.DataFrame Created bigrams and their frequencies. trigrams: pd.DataFrame Created trigrams and their frequencies. quadgrams: pd.DataFrame Created quadgrams and their frequencies. See Also TextCleaner Applies standard text cleaning to the corpus. TextNormalizer Normalize the corpus. Vectorizer Vectorize text data. Example atom stand-alone >>> from atom import ATOMClassifier >>> X = [ ... [ \"I \u00e0m in ne'w york\" ], ... [ \"New york is nice\" ], ... [ \"new york\" ], ... [ \"hi there this is a test!\" ], ... [ \"another line...\" ], ... [ \"new york is larger than washington\" ], ... [ \"running the test\" ], ... [ \"this is a test\" ], ... ] >>> y = [ 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 ] >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . dataset ) corpus target 0 new york 0 1 new york is larger than washington 1 2 New york is nice 0 3 I \u00e0m in ne'w york 1 4 this is a test 0 5 another line... 1 6 running the test 0 7 hi there this is a test! 1 >>> atom . tokenize ( verbose = 2 ) Fitting Tokenizer... Tokenizing the corpus... >>> print ( atom . dataset ) corpus target 0 [new, york] 0 1 [new, york, is, larger, than, washington] 1 2 [New, york, is, nice] 0 3 [I, \u00e0m, in, ne, ', w, york] 1 4 [this, is, a, test] 0 5 [another, line, ...] 1 6 [running, the, test] 0 7 [hi, there, this, is, a, test, !] 1 >>> from atom.nlp import TextCleaner >>> X = [ ... [ \"I \u00e0m in ne'w york\" ], ... [ \"New york is nice\" ], ... [ \"new york\" ], ... [ \"hi there this is a test!\" ], ... [ \"another line...\" ], ... [ \"new york is larger than washington\" ], ... [ \"running the test\" ], ... [ \"this is a test\" ], ... ] >>> y = [ 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 ] >>> tokenizer = Tokenizer ( bigram_freq = 2 , verbose = 2 ) >>> X = tokenizer . transform ( X ) Fitting Tokenizer... Tokenizing the corpus... --> Creating 5 bigrams on 10 locations. >>> print ( X ) corpus 0 [I, \u00e0m, in, ne, ', w, york] 1 [New, york_is, nice] 2 [new_york] 3 [hi, there, this_is, a_test, !] 4 [another, line, ...] 5 [new, york_is, larger, than, washington] 6 [running, the, test] 7 [this_is, a_test] Methods fit Does nothing. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Tokenize the text. method fit (X=None, y=None, **fit_params) [source] Does nothing. Implemented for continuity of the API. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Tokenize the text. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). If X is not a pd.DataFrame, it should be composed of a single feature containing the text documents. y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed corpus.","title":"Tokenizer"},{"location":"API/nlp/tokenizer/#tokenizer","text":"class atom.nlp. Tokenizer (bigram_freq=None, trigram_freq=None, quadgram_freq=None, verbose=0, logger=None) [source] Tokenize the corpus. Convert documents into sequences of words. Additionally, create n-grams (represented by words united with underscores, e.g. \"New_York\") based on their frequency in the corpus. The transformations are applied on the column named corpus . If there is no column with that name, an exception is raised. This class can be accessed from atom through the tokenize method. Read more in the user guide . Parameters bigram_freq: int, float or None, default=None Frequency threshold for bigram creation. If None: Don't create any bigrams. If int: Minimum number of occurrences to make a bigram. If float: Minimum frequency fraction to make a bigram. trigram_freq: int, float or None, default=None Frequency threshold for trigram creation. If None: Don't create any trigrams. If int: Minimum number of occurrences to make a trigram. If float: Minimum frequency fraction to make a trigram. quadgram_freq: int, float or None, default=None Frequency threshold for quadgram creation. If None: Don't create any quadgrams. If int: Minimum number of occurrences to make a quadgram. If float: Minimum frequency fraction to make a quadgram. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. Attributes bigrams: pd.DataFrame Created bigrams and their frequencies. trigrams: pd.DataFrame Created trigrams and their frequencies. quadgrams: pd.DataFrame Created quadgrams and their frequencies. See Also TextCleaner Applies standard text cleaning to the corpus. TextNormalizer Normalize the corpus. Vectorizer Vectorize text data.","title":"Tokenizer"},{"location":"API/nlp/tokenizer/#example","text":"atom stand-alone >>> from atom import ATOMClassifier >>> X = [ ... [ \"I \u00e0m in ne'w york\" ], ... [ \"New york is nice\" ], ... [ \"new york\" ], ... [ \"hi there this is a test!\" ], ... [ \"another line...\" ], ... [ \"new york is larger than washington\" ], ... [ \"running the test\" ], ... [ \"this is a test\" ], ... ] >>> y = [ 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 ] >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . dataset ) corpus target 0 new york 0 1 new york is larger than washington 1 2 New york is nice 0 3 I \u00e0m in ne'w york 1 4 this is a test 0 5 another line... 1 6 running the test 0 7 hi there this is a test! 1 >>> atom . tokenize ( verbose = 2 ) Fitting Tokenizer... Tokenizing the corpus... >>> print ( atom . dataset ) corpus target 0 [new, york] 0 1 [new, york, is, larger, than, washington] 1 2 [New, york, is, nice] 0 3 [I, \u00e0m, in, ne, ', w, york] 1 4 [this, is, a, test] 0 5 [another, line, ...] 1 6 [running, the, test] 0 7 [hi, there, this, is, a, test, !] 1 >>> from atom.nlp import TextCleaner >>> X = [ ... [ \"I \u00e0m in ne'w york\" ], ... [ \"New york is nice\" ], ... [ \"new york\" ], ... [ \"hi there this is a test!\" ], ... [ \"another line...\" ], ... [ \"new york is larger than washington\" ], ... [ \"running the test\" ], ... [ \"this is a test\" ], ... ] >>> y = [ 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 ] >>> tokenizer = Tokenizer ( bigram_freq = 2 , verbose = 2 ) >>> X = tokenizer . transform ( X ) Fitting Tokenizer... Tokenizing the corpus... --> Creating 5 bigrams on 10 locations. >>> print ( X ) corpus 0 [I, \u00e0m, in, ne, ', w, york] 1 [New, york_is, nice] 2 [new_york] 3 [hi, there, this_is, a_test, !] 4 [another, line, ...] 5 [new, york_is, larger, than, washington] 6 [running, the, test] 7 [this_is, a_test]","title":"Example"},{"location":"API/nlp/tokenizer/#methods","text":"fit Does nothing. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Tokenize the text. method fit (X=None, y=None, **fit_params) [source] Does nothing. Implemented for continuity of the API. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns self Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Tokenize the text. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). If X is not a pd.DataFrame, it should be composed of a single feature containing the text documents. y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed corpus.","title":"Methods"},{"location":"API/nlp/vectorizer/","text":"Vectorizer class atom.nlp. Vectorizer (strategy=\"bow\", return_sparse=True, device=\"cpu\", engine=\"sklearn\", verbose=0, logger=None, **kwargs) [source] Vectorize text data. Transform the corpus into meaningful vectors of numbers. The transformation is applied on the column named corpus . If there is no column with that name, an exception is raised. If strategy=\"bow\" or \"tfidf\", the transformed columns are named after the word they are embedding with the prefix corpus_ . If strategy=\"hashing\", the columns are named hash[N], where N stands for the n-th hashed column. This class can be accessed from atom through the vectorize method. Read more in the user guide . Parameters strategy: str, default=\"bow\" Strategy with which to vectorize the text. Choose from: \" bow \": Bag of Words. \" tfidf \": Term Frequency - Inverse Document Frequency. \" hashing \": Vectorize to a matrix of token occurrences. return_sparse: bool, default=True Whether to return the transformation output as a dataframe of sparse arrays. Must be False when there are other columns in X (besides corpus ) that are non-sparse. device: str, default=\"cpu\" Device on which to train the estimators. Use any string that follows the SYCL_DEVICE_FILTER filter selector, e.g. device=\"gpu\" to use the GPU. Read more in the user guide . engine: str, default=\"sklearn\" Execution engine to use for the estimators. Refer to the user guide for an explanation regarding every choice. Choose from: \"sklearn\" (only if device=\"cpu\") \"cuml\" (only if device=\"gpu\") verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. **kwargs Additional keyword arguments for the strategy estimator. Attributes [strategy]: sklearn transformer Estimator instance (lowercase strategy) used to vectorize the corpus, e.g. vectorizer.tfidf for the tfidf strategy. feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also TextCleaner Applies standard text cleaning to the corpus. TextNormalizer Normalize the corpus. Tokenizer Tokenize the corpus. Example atom stand-alone >>> from atom import ATOMClassifier >>> X = [ ... [ \"I \u00e0m in ne'w york\" ], ... [ \"New york is nice\" ], ... [ \"new york\" ], ... [ \"hi there this is a test!\" ], ... [ \"another line...\" ], ... [ \"new york is larger than washington\" ], ... [ \"running the test\" ], ... [ \"this is a test\" ], ... ] >>> y = [ 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 ] >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . dataset ) corpus target 0 new york 0 1 I \u00e0m in ne'w york 1 2 this is a test 0 3 running the test 0 4 another line... 1 5 hi there this is a test! 1 6 New york is nice 0 7 new york is larger than washington 1 >>> atom . vectorize ( strategy = \"tfidf\" , verbose = 2 ) Fitting Vectorizer... Vectorizing the corpus... >>> print ( atom . dataset ) corpus_another corpus_in corpus_is ... corpus_york corpus_\u00e0m target 0 0.000000 0.000000 0.000000 ... 0.627914 0.000000 0 1 0.000000 0.523358 0.000000 ... 0.422242 0.523358 1 2 0.000000 0.000000 0.614189 ... 0.000000 0.000000 0 3 0.000000 0.000000 0.000000 ... 0.000000 0.000000 0 4 0.707107 0.000000 0.000000 ... 0.000000 0.000000 1 5 0.000000 0.000000 0.614189 ... 0.000000 0.000000 1 6 0.000000 0.000000 0.614189 ... 0.495524 0.000000 0 7 0.000000 0.000000 0.614189 ... 0.495524 0.000000 1 [8 rows x 13 columns] >>> from atom.nlp import TextCleaner >>> X = [ ... [ \"I \u00e0m in ne'w york\" ], ... [ \"New york is nice\" ], ... [ \"new york\" ], ... [ \"hi there this is a test!\" ], ... [ \"another line...\" ], ... [ \"new york is larger than washington\" ], ... [ \"running the test\" ], ... [ \"this is a test\" ], ... ] >>> y = [ 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 ] >>> vectorizer = Vectorizer ( strategy = \"tfidf\" , verbose = 2 ) >>> X = vectorizer . fit_transform ( X ) Fitting Vectorizer... Vectorizing the corpus... >>> print ( X ) corpus_another corpus_hi ... corpus_york corpus_\u00e0m 0 0.000000 0.000000 ... 0.343774 0.542162 1 0.000000 0.000000 ... 0.415657 0.000000 2 0.000000 0.000000 ... 0.659262 0.000000 3 0.000000 0.525049 ... 0.000000 0.000000 4 0.707107 0.000000 ... 0.000000 0.000000 5 0.000000 0.000000 ... 0.304821 0.000000 6 0.000000 0.000000 ... 0.000000 0.000000 7 0.000000 0.000000 ... 0.000000 0.000000 [8 rows x 18 columns] Methods fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Vectorize the text. method fit (X, y=None) [source] Fit to data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). If X is not a pd.DataFrame, it should be composed of a single feature containing the text documents. y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns Vectorizer Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Vectorize the text. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). If X is not a pd.DataFrame, it should be composed of a single feature containing the text documents. y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed corpus.","title":"Vectorizer"},{"location":"API/nlp/vectorizer/#vectorizer","text":"class atom.nlp. Vectorizer (strategy=\"bow\", return_sparse=True, device=\"cpu\", engine=\"sklearn\", verbose=0, logger=None, **kwargs) [source] Vectorize text data. Transform the corpus into meaningful vectors of numbers. The transformation is applied on the column named corpus . If there is no column with that name, an exception is raised. If strategy=\"bow\" or \"tfidf\", the transformed columns are named after the word they are embedding with the prefix corpus_ . If strategy=\"hashing\", the columns are named hash[N], where N stands for the n-th hashed column. This class can be accessed from atom through the vectorize method. Read more in the user guide . Parameters strategy: str, default=\"bow\" Strategy with which to vectorize the text. Choose from: \" bow \": Bag of Words. \" tfidf \": Term Frequency - Inverse Document Frequency. \" hashing \": Vectorize to a matrix of token occurrences. return_sparse: bool, default=True Whether to return the transformation output as a dataframe of sparse arrays. Must be False when there are other columns in X (besides corpus ) that are non-sparse. device: str, default=\"cpu\" Device on which to train the estimators. Use any string that follows the SYCL_DEVICE_FILTER filter selector, e.g. device=\"gpu\" to use the GPU. Read more in the user guide . engine: str, default=\"sklearn\" Execution engine to use for the estimators. Refer to the user guide for an explanation regarding every choice. Choose from: \"sklearn\" (only if device=\"cpu\") \"cuml\" (only if device=\"gpu\") verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. **kwargs Additional keyword arguments for the strategy estimator. Attributes [strategy]: sklearn transformer Estimator instance (lowercase strategy) used to vectorize the corpus, e.g. vectorizer.tfidf for the tfidf strategy. feature_names_in_: np.array Names of features seen during fit. n_features_in_: int Number of features seen during fit. See Also TextCleaner Applies standard text cleaning to the corpus. TextNormalizer Normalize the corpus. Tokenizer Tokenize the corpus.","title":"Vectorizer"},{"location":"API/nlp/vectorizer/#example","text":"atom stand-alone >>> from atom import ATOMClassifier >>> X = [ ... [ \"I \u00e0m in ne'w york\" ], ... [ \"New york is nice\" ], ... [ \"new york\" ], ... [ \"hi there this is a test!\" ], ... [ \"another line...\" ], ... [ \"new york is larger than washington\" ], ... [ \"running the test\" ], ... [ \"this is a test\" ], ... ] >>> y = [ 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 ] >>> atom = ATOMClassifier ( X , y ) >>> print ( atom . dataset ) corpus target 0 new york 0 1 I \u00e0m in ne'w york 1 2 this is a test 0 3 running the test 0 4 another line... 1 5 hi there this is a test! 1 6 New york is nice 0 7 new york is larger than washington 1 >>> atom . vectorize ( strategy = \"tfidf\" , verbose = 2 ) Fitting Vectorizer... Vectorizing the corpus... >>> print ( atom . dataset ) corpus_another corpus_in corpus_is ... corpus_york corpus_\u00e0m target 0 0.000000 0.000000 0.000000 ... 0.627914 0.000000 0 1 0.000000 0.523358 0.000000 ... 0.422242 0.523358 1 2 0.000000 0.000000 0.614189 ... 0.000000 0.000000 0 3 0.000000 0.000000 0.000000 ... 0.000000 0.000000 0 4 0.707107 0.000000 0.000000 ... 0.000000 0.000000 1 5 0.000000 0.000000 0.614189 ... 0.000000 0.000000 1 6 0.000000 0.000000 0.614189 ... 0.495524 0.000000 0 7 0.000000 0.000000 0.614189 ... 0.495524 0.000000 1 [8 rows x 13 columns] >>> from atom.nlp import TextCleaner >>> X = [ ... [ \"I \u00e0m in ne'w york\" ], ... [ \"New york is nice\" ], ... [ \"new york\" ], ... [ \"hi there this is a test!\" ], ... [ \"another line...\" ], ... [ \"new york is larger than washington\" ], ... [ \"running the test\" ], ... [ \"this is a test\" ], ... ] >>> y = [ 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 ] >>> vectorizer = Vectorizer ( strategy = \"tfidf\" , verbose = 2 ) >>> X = vectorizer . fit_transform ( X ) Fitting Vectorizer... Vectorizing the corpus... >>> print ( X ) corpus_another corpus_hi ... corpus_york corpus_\u00e0m 0 0.000000 0.000000 ... 0.343774 0.542162 1 0.000000 0.000000 ... 0.415657 0.000000 2 0.000000 0.000000 ... 0.659262 0.000000 3 0.000000 0.525049 ... 0.000000 0.000000 4 0.707107 0.000000 ... 0.000000 0.000000 5 0.000000 0.000000 ... 0.304821 0.000000 6 0.000000 0.000000 ... 0.000000 0.000000 7 0.000000 0.000000 ... 0.000000 0.000000 [8 rows x 18 columns]","title":"Example"},{"location":"API/nlp/vectorizer/#methods","text":"fit Fit to data. fit_transform Fit to data, then transform it. get_params Get parameters for this estimator. inverse_transform Does nothing. log Print message and save to log file. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Vectorize the text. method fit (X, y=None) [source] Fit to data. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). If X is not a pd.DataFrame, it should be composed of a single feature containing the text documents. y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns Vectorizer Estimator instance. method fit_transform (X=None, y=None, **fit_params) [source] Fit to data, then transform it. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. **fit_params Additional keyword arguments for the fit method. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method get_params (deep=True) [source] Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. method inverse_transform (X=None, y=None) [source] Does nothing. Parameters X: dataframe-like or None, default=None Feature set with shape=(n_samples, n_features). If None, X is ignored. y: int, str, dict, sequence or None, default=None Target column corresponding to X. If None: y is ignored. If int: Position of the target column in X. If str: Name of the target column in X. Else: Array with shape=(n_samples,) to use as target. Returns pd.DataFrame Transformed feature set. Only returned if provided. pd.Series Transformed target column. Only returned if provided. method log (msg, level=0, severity=\"info\") [source] Print message and save to log file. Parameters msg: int, float or str Message to save to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. severity: str, default=\"info\" Severity level of the message. Choose from: debug, info, warning, error, critical. method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Parameters filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the dataset with the instance. This parameter is ignored if the method is not called from atom. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. method transform (X, y=None) [source] Vectorize the text. Parameters X: dataframe-like Feature set with shape=(n_samples, n_features). If X is not a pd.DataFrame, it should be composed of a single feature containing the text documents. y: int, str, dict, sequence or None, default=None Does nothing. Implemented for continuity of the API. Returns pd.DataFrame Transformed corpus.","title":"Methods"},{"location":"API/plots/plot_calibration/","text":"plot_calibration method plot_calibration (models=None, n_bins=10, title=None, figsize=(10, 10), filename=None, display=True) [source] Plot the calibration curve for a binary classifier. Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level. For instance a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approx. 80% actually belong to the positive class. Read more in sklearn's documentation . This figure shows two plots: the calibration curve, where the x-axis represents the average predicted probability in each bin, and the y-axis is the fraction of positives, i.e. the proportion of samples whose class is the positive class (in each bin); and a distribution of all predicted probabilities of the classifier. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. n_bins: int, default=10 Number of bins used for calibration. Minimum of 5 required. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 10) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Tip Use the calibrate method to calibrate the winning model. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"GNB\" , \"LR\" , \"LGB\" ], metric = \"average_precision\" ) atom . plot_calibration ()","title":"plot_calibration"},{"location":"API/plots/plot_calibration/#plot_calibration","text":"method plot_calibration (models=None, n_bins=10, title=None, figsize=(10, 10), filename=None, display=True) [source] Plot the calibration curve for a binary classifier. Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level. For instance a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approx. 80% actually belong to the positive class. Read more in sklearn's documentation . This figure shows two plots: the calibration curve, where the x-axis represents the average predicted probability in each bin, and the y-axis is the fraction of positives, i.e. the proportion of samples whose class is the positive class (in each bin); and a distribution of all predicted probabilities of the classifier. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. n_bins: int, default=10 Number of bins used for calibration. Minimum of 5 required. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 10) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Tip Use the calibrate method to calibrate the winning model.","title":"plot_calibration"},{"location":"API/plots/plot_calibration/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"GNB\" , \"LR\" , \"LGB\" ], metric = \"average_precision\" ) atom . plot_calibration ()","title":"Example"},{"location":"API/plots/plot_components/","text":"plot_components method plot_components (show=None, title=None, figsize=None, filename=None, display=True) [source] Plot the explained variance ratio per component. Only available if pca was applied on the data. Parameters: show: int or None, default=None Number of components to show. None to show all. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of components shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . feature_selection ( strategy = \"pca\" , n_features = 11 ) atom . plot_components ()","title":"plot_components"},{"location":"API/plots/plot_components/#plot_components","text":"method plot_components (show=None, title=None, figsize=None, filename=None, display=True) [source] Plot the explained variance ratio per component. Only available if pca was applied on the data. Parameters: show: int or None, default=None Number of components to show. None to show all. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of components shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_components"},{"location":"API/plots/plot_components/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . feature_selection ( strategy = \"pca\" , n_features = 11 ) atom . plot_components ()","title":"Example"},{"location":"API/plots/plot_confusion_matrix/","text":"plot_confusion_matrix method plot_confusion_matrix (models=None, dataset=\"test\", normalize=False, title=None, figsize=None, filename=None, display=True) [source] Plot a model's confusion matrix. For one model, the plot shows a heatmap. For multiple models, it compares TP, FP, FN and TN in a barplot (not implemented for multiclass classification tasks). Only for classification tasks. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the confusion matrix. Choose from: \"train\", \"test\" or \"holdout\". normalize: bool, default=False Whether to normalize the matrix. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=None Figure's size, format as (x, y). If None, it adapts the size to plot's type. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"Tree\" , \"Bag\" ]) atom . Tree . plot_confusion_matrix ( normalize = True ) # For one model atom . plot_confusion_matrix () # For multiple models","title":"plot_confusion_matrix"},{"location":"API/plots/plot_confusion_matrix/#plot_confusion_matrix","text":"method plot_confusion_matrix (models=None, dataset=\"test\", normalize=False, title=None, figsize=None, filename=None, display=True) [source] Plot a model's confusion matrix. For one model, the plot shows a heatmap. For multiple models, it compares TP, FP, FN and TN in a barplot (not implemented for multiclass classification tasks). Only for classification tasks. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the confusion matrix. Choose from: \"train\", \"test\" or \"holdout\". normalize: bool, default=False Whether to normalize the matrix. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=None Figure's size, format as (x, y). If None, it adapts the size to plot's type. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_confusion_matrix"},{"location":"API/plots/plot_confusion_matrix/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"Tree\" , \"Bag\" ]) atom . Tree . plot_confusion_matrix ( normalize = True ) # For one model atom . plot_confusion_matrix () # For multiple models","title":"Example"},{"location":"API/plots/plot_correlation/","text":"plot_correlation method plot_correlation (columns=None, method=\"pearson\", title=None, figsize=(8, 7), filename=None, display=True) [source] Plot the data's correlation matrix. Parameters: columns: slice, sequence or None, default=None Slice, names or indices of the columns to plot. If None, plot all columns in the dataset. Selected categorical columns are ignored. method: str, default=\"pearson\" Method of correlation. Choose from \"pearson\", \"kendall\" or \"spearman\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(8, 7) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . plot_correlation ()","title":"plot_correlation"},{"location":"API/plots/plot_correlation/#plot_correlation","text":"method plot_correlation (columns=None, method=\"pearson\", title=None, figsize=(8, 7), filename=None, display=True) [source] Plot the data's correlation matrix. Parameters: columns: slice, sequence or None, default=None Slice, names or indices of the columns to plot. If None, plot all columns in the dataset. Selected categorical columns are ignored. method: str, default=\"pearson\" Method of correlation. Choose from \"pearson\", \"kendall\" or \"spearman\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(8, 7) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_correlation"},{"location":"API/plots/plot_correlation/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . plot_correlation ()","title":"Example"},{"location":"API/plots/plot_det/","text":"plot_det method plot_det (models=None, dataset=\"test\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the Detection Error Tradeoff (DET) curve. Only for binary classification tasks. Read more about DET in sklearn's documentation . Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"LR\" , \"RF\" , \"LGB\" ], metric = \"fpr\" ) atom . plot_det ()","title":"plot_det"},{"location":"API/plots/plot_det/#plot_det","text":"method plot_det (models=None, dataset=\"test\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the Detection Error Tradeoff (DET) curve. Only for binary classification tasks. Read more about DET in sklearn's documentation . Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_det"},{"location":"API/plots/plot_det/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"LR\" , \"RF\" , \"LGB\" ], metric = \"fpr\" ) atom . plot_det ()","title":"Example"},{"location":"API/plots/plot_distribution/","text":"plot_distribution method plot_distribution (columns=0, distributions=None, show=None, title=None, figsize=None, filename=None, display=True, **kwargs) [source] Plot column distributions. Additionally, it is possible to plot any of scipy.stats probability distributions fitted to the column. Missing values are ignored. Parameters: columns: int, str, slice or sequence, default=0 Slice, names or indices of the columns to plot. It is only possible to plot one categorical column. If more than just the one categorical column is selected, all categorical columns are ignored. distributions: str, sequence or None, default=None Names of the scipy.stats distributions to fit to the column. If None, no distribution is fitted. Only for numerical columns. show: int or None, default=None Number of classes (ordered by number of occurrences) to show in the plot. None to show all. Only for categorical columns. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=None Figure's size, format as (x, y). If None, adapts size to the plot's type. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for seaborn's histplot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Tip Use atom's distribution method to check which distribution fits the column best. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . plot_distribution ( columns = [ 1 , 2 ]) # With numerical columns # With fitted distributions atom . plot_distribution ( columns = \"mean radius\" , distributions = [ \"norm\" , \"triang\" ]) # With categorical columns atom . plot_distribution ( columns = \"Location\" , show = 11 )","title":"plot_distribution"},{"location":"API/plots/plot_distribution/#plot_distribution","text":"method plot_distribution (columns=0, distributions=None, show=None, title=None, figsize=None, filename=None, display=True, **kwargs) [source] Plot column distributions. Additionally, it is possible to plot any of scipy.stats probability distributions fitted to the column. Missing values are ignored. Parameters: columns: int, str, slice or sequence, default=0 Slice, names or indices of the columns to plot. It is only possible to plot one categorical column. If more than just the one categorical column is selected, all categorical columns are ignored. distributions: str, sequence or None, default=None Names of the scipy.stats distributions to fit to the column. If None, no distribution is fitted. Only for numerical columns. show: int or None, default=None Number of classes (ordered by number of occurrences) to show in the plot. None to show all. Only for categorical columns. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=None Figure's size, format as (x, y). If None, adapts size to the plot's type. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for seaborn's histplot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Tip Use atom's distribution method to check which distribution fits the column best.","title":"plot_distribution"},{"location":"API/plots/plot_distribution/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . plot_distribution ( columns = [ 1 , 2 ]) # With numerical columns # With fitted distributions atom . plot_distribution ( columns = \"mean radius\" , distributions = [ \"norm\" , \"triang\" ]) # With categorical columns atom . plot_distribution ( columns = \"Location\" , show = 11 )","title":"Example"},{"location":"API/plots/plot_errors/","text":"plot_errors method plot_errors (models=None, dataset=\"test\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot a model's prediction errors, i.e. the actual targets from a set against the predicted values generated by the regressor. A linear fit is made on the data. The gray, intersected line shows the identity line. This pot can be useful to detect noise or heteroscedasticity along a range of the target domain. Only for regression tasks. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the errors. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ([ \"OLS\" , \"LGB\" ], metric = \"MAE\" ) atom . plot_errors ()","title":"plot_errors"},{"location":"API/plots/plot_errors/#plot_errors","text":"method plot_errors (models=None, dataset=\"test\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot a model's prediction errors, i.e. the actual targets from a set against the predicted values generated by the regressor. A linear fit is made on the data. The gray, intersected line shows the identity line. This pot can be useful to detect noise or heteroscedasticity along a range of the target domain. Only for regression tasks. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the errors. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_errors"},{"location":"API/plots/plot_errors/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ([ \"OLS\" , \"LGB\" ], metric = \"MAE\" ) atom . plot_errors ()","title":"Example"},{"location":"API/plots/plot_evals/","text":"plot_evals method plot_evals (models=None, dataset=\"both\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot evaluation curves for the train and test set. Only for models that allow in-training evaluation ( XGB , LGB , CatB ). The metric is provided by the estimator's package and is different for every model and every task. For this reason, the method only allows plotting one model. Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.lgb.plot_evals() . dataset: str, default=\"both\" Data set on which to calculate the evaluation curves. Options are \"train\", \"test\" or \"both\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ([ \"Bag\" , \"LGB\" ]) atom . lgb . plot_evals ()","title":"plot_evals"},{"location":"API/plots/plot_evals/#plot_evals","text":"method plot_evals (models=None, dataset=\"both\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot evaluation curves for the train and test set. Only for models that allow in-training evaluation ( XGB , LGB , CatB ). The metric is provided by the estimator's package and is different for every model and every task. For this reason, the method only allows plotting one model. Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.lgb.plot_evals() . dataset: str, default=\"both\" Data set on which to calculate the evaluation curves. Options are \"train\", \"test\" or \"both\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_evals"},{"location":"API/plots/plot_evals/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ([ \"Bag\" , \"LGB\" ]) atom . lgb . plot_evals ()","title":"Example"},{"location":"API/plots/plot_feature_importance/","text":"plot_feature_importance method plot_feature_importance (models=None, show=None, title=None, figsize=None, filename=None, display=True) [source] Plot a model's feature importance. The importances are normalized in order to be able to compare them between models. Only for models whose estimator has a feature_importances_ or coef_ attribute. The feature_importance attribute is updated with the extracted importance ranking. Parameters: models: int, str, slice, sequence or None, default=None Name of the models to plot. If None, all the models are selected. show: int, default=None Number of features (ordered by importance) to show. None to show all. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"LR\" , \"RF\" ], metric = \"recall_weighted\" ) atom . RF . plot_feature_importance ( show = 11 , filename = \"random_forest_importance\" )","title":"plot_feature_importance"},{"location":"API/plots/plot_feature_importance/#plot_feature_importance","text":"method plot_feature_importance (models=None, show=None, title=None, figsize=None, filename=None, display=True) [source] Plot a model's feature importance. The importances are normalized in order to be able to compare them between models. Only for models whose estimator has a feature_importances_ or coef_ attribute. The feature_importance attribute is updated with the extracted importance ranking. Parameters: models: int, str, slice, sequence or None, default=None Name of the models to plot. If None, all the models are selected. show: int, default=None Number of features (ordered by importance) to show. None to show all. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_feature_importance"},{"location":"API/plots/plot_feature_importance/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"LR\" , \"RF\" ], metric = \"recall_weighted\" ) atom . RF . plot_feature_importance ( show = 11 , filename = \"random_forest_importance\" )","title":"Example"},{"location":"API/plots/plot_gains/","text":"plot_gains method plot_gains (models=None, dataset=\"test\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the cumulative gains curve. Only for binary classification tasks. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the gains. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"GNB\" , \"RF\" , \"LGB\" ], metric = \"roc_auc\" ) atom . plot_gains ( filename = \"cumulative_gains_curve\" )","title":"plot_gains"},{"location":"API/plots/plot_gains/#plot_gains","text":"method plot_gains (models=None, dataset=\"test\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the cumulative gains curve. Only for binary classification tasks. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the gains. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_gains"},{"location":"API/plots/plot_gains/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"GNB\" , \"RF\" , \"LGB\" ], metric = \"roc_auc\" ) atom . plot_gains ( filename = \"cumulative_gains_curve\" )","title":"Example"},{"location":"API/plots/plot_learning_curve/","text":"plot_learning_curve method plot_learning_curve (models=None, metric=0, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the model's learning curve: score vs number of training samples. Only use with models fitted using train sizing . Ensemble models are ignored. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. metric: int or str, default=0 Index or name of the metric to plot. Only for multi-metric runs. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . train_sizing ( models = [ \"LDA\" , \"RF\" ], metric = \"precision\" , train_sizes = 10 , n_bootstrap = 4 , ) atom . plot_learning_curve ()","title":"plot_learning_curve"},{"location":"API/plots/plot_learning_curve/#plot_learning_curve","text":"method plot_learning_curve (models=None, metric=0, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the model's learning curve: score vs number of training samples. Only use with models fitted using train sizing . Ensemble models are ignored. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. metric: int or str, default=0 Index or name of the metric to plot. Only for multi-metric runs. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_learning_curve"},{"location":"API/plots/plot_learning_curve/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . train_sizing ( models = [ \"LDA\" , \"RF\" ], metric = \"precision\" , train_sizes = 10 , n_bootstrap = 4 , ) atom . plot_learning_curve ()","title":"Example"},{"location":"API/plots/plot_lift/","text":"plot_lift method plot_lift (models=None, dataset=\"test\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the lift curve. Only for binary classification. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the lift curve. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"GNB\" , \"RF\" , \"LGB\" ], metric = \"roc_auc\" ) atom . plot_lift ( filename = \"lift_curve\" )","title":"plot_lift"},{"location":"API/plots/plot_lift/#plot_lift","text":"method plot_lift (models=None, dataset=\"test\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the lift curve. Only for binary classification. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the lift curve. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_lift"},{"location":"API/plots/plot_lift/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"GNB\" , \"RF\" , \"LGB\" ], metric = \"roc_auc\" ) atom . plot_lift ( filename = \"lift_curve\" )","title":"Example"},{"location":"API/plots/plot_ngrams/","text":"plot_ngrams method plot_ngrams (ngram=\"words\", index=None, show=10, title=None, figsize=None, filename=None, display=True) [source] Plot n-gram frequencies. The text for the plot is extracted from the column named corpus . If there is no column with that name, an exception is raised. If the documents are not tokenized, the words are separated by spaces. Parameters: ngram: str or int, default=\"bigram\" Number of contiguous words to search for (size of n-gram). Choose from: words (1), bigrams (2), trigrams (3), quadgrams (4). index: int, str, sequence or None, default=None Index names or positions of the documents in the corpus to include in the search. If None, it selects all documents in the dataset. show: int, default=10 Number of n-grams (ordered by number of occurrences) to show in the plot. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of n-grams shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X_text , y_text ) atom . textclean () atom . plot_ngrams ( \"bigrams\" )","title":"plot_ngrams"},{"location":"API/plots/plot_ngrams/#plot_ngrams","text":"method plot_ngrams (ngram=\"words\", index=None, show=10, title=None, figsize=None, filename=None, display=True) [source] Plot n-gram frequencies. The text for the plot is extracted from the column named corpus . If there is no column with that name, an exception is raised. If the documents are not tokenized, the words are separated by spaces. Parameters: ngram: str or int, default=\"bigram\" Number of contiguous words to search for (size of n-gram). Choose from: words (1), bigrams (2), trigrams (3), quadgrams (4). index: int, str, sequence or None, default=None Index names or positions of the documents in the corpus to include in the search. If None, it selects all documents in the dataset. show: int, default=10 Number of n-grams (ordered by number of occurrences) to show in the plot. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of n-grams shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_ngrams"},{"location":"API/plots/plot_ngrams/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X_text , y_text ) atom . textclean () atom . plot_ngrams ( \"bigrams\" )","title":"Example"},{"location":"API/plots/plot_parshap/","text":"plot_parshap method plot_parshap (models=None, columns=None, target=1, title=None, figsize=(10, 6), filename=None, display=True) [source] Plots the train and test correlation between the shap value of every feature with its target value, after removing the effect of all other features (partial correlation). This plot is useful to identify the features that are contributing most to overfitting. Features that lie below the bisector (diagonal line) performed worse on the test set than on the training set. If the estimator has a feature_importances_ or coef_ attribute, its normalized values are shown in a color map. Read more about this plot here . Parameters: models: int, str, slice, sequence or None, default=None Name of the models to plot. If None, all the models are selected. columns: int, str, sequence or None, default=None Names or indices of the features to plot. None to show all. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"GNB\" , \"LGB\" ]) atom . gnb . plot_parshap () atom . lgb . plot_parshap ()","title":"plot_parshap"},{"location":"API/plots/plot_parshap/#plot_parshap","text":"method plot_parshap (models=None, columns=None, target=1, title=None, figsize=(10, 6), filename=None, display=True) [source] Plots the train and test correlation between the shap value of every feature with its target value, after removing the effect of all other features (partial correlation). This plot is useful to identify the features that are contributing most to overfitting. Features that lie below the bisector (diagonal line) performed worse on the test set than on the training set. If the estimator has a feature_importances_ or coef_ attribute, its normalized values are shown in a color map. Read more about this plot here . Parameters: models: int, str, slice, sequence or None, default=None Name of the models to plot. If None, all the models are selected. columns: int, str, sequence or None, default=None Names or indices of the features to plot. None to show all. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_parshap"},{"location":"API/plots/plot_parshap/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"GNB\" , \"LGB\" ]) atom . gnb . plot_parshap () atom . lgb . plot_parshap ()","title":"Example"},{"location":"API/plots/plot_partial_dependence/","text":"plot_partial_dependence method plot_partial_dependence (models=None, columns=None, kind=\"average\", target=None, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the partial dependence of features. The partial dependence of a feature (or a set of features) corresponds to the response of the model for each possible value of the feature. Two-way partial dependence plots are plotted as contour plots (only allowed for single model plots). The deciles of the feature values will be shown with tick marks on the x-axes for one-way plots, and on both axes for two-way plots. Read more about partial dependence on sklearn's documentation . Parameters: models: int, str, slice, sequence or None, default=None Name of the models to plot. If None, all the models are selected. columns: int, str, sequence or None, default=None Features or feature pairs (name or index) to get the partial dependence from. Maximum of 3 allowed. If None, it uses the top 3 features if the feature_importance attribute is defined, else it uses the first 3 features in the dataset. kind: str, default=\"average\" \"average\": Plot the partial dependence averaged across all the samples in the dataset. \"individual\": Plot the partial dependence per sample (Individual Conditional Expectation). \"both\": Plot both the average (as a thick line) and the individual (thin lines) partial dependence. This parameter is ignored when plotting feature pairs. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . feature_selection ( strategy = \"pca\" , n_features = 6 ) atom . run ([ \"Tree\" , \"Bag\" ], metric = \"precision\" ) atom . plot_partial_dependence () atom . tree . plot_partial_dependence ( features = ( 4 , ( 3 , 4 )))","title":"plot_partial_dependence"},{"location":"API/plots/plot_partial_dependence/#plot_partial_dependence","text":"method plot_partial_dependence (models=None, columns=None, kind=\"average\", target=None, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the partial dependence of features. The partial dependence of a feature (or a set of features) corresponds to the response of the model for each possible value of the feature. Two-way partial dependence plots are plotted as contour plots (only allowed for single model plots). The deciles of the feature values will be shown with tick marks on the x-axes for one-way plots, and on both axes for two-way plots. Read more about partial dependence on sklearn's documentation . Parameters: models: int, str, slice, sequence or None, default=None Name of the models to plot. If None, all the models are selected. columns: int, str, sequence or None, default=None Features or feature pairs (name or index) to get the partial dependence from. Maximum of 3 allowed. If None, it uses the top 3 features if the feature_importance attribute is defined, else it uses the first 3 features in the dataset. kind: str, default=\"average\" \"average\": Plot the partial dependence averaged across all the samples in the dataset. \"individual\": Plot the partial dependence per sample (Individual Conditional Expectation). \"both\": Plot both the average (as a thick line) and the individual (thin lines) partial dependence. This parameter is ignored when plotting feature pairs. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_partial_dependence"},{"location":"API/plots/plot_partial_dependence/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . feature_selection ( strategy = \"pca\" , n_features = 6 ) atom . run ([ \"Tree\" , \"Bag\" ], metric = \"precision\" ) atom . plot_partial_dependence () atom . tree . plot_partial_dependence ( features = ( 4 , ( 3 , 4 )))","title":"Example"},{"location":"API/plots/plot_pca/","text":"plot_pca method plot_pca (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the explained variance ratio vs the number of components. If the underlying estimator is pca (for dense datasets), all possible components are plotted. If the underlying estimator is TruncatedSVD (for sparse datasets), it only shows the selected components. The blue star marks the number of components selected by the user. Only available if pca was applied on the data. Parameters: title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . feature_selection ( strategy = \"pca\" , n_features = 11 ) atom . plot_pca ()","title":"plot_pca"},{"location":"API/plots/plot_pca/#plot_pca","text":"method plot_pca (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the explained variance ratio vs the number of components. If the underlying estimator is pca (for dense datasets), all possible components are plotted. If the underlying estimator is TruncatedSVD (for sparse datasets), it only shows the selected components. The blue star marks the number of components selected by the user. Only available if pca was applied on the data. Parameters: title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_pca"},{"location":"API/plots/plot_pca/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . feature_selection ( strategy = \"pca\" , n_features = 11 ) atom . plot_pca ()","title":"Example"},{"location":"API/plots/plot_permutation_importance/","text":"plot_permutation_importance method plot_permutation_importance (models=None, show=None, n_repeats=10, title=None, figsize=None, filename=None, display=True) [source] Plot the feature permutation importance of models. Calculating permutations can be time-consuming, especially if n_repeats is high. For this reason, the permutations are stored under the permutations attribute. If the plot is called again for the same model with the same n_repeats , it will use the stored values, making the method considerably faster. The feature_importance attribute is updated with the extracted importance ranking. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. show: int, default=None Number of features (ordered by importance) to show. None to show all. n_repeats: int, default=10 Number of times to permute each feature. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"LR\" , \"LDA\" ], metric = \"average_precision\" ) atom . lda . plot_permutation_importance ( show = 10 , n_repeats = 7 )","title":"plot_permutation_importance"},{"location":"API/plots/plot_permutation_importance/#plot_permutation_importance","text":"method plot_permutation_importance (models=None, show=None, n_repeats=10, title=None, figsize=None, filename=None, display=True) [source] Plot the feature permutation importance of models. Calculating permutations can be time-consuming, especially if n_repeats is high. For this reason, the permutations are stored under the permutations attribute. If the plot is called again for the same model with the same n_repeats , it will use the stored values, making the method considerably faster. The feature_importance attribute is updated with the extracted importance ranking. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. show: int, default=None Number of features (ordered by importance) to show. None to show all. n_repeats: int, default=10 Number of times to permute each feature. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_permutation_importance"},{"location":"API/plots/plot_permutation_importance/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"LR\" , \"LDA\" ], metric = \"average_precision\" ) atom . lda . plot_permutation_importance ( show = 10 , n_repeats = 7 )","title":"Example"},{"location":"API/plots/plot_pipeline/","text":"plot_pipeline method plot_pipeline (models=None, draw_hyperparameter_tuning=True, color_branches=None, title=None, figsize=None, filename=None, display=True) [source] Plot a diagram of the pipeline. Parameters: model: str or None, default=None Name or index of the models to plot. If None, all models are selected. draw_hyperparameter_tuning: bool, default=True Whether to draw if the models used Hyperparameter Tuning. color_branches: bool or None, default=None Whether to draw every branch in a different color. If None, branches are colored when there is more than one. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the pipeline drawn. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Tip Print atom.pipeline in a notebook for sklearn's interactive visualization of the current pipeline. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . impute ( strat_num = \"median\" ) atom . encode ( max_onehot = 5 ) atom . run ([ \"GNB\" , \"RNN\" , \"SGD\" , \"MLP\" ]) atom . voting ( models = atom . winners [: 2 ]) atom . plot_pipeline () # For a single branch from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . scale () atom . prune () atom . run ( \"RF\" , n_calls = 10 , n_initial_points = 3 ) atom . branch = \"oversample\" atom . balance ( strategy = \"adasyn\" ) atom . run ( \"RF_os\" ) atom . branch = \"undersample_from_master\" atom . balance ( strategy = \"nearmiss\" ) atom . run ( \"RF_us\" ) atom . plot_pipeline () # For multiple branches","title":"plot_pipeline"},{"location":"API/plots/plot_pipeline/#plot_pipeline","text":"method plot_pipeline (models=None, draw_hyperparameter_tuning=True, color_branches=None, title=None, figsize=None, filename=None, display=True) [source] Plot a diagram of the pipeline. Parameters: model: str or None, default=None Name or index of the models to plot. If None, all models are selected. draw_hyperparameter_tuning: bool, default=True Whether to draw if the models used Hyperparameter Tuning. color_branches: bool or None, default=None Whether to draw every branch in a different color. If None, branches are colored when there is more than one. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the pipeline drawn. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Tip Print atom.pipeline in a notebook for sklearn's interactive visualization of the current pipeline.","title":"plot_pipeline"},{"location":"API/plots/plot_pipeline/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . impute ( strat_num = \"median\" ) atom . encode ( max_onehot = 5 ) atom . run ([ \"GNB\" , \"RNN\" , \"SGD\" , \"MLP\" ]) atom . voting ( models = atom . winners [: 2 ]) atom . plot_pipeline () # For a single branch from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . scale () atom . prune () atom . run ( \"RF\" , n_calls = 10 , n_initial_points = 3 ) atom . branch = \"oversample\" atom . balance ( strategy = \"adasyn\" ) atom . run ( \"RF_os\" ) atom . branch = \"undersample_from_master\" atom . balance ( strategy = \"nearmiss\" ) atom . run ( \"RF_us\" ) atom . plot_pipeline () # For multiple branches","title":"Example"},{"location":"API/plots/plot_prc/","text":"plot_prc method plot_prc (models=None, dataset=\"test\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the Precision-Recall Curve (PRC). The legend shows the average precision (AP) score. Only for binary classification tasks. Read more about PRC in sklearn's documentation . Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"LR\" , \"RF\" , \"LGB\" ], metric = \"average_precision\" ) atom . plot_prc ()","title":"plot_prc"},{"location":"API/plots/plot_prc/#plot_prc","text":"method plot_prc (models=None, dataset=\"test\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the Precision-Recall Curve (PRC). The legend shows the average precision (AP) score. Only for binary classification tasks. Read more about PRC in sklearn's documentation . Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_prc"},{"location":"API/plots/plot_prc/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"LR\" , \"RF\" , \"LGB\" ], metric = \"average_precision\" ) atom . plot_prc ()","title":"Example"},{"location":"API/plots/plot_probabilities/","text":"plot_probabilities method plot_probabilities (models=None, dataset=\"test\", target=1, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the probability distribution of the classes in the target column. Only for classification tasks. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". target: int or str, default=1 Probability of being that class in the target column (as index or name). Only for multiclass classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y = \"RainTomorrow\" ) atom . run ( \"rf\" ) atom . plot_probabilities ()","title":"plot_probabilities"},{"location":"API/plots/plot_probabilities/#plot_probabilities","text":"method plot_probabilities (models=None, dataset=\"test\", target=1, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the probability distribution of the classes in the target column. Only for classification tasks. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". target: int or str, default=1 Probability of being that class in the target column (as index or name). Only for multiclass classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_probabilities"},{"location":"API/plots/plot_probabilities/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y = \"RainTomorrow\" ) atom . run ( \"rf\" ) atom . plot_probabilities ()","title":"Example"},{"location":"API/plots/plot_qq/","text":"plot_qq method plot_qq (columns=0, distributions=\"norm\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot a quantile-quantile plot. Parameters: columns: int, str, slice or sequence, default=0 Slice, names or indices of the columns to plot. Selected categorical columns are ignored. distributions: str, sequence or None, default=\"norm\" Name of the scipy.stats distributions to fit to the columns. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6)) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . plot_qq ( columns = [ 0 , 1 ], distributions = \"triang\" )","title":"plot_qq"},{"location":"API/plots/plot_qq/#plot_qq","text":"method plot_qq (columns=0, distributions=\"norm\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot a quantile-quantile plot. Parameters: columns: int, str, slice or sequence, default=0 Slice, names or indices of the columns to plot. Selected categorical columns are ignored. distributions: str, sequence or None, default=\"norm\" Name of the scipy.stats distributions to fit to the columns. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6)) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_qq"},{"location":"API/plots/plot_qq/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . plot_qq ( columns = [ 0 , 1 ], distributions = \"triang\" )","title":"Example"},{"location":"API/plots/plot_residuals/","text":"plot_residuals method plot_residuals (models=None, dataset=\"test\", title=None, figsize=(10, 6), filename=None, display=True) [source] The plot shows the residuals (difference between the predicted and the true value) on the vertical axis and the independent variable on the horizontal axis. The gray, intersected line shows the identity line. This plot can be useful to analyze the variance of the error of the regressor. If the points are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a non-linear model is more appropriate. Only for regression tasks. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the residuals. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ([ \"OLS\" , \"LGB\" ], metric = \"MAE\" ) atom . plot_residuals ()","title":"plot_residuals"},{"location":"API/plots/plot_residuals/#plot_residuals","text":"method plot_residuals (models=None, dataset=\"test\", title=None, figsize=(10, 6), filename=None, display=True) [source] The plot shows the residuals (difference between the predicted and the true value) on the vertical axis and the independent variable on the horizontal axis. The gray, intersected line shows the identity line. This plot can be useful to analyze the variance of the error of the regressor. If the points are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a non-linear model is more appropriate. Only for regression tasks. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the residuals. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_residuals"},{"location":"API/plots/plot_residuals/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ([ \"OLS\" , \"LGB\" ], metric = \"MAE\" ) atom . plot_residuals ()","title":"Example"},{"location":"API/plots/plot_results/","text":"plot_results method plot_results (models=None, metric=0, title=None, figsize=None, filename=None, display=True) [source] Plot of the model results after the evaluation. If all models applied bootstrap, the plot is a boxplot. If not, the plot is a barplot. Models are ordered based on their score from the top down. The score is either the mean_bootstrap or metric_test attribute of the model, selected in that order. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. metric: int or str, default=0 Index or name of the metric to plot. Only for multi-metric runs. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of models shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"QDA\" , \"Tree\" , \"RF\" , \"ET\" , \"LGB\" ], metric = \"f1\" , n_bootstrap = 5 ) atom . plot_results () # With bootstrap... # And without bootstrap... atom . run ([ \"QDA\" , \"Tree\" , \"RF\" , \"ET\" , \"LGB\" ], metric = \"f1\" , n_bootstrap = 0 ) atom . plot_results ()","title":"plot_results"},{"location":"API/plots/plot_results/#plot_results","text":"method plot_results (models=None, metric=0, title=None, figsize=None, filename=None, display=True) [source] Plot of the model results after the evaluation. If all models applied bootstrap, the plot is a boxplot. If not, the plot is a barplot. Models are ordered based on their score from the top down. The score is either the mean_bootstrap or metric_test attribute of the model, selected in that order. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. metric: int or str, default=0 Index or name of the metric to plot. Only for multi-metric runs. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of models shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_results"},{"location":"API/plots/plot_results/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"QDA\" , \"Tree\" , \"RF\" , \"ET\" , \"LGB\" ], metric = \"f1\" , n_bootstrap = 5 ) atom . plot_results () # With bootstrap... # And without bootstrap... atom . run ([ \"QDA\" , \"Tree\" , \"RF\" , \"ET\" , \"LGB\" ], metric = \"f1\" , n_bootstrap = 0 ) atom . plot_results ()","title":"Example"},{"location":"API/plots/plot_rfecv/","text":"plot_rfecv method plot_rfecv (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the rfecv results, i.e. the scores obtained by the estimator fitted on every subset of the dataset. Only available if rfecv was applied on the data. Parameters: title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . feature_selection ( strategy = \"rfecv\" , solver = \"LGB\" , scoring = \"precision\" ) atom . plot_rfecv ()","title":"plot_rfecv"},{"location":"API/plots/plot_rfecv/#plot_rfecv","text":"method plot_rfecv (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the rfecv results, i.e. the scores obtained by the estimator fitted on every subset of the dataset. Only available if rfecv was applied on the data. Parameters: title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_rfecv"},{"location":"API/plots/plot_rfecv/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . feature_selection ( strategy = \"rfecv\" , solver = \"LGB\" , scoring = \"precision\" ) atom . plot_rfecv ()","title":"Example"},{"location":"API/plots/plot_roc/","text":"plot_roc method plot_roc (models=None, dataset=\"test\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the Receiver Operating Characteristics Curve (ROC). The legend shows the Area Under the ROC Curve (AUC) score. Only for binary classification tasks. Read more about ROC in sklearn's documentation . Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"LR\" , \"RF\" , \"LGB\" ], metric = \"roc_auc\" ) atom . plot_roc ( filename = \"roc_curve\" )","title":"plot_roc"},{"location":"API/plots/plot_roc/#plot_roc","text":"method plot_roc (models=None, dataset=\"test\", title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the Receiver Operating Characteristics Curve (ROC). The legend shows the Area Under the ROC Curve (AUC) score. Only for binary classification tasks. Read more about ROC in sklearn's documentation . Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_roc"},{"location":"API/plots/plot_roc/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"LR\" , \"RF\" , \"LGB\" ], metric = \"roc_auc\" ) atom . plot_roc ( filename = \"roc_curve\" )","title":"Example"},{"location":"API/plots/plot_scatter_matrix/","text":"plot_scatter_matrix method plot_scatter_matrix (columns=None, title=None, figsize=(10, 10), filename=None, display=True) [source] Plot a matrix of scatter plots. A subset of max 250 random samples are selected from every column to not clutter the plot. Parameters: columns: slice, sequence or None, default=None Slice, names or indices of the columns to plot. If None, plot all columns in the dataset. Selected categorical columns are ignored. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 10) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for seaborn's pairplot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . plot_scatter_matrix ( columns = slice ( 0 , 5 ))","title":"plot_scatter_matrix"},{"location":"API/plots/plot_scatter_matrix/#plot_scatter_matrix","text":"method plot_scatter_matrix (columns=None, title=None, figsize=(10, 10), filename=None, display=True) [source] Plot a matrix of scatter plots. A subset of max 250 random samples are selected from every column to not clutter the plot. Parameters: columns: slice, sequence or None, default=None Slice, names or indices of the columns to plot. If None, plot all columns in the dataset. Selected categorical columns are ignored. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 10) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for seaborn's pairplot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_scatter_matrix"},{"location":"API/plots/plot_scatter_matrix/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . plot_scatter_matrix ( columns = slice ( 0 , 5 ))","title":"Example"},{"location":"API/plots/plot_shap_bar/","text":"plot_shap_bar method bar_plot (models=None, index=None, show=None, target=1, title=None, figsize=None, filename=None, display=True, **kwargs) [source] Plot SHAP's bar plot. Create a bar plot of a set of SHAP values. If a single sample is passed, then the SHAP values are plotted. If many samples are passed, then the mean absolute value for each feature column is plotted. Read more about SHAP plots in the user guide . Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.xgb.bar_plot() . index: int, str, sequence or None, default=None Index names or positions of the rows in the dataset to plot. If None, it selects all rows in the test set. show: int or None, default=None Number of features (ordered by importance) to show. None to show all. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for SHAP's bar plot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( \"RF\" ) atom . bar_plot () # For multiple samples atom . bar_plot ( index = 120 ) # For a single sample","title":"plot_shap_bar"},{"location":"API/plots/plot_shap_bar/#plot_shap_bar","text":"method bar_plot (models=None, index=None, show=None, target=1, title=None, figsize=None, filename=None, display=True, **kwargs) [source] Plot SHAP's bar plot. Create a bar plot of a set of SHAP values. If a single sample is passed, then the SHAP values are plotted. If many samples are passed, then the mean absolute value for each feature column is plotted. Read more about SHAP plots in the user guide . Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.xgb.bar_plot() . index: int, str, sequence or None, default=None Index names or positions of the rows in the dataset to plot. If None, it selects all rows in the test set. show: int or None, default=None Number of features (ordered by importance) to show. None to show all. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for SHAP's bar plot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_shap_bar"},{"location":"API/plots/plot_shap_bar/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( \"RF\" ) atom . bar_plot () # For multiple samples atom . bar_plot ( index = 120 ) # For a single sample","title":"Example"},{"location":"API/plots/plot_shap_beeswarm/","text":"plot_shap_beeswarm method beeswarm_plot (models=None, index=None, show=None, target=1, title=None, figsize=None, filename=None, display=True, **kwargs) [source] Plot SHAP's beeswarm plot. The plot is colored by feature values. Read more about SHAP plots in the user guide . Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.xgb.beeswarm_plot() . index: slice, sequence or None, default=None Index names or positions of the rows in the dataset to plot. If None, it selects all rows in the test set. The beeswarm plot does not support plotting a single sample. show: int or None, default=None Number of features (ordered by importance) to show. None to show all. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for SHAP's beeswarm plot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( \"RF\" ) atom . beeswarm_plot ()","title":"plot_shap_beeswarm"},{"location":"API/plots/plot_shap_beeswarm/#plot_shap_beeswarm","text":"method beeswarm_plot (models=None, index=None, show=None, target=1, title=None, figsize=None, filename=None, display=True, **kwargs) [source] Plot SHAP's beeswarm plot. The plot is colored by feature values. Read more about SHAP plots in the user guide . Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.xgb.beeswarm_plot() . index: slice, sequence or None, default=None Index names or positions of the rows in the dataset to plot. If None, it selects all rows in the test set. The beeswarm plot does not support plotting a single sample. show: int or None, default=None Number of features (ordered by importance) to show. None to show all. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for SHAP's beeswarm plot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_shap_beeswarm"},{"location":"API/plots/plot_shap_beeswarm/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( \"RF\" ) atom . beeswarm_plot ()","title":"Example"},{"location":"API/plots/plot_shap_decision/","text":"plot_shap_decision method decision_plot (models=None, index=None, show=None, target=1, title=None, figsize=None, filename=None, display=True, **kwargs) [source] Plot SHAP's decision plot. Visualize model decisions using cumulative SHAP values. Each plotted line explains a single model prediction. If a single prediction is plotted, feature values will be printed in the plot (if supplied). If multiple predictions are plotted together, feature values will not be printed. Plotting too many predictions together will make the plot unintelligible. Read more about SHAP plots in the user guide . Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.xgb.decision_plot() . index: int, str, sequence or None, default=None Index names or positions of the rows in the dataset to plot. If None, it selects all rows in the test set. show: int or None, default=None Number of features (ordered by importance) to show. None to show all. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for SHAP's decision plot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( \"RF\" ) atom . decision_plot () # For multiple samples atom . decision_plot ( index = 120 ) # For a single sample","title":"plot_shap_decision"},{"location":"API/plots/plot_shap_decision/#plot_shap_decision","text":"method decision_plot (models=None, index=None, show=None, target=1, title=None, figsize=None, filename=None, display=True, **kwargs) [source] Plot SHAP's decision plot. Visualize model decisions using cumulative SHAP values. Each plotted line explains a single model prediction. If a single prediction is plotted, feature values will be printed in the plot (if supplied). If multiple predictions are plotted together, feature values will not be printed. Plotting too many predictions together will make the plot unintelligible. Read more about SHAP plots in the user guide . Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.xgb.decision_plot() . index: int, str, sequence or None, default=None Index names or positions of the rows in the dataset to plot. If None, it selects all rows in the test set. show: int or None, default=None Number of features (ordered by importance) to show. None to show all. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for SHAP's decision plot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_shap_decision"},{"location":"API/plots/plot_shap_decision/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( \"RF\" ) atom . decision_plot () # For multiple samples atom . decision_plot ( index = 120 ) # For a single sample","title":"Example"},{"location":"API/plots/plot_shap_force/","text":"plot_shap_force method force_plot (models=None, index=None, show=None, target=1, title=None, figsize=(14, 6), filename=None, display=True, **kwargs) [source] Plot SHAP's force plot. Visualize the given SHAP values with an additive force layout. Note that by default this plot will render using javascript. For a regular figure use matplotlib=True (this option is only available when only a single sample is plotted). Read more about SHAP plots in the user guide . Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.xgb.force_plot() . index: int, str, sequence or None, default=None Index names or positions of the rows in the dataset to plot. If None, it selects all rows in the test set. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(14, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. If matplotlib=False, the figure is saved as a html file. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for SHAP's force plot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None and matplotlib=True . Warning This plot can not be called from a canvas because of incompatibility between the ATOM and shap API. Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( \"lr\" ) atom . force_plot ( index = 120 , matplotlib = True , filename = \"force_plot\" )","title":"plot_shap_force"},{"location":"API/plots/plot_shap_force/#plot_shap_force","text":"method force_plot (models=None, index=None, show=None, target=1, title=None, figsize=(14, 6), filename=None, display=True, **kwargs) [source] Plot SHAP's force plot. Visualize the given SHAP values with an additive force layout. Note that by default this plot will render using javascript. For a regular figure use matplotlib=True (this option is only available when only a single sample is plotted). Read more about SHAP plots in the user guide . Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.xgb.force_plot() . index: int, str, sequence or None, default=None Index names or positions of the rows in the dataset to plot. If None, it selects all rows in the test set. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(14, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. If matplotlib=False, the figure is saved as a html file. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for SHAP's force plot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None and matplotlib=True . Warning This plot can not be called from a canvas because of incompatibility between the ATOM and shap API.","title":"plot_shap_force"},{"location":"API/plots/plot_shap_force/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( \"lr\" ) atom . force_plot ( index = 120 , matplotlib = True , filename = \"force_plot\" )","title":"Example"},{"location":"API/plots/plot_shap_heatmap/","text":"plot_shap_heatmap method heatmap_plot (models=None, index=None, show=None, target=1, title=None, figsize=None, filename=None, display=True, **kwargs) [source] Plot SHAP's heatmap plot. This plot is designed to show the population substructure of a dataset using supervised clustering and a heatmap. Supervised clustering involves clustering data points not by their original feature values but by their explanations. Read more about SHAP plots in the user guide . Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.xgb.heatmap_plot() . index: slice, sequence or None, default=None Index names or positions of the rows in the dataset to plot. If None, it selects all rows in the test set. The heatmap plot does not support plotting a single sample. show: int or None, default=None Number of features (ordered by importance) to show. None to show all. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for SHAP's heatmap plot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( \"RF\" ) atom . heatmap_plot ()","title":"plot_shap_heatmap"},{"location":"API/plots/plot_shap_heatmap/#plot_shap_heatmap","text":"method heatmap_plot (models=None, index=None, show=None, target=1, title=None, figsize=None, filename=None, display=True, **kwargs) [source] Plot SHAP's heatmap plot. This plot is designed to show the population substructure of a dataset using supervised clustering and a heatmap. Supervised clustering involves clustering data points not by their original feature values but by their explanations. Read more about SHAP plots in the user guide . Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.xgb.heatmap_plot() . index: slice, sequence or None, default=None Index names or positions of the rows in the dataset to plot. If None, it selects all rows in the test set. The heatmap plot does not support plotting a single sample. show: int or None, default=None Number of features (ordered by importance) to show. None to show all. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for SHAP's heatmap plot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_shap_heatmap"},{"location":"API/plots/plot_shap_heatmap/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( \"RF\" ) atom . heatmap_plot ()","title":"Example"},{"location":"API/plots/plot_shap_scatter/","text":"plot_shap_scatter method scatter_plot (models=None, index=None, feature=0, target=1, title=None, figsize=(10, 6), filename=None, display=True, **kwargs) [source] Plot SHAP's scatter plot. Plots the value of the feature on the x-axis and the SHAP value of the same feature on the y-axis. This shows how the model depends on the given feature, and is like a richer extension of the classical partial dependence plots. Vertical dispersion of the data points represents interaction effects. Read more about SHAP plots in the user guide . Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.xgb.scatter_plot() . index: slice, sequence or None, default=None Index names or positions of the rows in the dataset to plot. If None, it selects all rows in the test set. The scatter plot does not support plotting a single sample. feature: int or str, default=0 Index or name of the feature to plot. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6)) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for SHAP's scatter plot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( \"RF\" ) atom . scatter_plot ( feature = \"bmi\" )","title":"plot_shap_scatter"},{"location":"API/plots/plot_shap_scatter/#plot_shap_scatter","text":"method scatter_plot (models=None, index=None, feature=0, target=1, title=None, figsize=(10, 6), filename=None, display=True, **kwargs) [source] Plot SHAP's scatter plot. Plots the value of the feature on the x-axis and the SHAP value of the same feature on the y-axis. This shows how the model depends on the given feature, and is like a richer extension of the classical partial dependence plots. Vertical dispersion of the data points represents interaction effects. Read more about SHAP plots in the user guide . Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.xgb.scatter_plot() . index: slice, sequence or None, default=None Index names or positions of the rows in the dataset to plot. If None, it selects all rows in the test set. The scatter plot does not support plotting a single sample. feature: int or str, default=0 Index or name of the feature to plot. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6)) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for SHAP's scatter plot . Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_shap_scatter"},{"location":"API/plots/plot_shap_scatter/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor ( X , y ) atom . run ( \"RF\" ) atom . scatter_plot ( feature = \"bmi\" )","title":"Example"},{"location":"API/plots/plot_shap_waterfall/","text":"plot_shap_waterfall method waterfall_plot (models=None, index=None, show=None, target=1, title=None, figsize=None, filename=None, display=True, **kwargs) [source] Plot SHAP's waterfall plot for a single prediction. The SHAP value of a feature represents the impact of the evidence provided by that feature on the model\u2019s output. The waterfall plot is designed to visually display how the SHAP values (evidence) of each feature move the model output from our prior expectation under the background data distribution, to the final model prediction given the evidence of all the features. Features are sorted by the magnitude of their SHAP values with the smallest magnitude features grouped together at the bottom of the plot when the number of features in the models exceeds the show parameter. Read more about SHAP plots in the user guide . Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.xgb.waterfall_plot() . index: int, str or None, default=None Index or position of the row in the dataset to plot. If None, it selects the first row in the test set. The waterfall plot does not support plotting multiple samples. show: int or None, default=None Number of features to show in the plot. None to show all. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( \"Tree\" ) atom . tree . waterfall_plot ( index = 120 )","title":"plot_shap_waterfall"},{"location":"API/plots/plot_shap_waterfall/#plot_shap_waterfall","text":"method waterfall_plot (models=None, index=None, show=None, target=1, title=None, figsize=None, filename=None, display=True, **kwargs) [source] Plot SHAP's waterfall plot for a single prediction. The SHAP value of a feature represents the impact of the evidence provided by that feature on the model\u2019s output. The waterfall plot is designed to visually display how the SHAP values (evidence) of each feature move the model output from our prior expectation under the background data distribution, to the final model prediction given the evidence of all the features. Features are sorted by the magnitude of their SHAP values with the smallest magnitude features grouped together at the bottom of the plot when the number of features in the models exceeds the show parameter. Read more about SHAP plots in the user guide . Parameters: models: int, str, slice, sequence or None, default=None Name of the model to plot. If None, all models are selected. Note that leaving the default option could raise an exception if there are multiple models. To avoid this, call the plot from a model, e.g. atom.xgb.waterfall_plot() . index: int, str or None, default=None Index or position of the row in the dataset to plot. If None, it selects the first row in the test set. The waterfall plot does not support plotting multiple samples. show: int or None, default=None Number of features to show in the plot. None to show all. target: int or str, default=1 Index or name of the class in the target column to look at. Only for multi-class classification tasks. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of features shown. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_shap_waterfall"},{"location":"API/plots/plot_shap_waterfall/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ( \"Tree\" ) atom . tree . waterfall_plot ( index = 120 )","title":"Example"},{"location":"API/plots/plot_successive_halving/","text":"plot_successive_halving method plot_successive_halving (models=None, metric=0, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot of the models' scores per iteration of the successive halving. Only use with models fitted using successive halving . Ensemble models are ignored. Parameters: models: int, str, slice, sequence or None, default=None Name of the models to plot. If None, all the models are selected. metric: int or str, default=0 Index or name of the metric to plot. Only for multi-metric runs. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . successive_halving ( models = [ \"tree\" , \"et\" , \"rf\" , \"xgb\" , \"lgb\" , \"catb\" ], metric = \"f1_weighted\" , n_bootstrap = 6 , ) atom . plot_successive_halving ( filename = \"successive_halving\" )","title":"plot_successive_halving"},{"location":"API/plots/plot_successive_halving/#plot_successive_halving","text":"method plot_successive_halving (models=None, metric=0, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot of the models' scores per iteration of the successive halving. Only use with models fitted using successive halving . Ensemble models are ignored. Parameters: models: int, str, slice, sequence or None, default=None Name of the models to plot. If None, all the models are selected. metric: int or str, default=0 Index or name of the metric to plot. Only for multi-metric runs. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_successive_halving"},{"location":"API/plots/plot_successive_halving/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . successive_halving ( models = [ \"tree\" , \"et\" , \"rf\" , \"xgb\" , \"lgb\" , \"catb\" ], metric = \"f1_weighted\" , n_bootstrap = 6 , ) atom . plot_successive_halving ( filename = \"successive_halving\" )","title":"Example"},{"location":"API/plots/plot_threshold/","text":"plot_threshold method plot_threshold (models=None, metric=None, dataset=\"test\", steps=100, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot metric performances against threshold values. Only for binary classification tasks. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. metric: str, func, scorer, sequence or None, default=None Metric to plot. Choose from any of sklearn's SCORERS , a function with signature metric(y_true, y_pred) , a scorer object or a sequence of these. If None, the metric used to run the pipeline is plotted. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". steps: int, default=100 Number of thresholds measured. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier from sklearn.metrics import recall_score atom = ATOMClassifier ( X , y ) atom . run ( \"LGB\" ) atom . plot_threshold ( metric = [ \"accuracy\" , \"f1\" , recall_score ])","title":"plot_threshold"},{"location":"API/plots/plot_threshold/#plot_threshold","text":"method plot_threshold (models=None, metric=None, dataset=\"test\", steps=100, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot metric performances against threshold values. Only for binary classification tasks. Parameters: models: int, str, slice, sequence or None, default=None Name or index of the models to plot. If None, all models are selected. metric: str, func, scorer, sequence or None, default=None Metric to plot. Choose from any of sklearn's SCORERS , a function with signature metric(y_true, y_pred) , a scorer object or a sequence of these. If None, the metric used to run the pipeline is plotted. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\", \"both\" (train and test) or \"holdout\". steps: int, default=100 Number of thresholds measured. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_threshold"},{"location":"API/plots/plot_threshold/#example","text":"from atom import ATOMClassifier from sklearn.metrics import recall_score atom = ATOMClassifier ( X , y ) atom . run ( \"LGB\" ) atom . plot_threshold ( metric = [ \"accuracy\" , \"f1\" , recall_score ])","title":"Example"},{"location":"API/plots/plot_trials/","text":"plot_trials method plot_bo (models=None, metric=0, title=None, figsize=(10, 8), filename=None, display=True) [source] Plot the bayesian optimization scores. Only for models that ran hyperparameter tuning. This is the same plot as the one produced by bo_params={\"plot\": True} while running the BO. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Parameters: models: int, str, slice, sequence or None, default=None Name of the models to plot. If None, all models that used hyperparameter tuning are selected. metric: int or str, default=0 Index or name of the metric to plot. Only for multi-metric runs. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 8) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"LDA\" , \"LGB\" ], metric = \"f1\" , n_calls = 24 , n_initial_points = 10 ) atom . plot_bo ()","title":"plot_trials"},{"location":"API/plots/plot_trials/#plot_trials","text":"method plot_bo (models=None, metric=0, title=None, figsize=(10, 8), filename=None, display=True) [source] Plot the bayesian optimization scores. Only for models that ran hyperparameter tuning. This is the same plot as the one produced by bo_params={\"plot\": True} while running the BO. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Parameters: models: int, str, slice, sequence or None, default=None Name of the models to plot. If None, all models that used hyperparameter tuning are selected. metric: int or str, default=0 Index or name of the metric to plot. Only for multi-metric runs. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 8) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_trials"},{"location":"API/plots/plot_trials/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X , y ) atom . run ([ \"LDA\" , \"LGB\" ], metric = \"f1\" , n_calls = 24 , n_initial_points = 10 ) atom . plot_bo ()","title":"Example"},{"location":"API/plots/plot_wordcloud/","text":"plot_wordcloud method plot_wordcloud (index=None, title=None, figsize=(10, 6), filename=None, display=True, **kwargs) [source] Plot a wordcloud from the corpus. The text for the plot is extracted from the column named corpus . If there is no column with that name, an exception is raised. Parameters: index: int, str, sequence or None, default=None Index names or positions of the documents in the corpus to include in the wordcloud. If None, it selects all documents in the dataset. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6)) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for the WordCloud class. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None . Example from atom import ATOMClassifier atom = ATOMClassifier ( X_text , y_text ) atom . plot_wordcloud ()","title":"plot_wordcloud"},{"location":"API/plots/plot_wordcloud/#plot_wordcloud","text":"method plot_wordcloud (index=None, title=None, figsize=(10, 6), filename=None, display=True, **kwargs) [source] Plot a wordcloud from the corpus. The text for the plot is extracted from the column named corpus . If there is no column with that name, an exception is raised. Parameters: index: int, str, sequence or None, default=None Index names or positions of the documents in the corpus to include in the wordcloud. If None, it selects all documents in the dataset. title: str or None, default=None Plot's title. If None, the title is left empty. figsize: tuple, default=(10, 6)) Figure's size, format as (x, y). filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool or None, default=True Whether to render the plot. If None, it returns the matplotlib figure. **kwargs Additional keyword arguments for the WordCloud class. Returns: matplotlib.figure.Figure Plot object. Only returned if display=None .","title":"plot_wordcloud"},{"location":"API/plots/plot_wordcloud/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier ( X_text , y_text ) atom . plot_wordcloud ()","title":"Example"},{"location":"API/predicting/decision_function/","text":"decision_function method decision_function (X, verbose=None) [source] Get confidence scores on new data or rows in the dataset. New data is first transformed through the model's pipeline. Transformers that are only applied on the training set are skipped. The estimator must have a decision_function method. Read more in the user guide . Parameters X: int, str, slice, sequence or dataframe-like Names or indices of rows in the dataset, or new feature set with shape=(n_samples, n_features). verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns pd.Series or pd.DataFrame Predicted confidence scores, with shape=(n_samples,) for binary classification tasks and (n_samples, n_classes) for multiclass classification tasks. Example >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> # Load data and separate last 5 rows for predictions >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X_new , y_new = X . iloc [ - 5 :], y . iloc [ - 5 :] >>> X , y = X . iloc [: - 5 ], y . iloc [: - 5 ] >>> atom = ATOMClassifier ( data ) >>> atom . run ( \"LR\" ) >>> # Using new data >>> atom . lr . decision_function ( X_new ) 0 -20.872124 1 -13.856470 2 -4.496618 3 -23.196171 4 10.066044 Name: decision_function, dtype: float64 >>> # Using indices >>> atom . lr . decision_function ([ 23 , 25 ]) # Retrieve prediction of rows 23 and 25 23 -15.286529 25 -4.457036 dtype: float64","title":"decision_function"},{"location":"API/predicting/decision_function/#decision_function","text":"method decision_function (X, verbose=None) [source] Get confidence scores on new data or rows in the dataset. New data is first transformed through the model's pipeline. Transformers that are only applied on the training set are skipped. The estimator must have a decision_function method. Read more in the user guide . Parameters X: int, str, slice, sequence or dataframe-like Names or indices of rows in the dataset, or new feature set with shape=(n_samples, n_features). verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns pd.Series or pd.DataFrame Predicted confidence scores, with shape=(n_samples,) for binary classification tasks and (n_samples, n_classes) for multiclass classification tasks.","title":"decision_function"},{"location":"API/predicting/decision_function/#example","text":">>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> # Load data and separate last 5 rows for predictions >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X_new , y_new = X . iloc [ - 5 :], y . iloc [ - 5 :] >>> X , y = X . iloc [: - 5 ], y . iloc [: - 5 ] >>> atom = ATOMClassifier ( data ) >>> atom . run ( \"LR\" ) >>> # Using new data >>> atom . lr . decision_function ( X_new ) 0 -20.872124 1 -13.856470 2 -4.496618 3 -23.196171 4 10.066044 Name: decision_function, dtype: float64 >>> # Using indices >>> atom . lr . decision_function ([ 23 , 25 ]) # Retrieve prediction of rows 23 and 25 23 -15.286529 25 -4.457036 dtype: float64","title":"Example"},{"location":"API/predicting/predict/","text":"predict method predict (X, verbose=None) [source] Get class predictions on new data or rows in the dataset. New data is first transformed through the model's pipeline. Transformers that are only applied on the training set are skipped. The estimator must have a predict method. Read more in the user guide . Parameters X: int, str, slice, sequence or dataframe-like Names or indices of rows in the dataset, or new feature set with shape=(n_samples, n_features). verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns pd.Series Predicted classes with shape=(n_samples,). Example >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> # Load data and separate last 5 rows for predictions >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X_new , y_new = X . iloc [ - 5 :], y . iloc [ - 5 :] >>> X , y = X . iloc [: - 5 ], y . iloc [: - 5 ] >>> atom = ATOMClassifier ( data ) >>> atom . run ( \"LR\" ) >>> # Using new data >>> atom . predict ( X_new ) 0 0 1 0 2 0 3 0 4 1 Name: predict, dtype: int32 >>> # Using indices >>> atom . predict ([ 23 , 25 ]) # Retrieve prediction of rows 23 and 25 23 1 25 1 dtype: int32","title":"predict"},{"location":"API/predicting/predict/#predict","text":"method predict (X, verbose=None) [source] Get class predictions on new data or rows in the dataset. New data is first transformed through the model's pipeline. Transformers that are only applied on the training set are skipped. The estimator must have a predict method. Read more in the user guide . Parameters X: int, str, slice, sequence or dataframe-like Names or indices of rows in the dataset, or new feature set with shape=(n_samples, n_features). verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns pd.Series Predicted classes with shape=(n_samples,).","title":"predict"},{"location":"API/predicting/predict/#example","text":">>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> # Load data and separate last 5 rows for predictions >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X_new , y_new = X . iloc [ - 5 :], y . iloc [ - 5 :] >>> X , y = X . iloc [: - 5 ], y . iloc [: - 5 ] >>> atom = ATOMClassifier ( data ) >>> atom . run ( \"LR\" ) >>> # Using new data >>> atom . predict ( X_new ) 0 0 1 0 2 0 3 0 4 1 Name: predict, dtype: int32 >>> # Using indices >>> atom . predict ([ 23 , 25 ]) # Retrieve prediction of rows 23 and 25 23 1 25 1 dtype: int32","title":"Example"},{"location":"API/predicting/predict_log_proba/","text":"predict_log_proba method predict_log_proba (X, verbose=None) [source] Get class log-probabilities on new data or rows in the dataset. New data is first transformed through the model's pipeline. Transformers that are only applied on the training set are skipped. The estimator must have a predict_log_proba method. Read more in the user guide . Parameters X: int, str, slice, sequence or dataframe-like Names or indices of rows in the dataset, or new feature set with shape=(n_samples, n_features). verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns pd.DataFrame Predicted class log-probabilities with shape=(n_samples, n_classes). Example >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> # Load data and separate last 5 rows for predictions >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X_new , y_new = X . iloc [ - 5 :], y . iloc [ - 5 :] >>> X , y = X . iloc [: - 5 ], y . iloc [: - 5 ] >>> atom = ATOMClassifier ( data ) >>> atom . run ( \"LR\" ) >>> # Using new data >>> atom . predict_log_proba ( X_new ) 0 1 0 -6.024211e-10 -21.230064 1 -3.525172e-07 -14.858167 2 -1.285206e-02 -4.360670 3 -6.837442e-11 -23.406023 4 -1.076932e+01 -0.000021 >>> # Using indices >>> atom . predict_log_proba ([ 23 , 25 ]) # Retrieve prediction of rows 23 and 25 0 1 23 -4.191844 -0.015234 25 -5.207398 -0.005491","title":"predict_log_proba"},{"location":"API/predicting/predict_log_proba/#predict_log_proba","text":"method predict_log_proba (X, verbose=None) [source] Get class log-probabilities on new data or rows in the dataset. New data is first transformed through the model's pipeline. Transformers that are only applied on the training set are skipped. The estimator must have a predict_log_proba method. Read more in the user guide . Parameters X: int, str, slice, sequence or dataframe-like Names or indices of rows in the dataset, or new feature set with shape=(n_samples, n_features). verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns pd.DataFrame Predicted class log-probabilities with shape=(n_samples, n_classes).","title":"predict_log_proba"},{"location":"API/predicting/predict_log_proba/#example","text":">>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> # Load data and separate last 5 rows for predictions >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X_new , y_new = X . iloc [ - 5 :], y . iloc [ - 5 :] >>> X , y = X . iloc [: - 5 ], y . iloc [: - 5 ] >>> atom = ATOMClassifier ( data ) >>> atom . run ( \"LR\" ) >>> # Using new data >>> atom . predict_log_proba ( X_new ) 0 1 0 -6.024211e-10 -21.230064 1 -3.525172e-07 -14.858167 2 -1.285206e-02 -4.360670 3 -6.837442e-11 -23.406023 4 -1.076932e+01 -0.000021 >>> # Using indices >>> atom . predict_log_proba ([ 23 , 25 ]) # Retrieve prediction of rows 23 and 25 0 1 23 -4.191844 -0.015234 25 -5.207398 -0.005491","title":"Example"},{"location":"API/predicting/predict_proba/","text":"predict_proba method predict_proba (X, verbose=None) [source] Get class probabilities on new data or rows in the dataset. New data is first transformed through the model's pipeline. Transformers that are only applied on the training set are skipped. The estimator must have a predict_proba method. Read more in the user guide . Parameters X: int, str, slice, sequence or dataframe-like Names or indices of rows in the dataset, or new feature set with shape=(n_samples, n_features). verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns pd.DataFrame Predicted class probabilities with shape=(n_samples, n_classes). Example >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> # Load data and separate last 5 rows for predictions >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X_new , y_new = X . iloc [ - 5 :], y . iloc [ - 5 :] >>> X , y = X . iloc [: - 5 ], y . iloc [: - 5 ] >>> atom = ATOMClassifier ( data ) >>> atom . run ( \"LR\" ) >>> # Using new data >>> atom . predict_proba ( X_new ) 0 1 0 1.000000 4.036791e-10 1 1.000000 4.856420e-07 2 0.981879 1.812090e-02 3 1.000000 6.081561e-11 4 0.000025 9.999746e-01 >>> # Using indices >>> atom . predict_proba ([ 23 , 25 ]) # Retrieve prediction of rows 23 and 25 0 1 23 0.000892 0.999108 25 0.975733 0.024267","title":"predict_proba"},{"location":"API/predicting/predict_proba/#predict_proba","text":"method predict_proba (X, verbose=None) [source] Get class probabilities on new data or rows in the dataset. New data is first transformed through the model's pipeline. Transformers that are only applied on the training set are skipped. The estimator must have a predict_proba method. Read more in the user guide . Parameters X: int, str, slice, sequence or dataframe-like Names or indices of rows in the dataset, or new feature set with shape=(n_samples, n_features). verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns pd.DataFrame Predicted class probabilities with shape=(n_samples, n_classes).","title":"predict_proba"},{"location":"API/predicting/predict_proba/#example","text":">>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> # Load data and separate last 5 rows for predictions >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X_new , y_new = X . iloc [ - 5 :], y . iloc [ - 5 :] >>> X , y = X . iloc [: - 5 ], y . iloc [: - 5 ] >>> atom = ATOMClassifier ( data ) >>> atom . run ( \"LR\" ) >>> # Using new data >>> atom . predict_proba ( X_new ) 0 1 0 1.000000 4.036791e-10 1 1.000000 4.856420e-07 2 0.981879 1.812090e-02 3 1.000000 6.081561e-11 4 0.000025 9.999746e-01 >>> # Using indices >>> atom . predict_proba ([ 23 , 25 ]) # Retrieve prediction of rows 23 and 25 0 1 23 0.000892 0.999108 25 0.975733 0.024267","title":"Example"},{"location":"API/predicting/score/","text":"score method score (X, y=None, metric=None, sample_weight=None, verbose=None) [source] Get a metric score on new data. New data is first transformed through the model's pipeline. Transformers that are only applied on the training set are skipped. If called from atom, the best model (under the winner attribute) is used. If called from a model, that model is used. Read more in the user guide . Info If the metric parameter is left to its default value, the method uses the same metric as sklearn's score method. Parameters X: int, str, slice, sequence or dataframe-like Names or indices of rows in the dataset, or new feature set with shape=(n_samples, n_features). y: int, str or sequence Target column corresponding to X. - If int: Position of the target column in X. - If str: Name of the target column in X. - Else: Array with shape=(n_samples,) to use as target. metric: str, func, scorer or None, default=None Metric to calculate. Choose from any of sklearn's scorers, a function with signature metric(y_true, y_pred) -> score or a scorer object. If None, it returns mean accuracy for classification tasks and r2 for regression tasks. sample_weight: sequence or None, default=None Sample weights corresponding to y. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns float Metric score of X with respect to y. Example >>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> # Load data and separate last 5 rows for predictions >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X_new , y_new = X . iloc [ - 5 :], y . iloc [ - 5 :] >>> X , y = X . iloc [: - 5 ], y . iloc [: - 5 ] >>> atom = ATOMClassifier ( data ) >>> atom . run ( \"LR\" ) >>> # Using new data >>> atom . score ( X_new , y_new ) 1.0 >>> # Using indices >>> atom . score ( slice ( 10 , 92 )) 0.975609756097561 >>> # Using a custom metric >>> atom . score ( slice ( 10 , 92 ), metric = \"f1\" ) 0.9824561403508771","title":"score"},{"location":"API/predicting/score/#score","text":"method score (X, y=None, metric=None, sample_weight=None, verbose=None) [source] Get a metric score on new data. New data is first transformed through the model's pipeline. Transformers that are only applied on the training set are skipped. If called from atom, the best model (under the winner attribute) is used. If called from a model, that model is used. Read more in the user guide . Info If the metric parameter is left to its default value, the method uses the same metric as sklearn's score method. Parameters X: int, str, slice, sequence or dataframe-like Names or indices of rows in the dataset, or new feature set with shape=(n_samples, n_features). y: int, str or sequence Target column corresponding to X. - If int: Position of the target column in X. - If str: Name of the target column in X. - Else: Array with shape=(n_samples,) to use as target. metric: str, func, scorer or None, default=None Metric to calculate. Choose from any of sklearn's scorers, a function with signature metric(y_true, y_pred) -> score or a scorer object. If None, it returns mean accuracy for classification tasks and r2 for regression tasks. sample_weight: sequence or None, default=None Sample weights corresponding to y. verbose: int or None, default=None Verbosity level of the output. If None, it uses the transformer's own verbosity. Returns float Metric score of X with respect to y.","title":"score"},{"location":"API/predicting/score/#example","text":">>> from atom import ATOMClassifier >>> from sklearn.datasets import load_breast_cancer >>> # Load data and separate last 5 rows for predictions >>> X , y = load_breast_cancer ( return_X_y = True , as_frame = True ) >>> X_new , y_new = X . iloc [ - 5 :], y . iloc [ - 5 :] >>> X , y = X . iloc [: - 5 ], y . iloc [: - 5 ] >>> atom = ATOMClassifier ( data ) >>> atom . run ( \"LR\" ) >>> # Using new data >>> atom . score ( X_new , y_new ) 1.0 >>> # Using indices >>> atom . score ( slice ( 10 , 92 )) 0.975609756097561 >>> # Using a custom metric >>> atom . score ( slice ( 10 , 92 ), metric = \"f1\" ) 0.9824561403508771","title":"Example"},{"location":"API/training/directclassifier/","text":"DirectClassifier class atom.training. DirectClassifier (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, n_jobs=1, gpu=False, verbose=0, warnings=True, logger=None, experiment=None, random_state=None) [source] Fit and evaluate the models. The following steps are applied to every model: Hyperparameter tuning is performed using a Bayesian Optimization approach (optional). The model is fitted on the training set using the best combination of hyperparameters found. The model is evaluated on the test set. The model is trained on various bootstrapped samples of the training set and scored again on the test set (optional). You can predict , plot and call any model from the instance. Read more in the user guide . Parameters: models: str, estimator or sequence, default=None Models to fit to the data. Allowed inputs are: an acronym from any of ATOM's predefined models, an ATOMModel or a custom estimator as class or instance. If None, all the predefined models are used. Available predefined models are: \"GP\" for Gaussian Process \"GNB\" for Gaussian Naive Bayes \"MNB\" for Multinomial Naive Bayes \"BNB\" for Bernoulli Naive Bayes \"CatNB\" for Categorical Naive Bayes \"CNB\" for Complement Naive Bayes \"Ridge\" for Ridge Classification \"LR\" for Logistic Regression \"LDA\" for Linear Discriminant Analysis \"QDA\" for Quadratic Discriminant Analysis \"KNN\" for K-Nearest Neighbors \"RNN\" for Radius Nearest Neighbors \"Tree\" for a single Decision Tree \"Bag\" for Bagging \"ET\" for Extra-Trees \"RF\" for Random Forest \"AdaB\" for AdaBoost \"GBM\" for Gradient Boosting Machine \"XGB\" for XGBoost (only available if package is installed) \"LGB\" for LightGBM (only available if package is installed) \"CatB\" for CatBoost (only available if package is installed) \"lSVM\" for Linear SVM \"kSVM\" for Kernel SVM \"PA\" for Passive Aggressive \"SGD\" for Stochastic Gradient Descent \"MLP\" for Multi-layer Perceptron metric: str, func, scorer, sequence or None, default=None Metric on which to fit the models. Choose from any of sklearn's SCORERS , a function with signature metric(y_true, y_pred) , a scorer object or a sequence of these. If multiple metrics are selected, only the first is used to optimize the BO. If None, a default metric is selected: \"f1\" for binary classification \"f1_weighted\" for multiclass classification \"r2\" for regression greater_is_better: bool or sequence, default=True Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_proba: bool or sequence, default=False Whether the metric function requires probability estimates out of a classifier. If True, make sure that every selected model has a predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_threshold: bool or sequence, default=False Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. n_calls: int or sequence, default=0 Maximum number of iterations of the BO. It includes the random points of n_initial_points . If 0, skip the BO and fit the model on its default parameters. If sequence, the n-th value applies to the n-th model. n_initial_points: int or sequence, default=5 Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value applies to the n-th model. est_params: dict, default=None Additional parameters for the estimators. See the corresponding documentation for the available options. For multiple models, use the acronyms as key (or 'all' for all models) and a dict of the parameters as value. Add _fit to the parameter's name to pass it to the fit method instead of the initializer. bo_params: dict, default=None Additional parameters to for the BO. These can include: base_estimator: str, default=\"GP\" Base estimator to use in the BO. Choose from: \"GP\" for Gaussian Process \"RF\" for Random Forest \"ET\" for Extra-Trees \"GBRT\" for Gradient Boosted Regression Trees max_time: int, default=np.inf Stop the optimization after max_time seconds. delta_x: int or float, default=0 Stop the optimization when |x1 - x2| < delta_x . delta_y: int or float, default=0) Stop the optimization if the 5 minima are within delta_y (the function is always minimized. cv: int, default=1 Number of folds for the cross-validation. If 1, the training set is randomly split in a subtrain and validation set. early stopping: int, float or None, default=None Training will stop if the model didn't improve in last early_stopping rounds. If < 1, fraction of rounds from the total. If None, no early stopping is performed. Only available for models that allow in-training evaluation. callback: callable or list of callables, default=None Callbacks for the BO. dimensions: dict, list or None, default=None Custom hyperparameter space for the bayesian optimization. Can be a list to share dimensions across models or a dict with the model's name as key (or 'all' for all models). If None, ATOM's predefined dimensions are used. plot: bool, default=False Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Additional keyword arguments for skopt's optimizer. bootstrap: int or sequence, default=0 Number of data sets (bootstrapped from the training set) to use in the bootstrap algorithm. If 0, no bootstrap is performed. If sequence, the n-th value will apply to the n-th model. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs . Beware that using multiple processes on the same machine may cause memory issues for large datasets. gpu: bool or str, default=False Train models on GPU. Refer to the documentation to check which estimators are supported. If False: Always use CPU implementation. If True: Use GPU implementation if possible. If \"force\": Force GPU implementation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of the actions in python's warnings environment. Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go directly from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random . Magic methods The class contains some magic methods to help you access some of its elements faster. __len__: Returns the length of the dataset. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a model, column or subset of the dataset. Attributes Data attributes The dataset can be accessed at any time through multiple attributes, e.g. calling trainer.train will return the training set. Updating one of the data attributes will automatically update the rest as well. Changing the branch will also change the response from these attributes accordingly. Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: models: list Names of the models in the instance. metric: str or list Metric(s) used to fit the models. errors: dict Dictionary of the encountered exceptions (if any). winners: list of str Model names ordered by performance on the test set (either through the metric_test or mean_bootstrap attribute). winner: model Model subclass that performed best on the test set (either through the metric_test or mean_bootstrap attribute). results: pd.DataFrame Dataframe of the training results. Columns can include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Plot attributes Attributes: style: str Plotting style. See seaborn's documentation . palette: str Color palette. See seaborn's documentation . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes. Methods available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. evaluate Get all models' scores for the provided metrics. get_class_weight Return class weights for a balanced dataset. get_params Get parameters for this estimator. log Save information to the logger and print to stdout. merge Merge another instance of the same class into this one. reset_aesthetics Reset the plot aesthetics to their default values. run Fit and evaluate the models. save Save the instance to a pickle file. set_params Set the parameters of this estimator. stacking Train a Stacking model. voting Train a Voting model. method available_models () [source] Give an overview of the available predefined models. Returns: pd.DataFrame Information about the predefined models available for the current task. Columns include: acronym: Model's acronym (used to call the model). fullname: Complete name of the model. estimator: The model's underlying estimator. module: The estimator's module. needs_scaling: Whether the model requires feature scaling. accepts_sparse: Whether the model has native support for sparse matrices. supports_gpu: Whether the model has GPU support. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters: nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. Deleted models are not removed from any active mlflow experiment. Parameters: models: str or sequence, default=None Models to delete. If None, delete them all. method evaluate (metric=None, dataset=\"test\", threshold=0.5) [source] Get all the models' scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. Returns: pd.DataFrame Scores of the models. method get_class_weights (dataset=\"train\") [source] Return class weights for a balanced data set. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters: dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns: dict Classes with the corresponding weights. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters: other: DirectClassifier Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method run (*arrays) [source] Fit and evaluate the models. Parameters: *arrays: sequence of indexables Training and test set (and optionally a holdout set). Allowed formats are: train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Remember that the class contains the complete dataset as attribute, so the file can become large for big datasets! To avoid this, use save_data=False . Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the data as an attribute of the instance. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: DirectClassifier Estimator instance. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters: name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: Stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's StackingClassifier instance. The predefined model's acronyms can be used for the final_estimator parameter. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters: name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: Vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's VotingClassifier instance. Example from atom.training import DirectClassifier trainer = DirectClassifier ([ \"Tree\" , \"RF\" ], n_calls = 25 , n_initial_points = 10 ) trainer . run ( train , test ) # Analyze the results trainer . plot_prc () print ( trainer . results )","title":"DirectClassifier"},{"location":"API/training/directclassifier/#directclassifier","text":"class atom.training. DirectClassifier (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, n_jobs=1, gpu=False, verbose=0, warnings=True, logger=None, experiment=None, random_state=None) [source] Fit and evaluate the models. The following steps are applied to every model: Hyperparameter tuning is performed using a Bayesian Optimization approach (optional). The model is fitted on the training set using the best combination of hyperparameters found. The model is evaluated on the test set. The model is trained on various bootstrapped samples of the training set and scored again on the test set (optional). You can predict , plot and call any model from the instance. Read more in the user guide . Parameters: models: str, estimator or sequence, default=None Models to fit to the data. Allowed inputs are: an acronym from any of ATOM's predefined models, an ATOMModel or a custom estimator as class or instance. If None, all the predefined models are used. Available predefined models are: \"GP\" for Gaussian Process \"GNB\" for Gaussian Naive Bayes \"MNB\" for Multinomial Naive Bayes \"BNB\" for Bernoulli Naive Bayes \"CatNB\" for Categorical Naive Bayes \"CNB\" for Complement Naive Bayes \"Ridge\" for Ridge Classification \"LR\" for Logistic Regression \"LDA\" for Linear Discriminant Analysis \"QDA\" for Quadratic Discriminant Analysis \"KNN\" for K-Nearest Neighbors \"RNN\" for Radius Nearest Neighbors \"Tree\" for a single Decision Tree \"Bag\" for Bagging \"ET\" for Extra-Trees \"RF\" for Random Forest \"AdaB\" for AdaBoost \"GBM\" for Gradient Boosting Machine \"XGB\" for XGBoost (only available if package is installed) \"LGB\" for LightGBM (only available if package is installed) \"CatB\" for CatBoost (only available if package is installed) \"lSVM\" for Linear SVM \"kSVM\" for Kernel SVM \"PA\" for Passive Aggressive \"SGD\" for Stochastic Gradient Descent \"MLP\" for Multi-layer Perceptron metric: str, func, scorer, sequence or None, default=None Metric on which to fit the models. Choose from any of sklearn's SCORERS , a function with signature metric(y_true, y_pred) , a scorer object or a sequence of these. If multiple metrics are selected, only the first is used to optimize the BO. If None, a default metric is selected: \"f1\" for binary classification \"f1_weighted\" for multiclass classification \"r2\" for regression greater_is_better: bool or sequence, default=True Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_proba: bool or sequence, default=False Whether the metric function requires probability estimates out of a classifier. If True, make sure that every selected model has a predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_threshold: bool or sequence, default=False Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. n_calls: int or sequence, default=0 Maximum number of iterations of the BO. It includes the random points of n_initial_points . If 0, skip the BO and fit the model on its default parameters. If sequence, the n-th value applies to the n-th model. n_initial_points: int or sequence, default=5 Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value applies to the n-th model. est_params: dict, default=None Additional parameters for the estimators. See the corresponding documentation for the available options. For multiple models, use the acronyms as key (or 'all' for all models) and a dict of the parameters as value. Add _fit to the parameter's name to pass it to the fit method instead of the initializer. bo_params: dict, default=None Additional parameters to for the BO. These can include: base_estimator: str, default=\"GP\" Base estimator to use in the BO. Choose from: \"GP\" for Gaussian Process \"RF\" for Random Forest \"ET\" for Extra-Trees \"GBRT\" for Gradient Boosted Regression Trees max_time: int, default=np.inf Stop the optimization after max_time seconds. delta_x: int or float, default=0 Stop the optimization when |x1 - x2| < delta_x . delta_y: int or float, default=0) Stop the optimization if the 5 minima are within delta_y (the function is always minimized. cv: int, default=1 Number of folds for the cross-validation. If 1, the training set is randomly split in a subtrain and validation set. early stopping: int, float or None, default=None Training will stop if the model didn't improve in last early_stopping rounds. If < 1, fraction of rounds from the total. If None, no early stopping is performed. Only available for models that allow in-training evaluation. callback: callable or list of callables, default=None Callbacks for the BO. dimensions: dict, list or None, default=None Custom hyperparameter space for the bayesian optimization. Can be a list to share dimensions across models or a dict with the model's name as key (or 'all' for all models). If None, ATOM's predefined dimensions are used. plot: bool, default=False Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Additional keyword arguments for skopt's optimizer. bootstrap: int or sequence, default=0 Number of data sets (bootstrapped from the training set) to use in the bootstrap algorithm. If 0, no bootstrap is performed. If sequence, the n-th value will apply to the n-th model. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs . Beware that using multiple processes on the same machine may cause memory issues for large datasets. gpu: bool or str, default=False Train models on GPU. Refer to the documentation to check which estimators are supported. If False: Always use CPU implementation. If True: Use GPU implementation if possible. If \"force\": Force GPU implementation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of the actions in python's warnings environment. Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go directly from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random .","title":"DirectClassifier"},{"location":"API/training/directclassifier/#magic-methods","text":"The class contains some magic methods to help you access some of its elements faster. __len__: Returns the length of the dataset. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a model, column or subset of the dataset.","title":"Magic methods"},{"location":"API/training/directclassifier/#attributes","text":"","title":"Attributes"},{"location":"API/training/directclassifier/#data-attributes","text":"The dataset can be accessed at any time through multiple attributes, e.g. calling trainer.train will return the training set. Updating one of the data attributes will automatically update the rest as well. Changing the branch will also change the response from these attributes accordingly. Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/training/directclassifier/#utility-attributes","text":"Attributes: models: list Names of the models in the instance. metric: str or list Metric(s) used to fit the models. errors: dict Dictionary of the encountered exceptions (if any). winners: list of str Model names ordered by performance on the test set (either through the metric_test or mean_bootstrap attribute). winner: model Model subclass that performed best on the test set (either through the metric_test or mean_bootstrap attribute). results: pd.DataFrame Dataframe of the training results. Columns can include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/training/directclassifier/#plot-attributes","text":"Attributes: style: str Plotting style. See seaborn's documentation . palette: str Color palette. See seaborn's documentation . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes.","title":"Plot attributes"},{"location":"API/training/directclassifier/#methods","text":"available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. evaluate Get all models' scores for the provided metrics. get_class_weight Return class weights for a balanced dataset. get_params Get parameters for this estimator. log Save information to the logger and print to stdout. merge Merge another instance of the same class into this one. reset_aesthetics Reset the plot aesthetics to their default values. run Fit and evaluate the models. save Save the instance to a pickle file. set_params Set the parameters of this estimator. stacking Train a Stacking model. voting Train a Voting model. method available_models () [source] Give an overview of the available predefined models. Returns: pd.DataFrame Information about the predefined models available for the current task. Columns include: acronym: Model's acronym (used to call the model). fullname: Complete name of the model. estimator: The model's underlying estimator. module: The estimator's module. needs_scaling: Whether the model requires feature scaling. accepts_sparse: Whether the model has native support for sparse matrices. supports_gpu: Whether the model has GPU support. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters: nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. Deleted models are not removed from any active mlflow experiment. Parameters: models: str or sequence, default=None Models to delete. If None, delete them all. method evaluate (metric=None, dataset=\"test\", threshold=0.5) [source] Get all the models' scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. Returns: pd.DataFrame Scores of the models. method get_class_weights (dataset=\"train\") [source] Return class weights for a balanced data set. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters: dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns: dict Classes with the corresponding weights. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters: other: DirectClassifier Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method run (*arrays) [source] Fit and evaluate the models. Parameters: *arrays: sequence of indexables Training and test set (and optionally a holdout set). Allowed formats are: train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Remember that the class contains the complete dataset as attribute, so the file can become large for big datasets! To avoid this, use save_data=False . Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the data as an attribute of the instance. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: DirectClassifier Estimator instance. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters: name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: Stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's StackingClassifier instance. The predefined model's acronyms can be used for the final_estimator parameter. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters: name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: Vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's VotingClassifier instance.","title":"Methods"},{"location":"API/training/directclassifier/#example","text":"from atom.training import DirectClassifier trainer = DirectClassifier ([ \"Tree\" , \"RF\" ], n_calls = 25 , n_initial_points = 10 ) trainer . run ( train , test ) # Analyze the results trainer . plot_prc () print ( trainer . results )","title":"Example"},{"location":"API/training/directregressor/","text":"DirectRegressor class atom.training. DirectRegressor (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, n_jobs=1, gpu=False, verbose=0, warnings=True, logger=None, experiment=None, random_state=None) [source] Fit and evaluate the models. The following steps are applied to every model: Hyperparameter tuning is performed using a Bayesian Optimization approach (optional). The model is fitted on the training set using the best combination of hyperparameters found. The model is evaluated on the test set. The model is trained on various bootstrapped samples of the training set and scored again on the test set (optional). You can predict , plot and call any model from the instance. Read more in the user guide . Parameters: models: str, estimator or sequence, default=None Models to fit to the data. Allowed inputs are: an acronym from any of ATOM's predefined models, an ATOMModel or a custom estimator as class or instance. If None, all the predefined models are used. Available predefined models are: \"GP\" for Gaussian Process \"OLS\" for Ordinary Least Squares \"Ridge\" for Ridge Regression \"Lasso\" for Lasso Regression \"EN\" for ElasticNet \"BR\" for Bayesian Ridge \"ARD\" for Automated Relevance Determination \"KNN\" for K-Nearest Neighbors \"RNN\" for Radius Nearest Neighbors \"Tree\" for a single Decision Tree \"Bag\" for Bagging \"ET\" for Extra-Trees \"RF\" for Random Forest \"AdaB\" for AdaBoost \"GBM\" for Gradient Boosting Machine \"XGB\" for XGBoost (only available if package is installed) \"LGB\" for LightGBM (only available if package is installed) \"CatB\" for CatBoost (only available if package is installed) \"lSVM\" for Linear SVM \"kSVM\" for Kernel SVM \"PA\" for Passive Aggressive \"SGD\" for Stochastic Gradient Descent \"MLP\" for Multi-layer Perceptron metric: str, func, scorer, sequence or None, default=None Metric on which to fit the models. Choose from any of sklearn's SCORERS , a function with signature metric(y_true, y_pred) , a scorer object or a sequence of these. If multiple metrics are selected, only the first is used to optimize the BO. If None, a default metric is selected: \"f1\" for binary classification \"f1_weighted\" for multiclass classification \"r2\" for regression greater_is_better: bool or sequence, default=True Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_proba: bool or sequence, default=False Whether the metric function requires probability estimates out of a classifier. If True, make sure that every selected model has a predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_threshold: bool or sequence, default=False Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. n_calls: int or sequence, default=0 Maximum number of iterations of the BO. It includes the random points of n_initial_points . If 0, skip the BO and fit the model on its default parameters. If sequence, the n-th value applies to the n-th model. n_initial_points: int or sequence, default=5 Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value applies to the n-th model. est_params: dict, default=None Additional parameters for the estimators. See the corresponding documentation for the available options. For multiple models, use the acronyms as key (or 'all' for all models) and a dict of the parameters as value. Add _fit to the parameter's name to pass it to the fit method instead of the initializer. bo_params: dict, default=None Additional parameters to for the BO. These can include: base_estimator: str, default=\"GP\" Base estimator to use in the BO. Choose from: \"GP\" for Gaussian Process \"RF\" for Random Forest \"ET\" for Extra-Trees \"GBRT\" for Gradient Boosted Regression Trees max_time: int, default=np.inf Stop the optimization after max_time seconds. delta_x: int or float, default=0 Stop the optimization when |x1 - x2| < delta_x . delta_y: int or float, default=0) Stop the optimization if the 5 minima are within delta_y (the function is always minimized. cv: int, default=1 Number of folds for the cross-validation. If 1, the training set is randomly split in a subtrain and validation set. early stopping: int, float or None, default=None Training will stop if the model didn't improve in last early_stopping rounds. If < 1, fraction of rounds from the total. If None, no early stopping is performed. Only available for models that allow in-training evaluation. callback: callable or list of callables, default=None Callbacks for the BO. dimensions: dict, list or None, default=None Custom hyperparameter space for the bayesian optimization. Can be a list to share dimensions across models or a dict with the model's name as key (or 'all' for all models). If None, ATOM's predefined dimensions are used. plot: bool, default=False Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Additional keyword arguments for skopt's optimizer. bootstrap: int or sequence, default=0 Number of data sets (bootstrapped from the training set) to use in the bootstrap algorithm. If 0, no bootstrap is performed. If sequence, the n-th value will apply to the n-th model. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs . Beware that using multiple processes on the same machine may cause memory issues for large datasets. gpu: bool or str, default=False Train models on GPU. Refer to the documentation to check which estimators are supported. If False: Always use CPU implementation. If True: Use GPU implementation if possible. If \"force\": Force GPU implementation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of the actions in python's warnings environment. Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go directly from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random . Magic methods The class contains some magic methods to help you access some of its elements faster. __len__: Returns the length of the dataset. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a model, column or subset of the dataset. Attributes Data attributes The dataset can be accessed at any time through multiple attributes, e.g. calling trainer.train will return the training set. Updating one of the data attributes will automatically update the rest as well. Changing the branch will also change the response from these attributes accordingly. Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: models: list Names of the models in the instance. metric: str or list Metric(s) used to fit the models. errors: dict Dictionary of the encountered exceptions (if any). winners: list of str Model names ordered by performance on the test set (either through the metric_test or mean_bootstrap attribute). winner: model Model subclass that performed best on the test set (either through the metric_test or mean_bootstrap attribute). results: pd.DataFrame Dataframe of the training results. Columns can include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Plot attributes Attributes: style: str Plotting style. See seaborn's documentation . palette: str Color palette. See seaborn's documentation . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes. Methods available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. evaluate Get all models' scores for the provided metrics. get_params Get parameters for this estimator. log Save information to the logger and print to stdout. merge Merge another instance of the same class into this one. reset_aesthetics Reset the plot aesthetics to their default values. run Fit and evaluate the models. save Save the instance to a pickle file. set_params Set the parameters of this estimator. stacking Train a Stacking model. voting Train a Voting model. method available_models () [source] Give an overview of the available predefined models. Returns: pd.DataFrame Information about the predefined models available for the current task. Columns include: acronym: Model's acronym (used to call the model). fullname: Complete name of the model. estimator: The model's underlying estimator. module: The estimator's module. needs_scaling: Whether the model requires feature scaling. accepts_sparse: Whether the model has native support for sparse matrices. supports_gpu: Whether the model has GPU support. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters: nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. Deleted models are not removed from any active mlflow experiment. Parameters: models: str or sequence, default=None Models to delete. If None, delete them all. method evaluate (metric=None, dataset=\"test\") [source] Get all the models' scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". Returns: pd.DataFrame Scores of the models. method get_class_weights (dataset=\"train\") [source] Return class weights for a balanced data set. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters: dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns: dict Classes with the corresponding weights. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters: other: DirectRegressor Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method run (*arrays) [source] Fit and evaluate the models. Parameters: *arrays: sequence of indexables Training and test set (and optionally a holdout set). Allowed formats are: train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Remember that the class contains the complete dataset as attribute, so the file can become large for big datasets! To avoid this, use save_data=False . Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the data as an attribute of the instance. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: DirectRegressor Estimator instance. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters: name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: Stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's StackingRegressor instance. The predefined model's acronyms can be used for the final_estimator parameter. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters: name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: Vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's VotingRegressor instance. Example from atom.training import DirectRegressor trainer = DirectRegressor ([ \"OLS\" , \"BR\" ], n_calls = 5 , n_initial_points = 3 ) trainer . run ( train , test ) # Analyze the results trainer . plot_residuals () print ( trainer . results )","title":"DirectRegressor"},{"location":"API/training/directregressor/#directregressor","text":"class atom.training. DirectRegressor (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, n_jobs=1, gpu=False, verbose=0, warnings=True, logger=None, experiment=None, random_state=None) [source] Fit and evaluate the models. The following steps are applied to every model: Hyperparameter tuning is performed using a Bayesian Optimization approach (optional). The model is fitted on the training set using the best combination of hyperparameters found. The model is evaluated on the test set. The model is trained on various bootstrapped samples of the training set and scored again on the test set (optional). You can predict , plot and call any model from the instance. Read more in the user guide . Parameters: models: str, estimator or sequence, default=None Models to fit to the data. Allowed inputs are: an acronym from any of ATOM's predefined models, an ATOMModel or a custom estimator as class or instance. If None, all the predefined models are used. Available predefined models are: \"GP\" for Gaussian Process \"OLS\" for Ordinary Least Squares \"Ridge\" for Ridge Regression \"Lasso\" for Lasso Regression \"EN\" for ElasticNet \"BR\" for Bayesian Ridge \"ARD\" for Automated Relevance Determination \"KNN\" for K-Nearest Neighbors \"RNN\" for Radius Nearest Neighbors \"Tree\" for a single Decision Tree \"Bag\" for Bagging \"ET\" for Extra-Trees \"RF\" for Random Forest \"AdaB\" for AdaBoost \"GBM\" for Gradient Boosting Machine \"XGB\" for XGBoost (only available if package is installed) \"LGB\" for LightGBM (only available if package is installed) \"CatB\" for CatBoost (only available if package is installed) \"lSVM\" for Linear SVM \"kSVM\" for Kernel SVM \"PA\" for Passive Aggressive \"SGD\" for Stochastic Gradient Descent \"MLP\" for Multi-layer Perceptron metric: str, func, scorer, sequence or None, default=None Metric on which to fit the models. Choose from any of sklearn's SCORERS , a function with signature metric(y_true, y_pred) , a scorer object or a sequence of these. If multiple metrics are selected, only the first is used to optimize the BO. If None, a default metric is selected: \"f1\" for binary classification \"f1_weighted\" for multiclass classification \"r2\" for regression greater_is_better: bool or sequence, default=True Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_proba: bool or sequence, default=False Whether the metric function requires probability estimates out of a classifier. If True, make sure that every selected model has a predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_threshold: bool or sequence, default=False Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. n_calls: int or sequence, default=0 Maximum number of iterations of the BO. It includes the random points of n_initial_points . If 0, skip the BO and fit the model on its default parameters. If sequence, the n-th value applies to the n-th model. n_initial_points: int or sequence, default=5 Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value applies to the n-th model. est_params: dict, default=None Additional parameters for the estimators. See the corresponding documentation for the available options. For multiple models, use the acronyms as key (or 'all' for all models) and a dict of the parameters as value. Add _fit to the parameter's name to pass it to the fit method instead of the initializer. bo_params: dict, default=None Additional parameters to for the BO. These can include: base_estimator: str, default=\"GP\" Base estimator to use in the BO. Choose from: \"GP\" for Gaussian Process \"RF\" for Random Forest \"ET\" for Extra-Trees \"GBRT\" for Gradient Boosted Regression Trees max_time: int, default=np.inf Stop the optimization after max_time seconds. delta_x: int or float, default=0 Stop the optimization when |x1 - x2| < delta_x . delta_y: int or float, default=0) Stop the optimization if the 5 minima are within delta_y (the function is always minimized. cv: int, default=1 Number of folds for the cross-validation. If 1, the training set is randomly split in a subtrain and validation set. early stopping: int, float or None, default=None Training will stop if the model didn't improve in last early_stopping rounds. If < 1, fraction of rounds from the total. If None, no early stopping is performed. Only available for models that allow in-training evaluation. callback: callable or list of callables, default=None Callbacks for the BO. dimensions: dict, list or None, default=None Custom hyperparameter space for the bayesian optimization. Can be a list to share dimensions across models or a dict with the model's name as key (or 'all' for all models). If None, ATOM's predefined dimensions are used. plot: bool, default=False Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Additional keyword arguments for skopt's optimizer. bootstrap: int or sequence, default=0 Number of data sets (bootstrapped from the training set) to use in the bootstrap algorithm. If 0, no bootstrap is performed. If sequence, the n-th value will apply to the n-th model. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs . Beware that using multiple processes on the same machine may cause memory issues for large datasets. gpu: bool or str, default=False Train models on GPU. Refer to the documentation to check which estimators are supported. If False: Always use CPU implementation. If True: Use GPU implementation if possible. If \"force\": Force GPU implementation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of the actions in python's warnings environment. Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go directly from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random .","title":"DirectRegressor"},{"location":"API/training/directregressor/#magic-methods","text":"The class contains some magic methods to help you access some of its elements faster. __len__: Returns the length of the dataset. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a model, column or subset of the dataset.","title":"Magic methods"},{"location":"API/training/directregressor/#attributes","text":"","title":"Attributes"},{"location":"API/training/directregressor/#data-attributes","text":"The dataset can be accessed at any time through multiple attributes, e.g. calling trainer.train will return the training set. Updating one of the data attributes will automatically update the rest as well. Changing the branch will also change the response from these attributes accordingly. Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/training/directregressor/#utility-attributes","text":"Attributes: models: list Names of the models in the instance. metric: str or list Metric(s) used to fit the models. errors: dict Dictionary of the encountered exceptions (if any). winners: list of str Model names ordered by performance on the test set (either through the metric_test or mean_bootstrap attribute). winner: model Model subclass that performed best on the test set (either through the metric_test or mean_bootstrap attribute). results: pd.DataFrame Dataframe of the training results. Columns can include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/training/directregressor/#plot-attributes","text":"Attributes: style: str Plotting style. See seaborn's documentation . palette: str Color palette. See seaborn's documentation . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes.","title":"Plot attributes"},{"location":"API/training/directregressor/#methods","text":"available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. evaluate Get all models' scores for the provided metrics. get_params Get parameters for this estimator. log Save information to the logger and print to stdout. merge Merge another instance of the same class into this one. reset_aesthetics Reset the plot aesthetics to their default values. run Fit and evaluate the models. save Save the instance to a pickle file. set_params Set the parameters of this estimator. stacking Train a Stacking model. voting Train a Voting model. method available_models () [source] Give an overview of the available predefined models. Returns: pd.DataFrame Information about the predefined models available for the current task. Columns include: acronym: Model's acronym (used to call the model). fullname: Complete name of the model. estimator: The model's underlying estimator. module: The estimator's module. needs_scaling: Whether the model requires feature scaling. accepts_sparse: Whether the model has native support for sparse matrices. supports_gpu: Whether the model has GPU support. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters: nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. Deleted models are not removed from any active mlflow experiment. Parameters: models: str or sequence, default=None Models to delete. If None, delete them all. method evaluate (metric=None, dataset=\"test\") [source] Get all the models' scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". Returns: pd.DataFrame Scores of the models. method get_class_weights (dataset=\"train\") [source] Return class weights for a balanced data set. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters: dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns: dict Classes with the corresponding weights. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters: other: DirectRegressor Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method run (*arrays) [source] Fit and evaluate the models. Parameters: *arrays: sequence of indexables Training and test set (and optionally a holdout set). Allowed formats are: train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Remember that the class contains the complete dataset as attribute, so the file can become large for big datasets! To avoid this, use save_data=False . Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the data as an attribute of the instance. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: DirectRegressor Estimator instance. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters: name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: Stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's StackingRegressor instance. The predefined model's acronyms can be used for the final_estimator parameter. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters: name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: Vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's VotingRegressor instance.","title":"Methods"},{"location":"API/training/directregressor/#example","text":"from atom.training import DirectRegressor trainer = DirectRegressor ([ \"OLS\" , \"BR\" ], n_calls = 5 , n_initial_points = 3 ) trainer . run ( train , test ) # Analyze the results trainer . plot_residuals () print ( trainer . results )","title":"Example"},{"location":"API/training/successivehalvingclassifier/","text":"SuccessiveHalvingClassifier class atom.training. SuccessiveHalvingClassifier (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_runs=0, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, n_jobs=1, gpu=False, verbose=0, warnings=True, logger=None, experiment=None, random_state=None) [source] Fit and evaluate the models in a successive halving fashion. The following steps are applied to every model (per iteration): Hyperparameter tuning is performed using a Bayesian Optimization approach (optional). The model is fitted on the training set using the best combination of hyperparameters found. The model is evaluated on the test set. The model is trained on various bootstrapped samples of the training set and scored again on the test set (optional). You can predict , plot and call any model from the instance. Read more in the user guide . Parameters: models: str, estimator or sequence, default=None Models to fit to the data. Allowed inputs are: an acronym from any of ATOM's predefined models, an ATOMModel or a custom estimator as class or instance. If None, all the predefined models are used. Available predefined models are: \"GP\" for Gaussian Process \"GNB\" for Gaussian Naive Bayes \"MNB\" for Multinomial Naive Bayes \"BNB\" for Bernoulli Naive Bayes \"CatNB\" for Categorical Naive Bayes \"CNB\" for Complement Naive Bayes \"Ridge\" for Ridge Classification \"LR\" for Logistic Regression \"LDA\" for Linear Discriminant Analysis \"QDA\" for Quadratic Discriminant Analysis \"KNN\" for K-Nearest Neighbors \"RNN\" for Radius Nearest Neighbors \"Tree\" for a single Decision Tree \"Bag\" for Bagging \"ET\" for Extra-Trees \"RF\" for Random Forest \"AdaB\" for AdaBoost \"GBM\" for Gradient Boosting Machine \"XGB\" for XGBoost (only available if package is installed) \"LGB\" for LightGBM (only available if package is installed) \"CatB\" for CatBoost (only available if package is installed) \"lSVM\" for Linear SVM \"kSVM\" for Kernel SVM \"PA\" for Passive Aggressive \"SGD\" for Stochastic Gradient Descent \"MLP\" for Multi-layer Perceptron metric: str, func, scorer, sequence or None, default=None Metric on which to fit the models. Choose from any of sklearn's SCORERS , a function with signature metric(y_true, y_pred) , a scorer object or a sequence of these. If multiple metrics are selected, only the first is used to optimize the BO. If None, a default metric is selected: \"f1\" for binary classification \"f1_weighted\" for multiclass classification \"r2\" for regression greater_is_better: bool or sequence, default=True Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_proba: bool or sequence, default=False Whether the metric function requires probability estimates out of a classifier. If True, make sure that every selected model has a predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_threshold: bool or sequence, default=False Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. skip_runs: int, default=0 Skip last skip_runs runs of the successive halving. n_calls: int or sequence, default=0 Maximum number of iterations of the BO. It includes the random points of n_initial_points . If 0, skip the BO and fit the model on its default parameters. If sequence, the n-th value applies to the n-th model. n_initial_points: int or sequence, default=5 Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value applies to the n-th model. est_params: dict, default=None Additional parameters for the estimators. See the corresponding documentation for the available options. For multiple models, use the acronyms as key (or 'all' for all models) and a dict of the parameters as value. Add _fit to the parameter's name to pass it to the fit method instead of the initializer. bo_params: dict, default=None Additional parameters to for the BO. These can include: base_estimator: str, default=\"GP\" Base estimator to use in the BO. Choose from: \"GP\" for Gaussian Process \"RF\" for Random Forest \"ET\" for Extra-Trees \"GBRT\" for Gradient Boosted Regression Trees max_time: int, default=np.inf Stop the optimization after max_time seconds. delta_x: int or float, default=0 Stop the optimization when |x1 - x2| < delta_x . delta_y: int or float, default=0) Stop the optimization if the 5 minima are within delta_y (the function is always minimized. cv: int, default=1 Number of folds for the cross-validation. If 1, the training set is randomly split in a subtrain and validation set. early stopping: int, float or None, default=None Training will stop if the model didn't improve in last early_stopping rounds. If < 1, fraction of rounds from the total. If None, no early stopping is performed. Only available for models that allow in-training evaluation. callback: callable or list of callables, default=None Callbacks for the BO. dimensions: dict, list or None, default=None Custom hyperparameter space for the bayesian optimization. Can be a list to share dimensions across models or a dict with the model's name as key (or 'all' for all models). If None, ATOM's predefined dimensions are used. plot: bool, default=False Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Additional keyword arguments for skopt's optimizer. bootstrap: int or sequence, default=0 Number of data sets (bootstrapped from the training set) to use in the bootstrap algorithm. If 0, no bootstrap is performed. If sequence, the n-th value will apply to the n-th model. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs . Beware that using multiple processes on the same machine may cause memory issues for large datasets. gpu: bool or str, default=False Train models on GPU. Refer to the documentation to check which estimators are supported. If False: Always use CPU implementation. If True: Use GPU implementation if possible. If \"force\": Force GPU implementation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of the actions in python's warnings environment. Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go directly from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random . Magic methods The class contains some magic methods to help you access some of its elements faster. __len__: Returns the length of the dataset. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a model, column or subset of the dataset. Attributes Data attributes The dataset can be accessed at any time through multiple attributes, e.g. calling trainer.train will return the training set. Updating one of the data attributes will automatically update the rest as well. Changing the branch will also change the response from these attributes accordingly. Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: models: list Names of the models in the instance. metric: str or list Metric(s) used to fit the models. errors: dict Dictionary of the encountered exceptions (if any). winners: list of str Model names ordered by performance on the test set (either through the metric_test or mean_bootstrap attribute). winner: model Model subclass that performed best on the test set (either through the metric_test or mean_bootstrap attribute). results: pd.DataFrame Dataframe of the training results. Columns can include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Plot attributes Attributes: style: str Plotting style. See seaborn's documentation . palette: str Color palette. See seaborn's documentation . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes. Methods available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. evaluate Get all models' scores for the provided metrics. get_class_weight Return class weights for a balanced dataset. get_params Get parameters for this estimator. log Save information to the logger and print to stdout. merge Merge another instance of the same class into this one. reset_aesthetics Reset the plot aesthetics to their default values. run Fit and evaluate the models. save Save the instance to a pickle file. set_params Set the parameters of this estimator. stacking Train a Stacking model. voting Train a Voting model. method available_models () [source] Give an overview of the available predefined models. Returns: pd.DataFrame Information about the predefined models available for the current task. Columns include: acronym: Model's acronym (used to call the model). fullname: Complete name of the model. estimator: The model's underlying estimator. module: The estimator's module. needs_scaling: Whether the model requires feature scaling. accepts_sparse: Whether the model has native support for sparse matrices. supports_gpu: Whether the model has GPU support. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters: nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. Deleted models are not removed from any active mlflow experiment. Parameters: models: str or sequence, default=None Models to delete. If None, delete them all. method evaluate (metric=None, dataset=\"test\", threshold=0.5) [source] Get all the models' scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. Returns: pd.DataFrame Scores of the models. method get_class_weights (dataset=\"train\") [source] Return class weights for a balanced data set. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters: dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns: dict Classes with the corresponding weights. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters: other: SuccessiveHalvingClassifier Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method run (*arrays) [source] Fit and evaluate the models. Parameters: *arrays: sequence of indexables Training and test set (and optionally a holdout set). Allowed formats are: train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Remember that the class contains the complete dataset as attribute, so the file can become large for big datasets! To avoid this, use save_data=False . Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the data as an attribute of the instance. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: SuccessiveHalvingClassifier Estimator instance. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters: name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: Stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's StackingClassifier instance. The predefined model's acronyms can be used for the final_estimator parameter. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters: name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: Vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's VotingClassifier instance. Example from atom.training import SuccessiveHalvingClassifier # Run the pipeline trainer = SuccessiveHalvingClassifier ([ \"Tree\" , \"Bag\" , \"RF\" , \"ET\" ], metric = \"f1\" ) trainer . run ( train , test ) # Analyze the results trainer . plot_successive_halving () print ( trainer . results )","title":"SuccessiveHalvingClassifier"},{"location":"API/training/successivehalvingclassifier/#successivehalvingclassifier","text":"class atom.training. SuccessiveHalvingClassifier (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_runs=0, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, n_jobs=1, gpu=False, verbose=0, warnings=True, logger=None, experiment=None, random_state=None) [source] Fit and evaluate the models in a successive halving fashion. The following steps are applied to every model (per iteration): Hyperparameter tuning is performed using a Bayesian Optimization approach (optional). The model is fitted on the training set using the best combination of hyperparameters found. The model is evaluated on the test set. The model is trained on various bootstrapped samples of the training set and scored again on the test set (optional). You can predict , plot and call any model from the instance. Read more in the user guide . Parameters: models: str, estimator or sequence, default=None Models to fit to the data. Allowed inputs are: an acronym from any of ATOM's predefined models, an ATOMModel or a custom estimator as class or instance. If None, all the predefined models are used. Available predefined models are: \"GP\" for Gaussian Process \"GNB\" for Gaussian Naive Bayes \"MNB\" for Multinomial Naive Bayes \"BNB\" for Bernoulli Naive Bayes \"CatNB\" for Categorical Naive Bayes \"CNB\" for Complement Naive Bayes \"Ridge\" for Ridge Classification \"LR\" for Logistic Regression \"LDA\" for Linear Discriminant Analysis \"QDA\" for Quadratic Discriminant Analysis \"KNN\" for K-Nearest Neighbors \"RNN\" for Radius Nearest Neighbors \"Tree\" for a single Decision Tree \"Bag\" for Bagging \"ET\" for Extra-Trees \"RF\" for Random Forest \"AdaB\" for AdaBoost \"GBM\" for Gradient Boosting Machine \"XGB\" for XGBoost (only available if package is installed) \"LGB\" for LightGBM (only available if package is installed) \"CatB\" for CatBoost (only available if package is installed) \"lSVM\" for Linear SVM \"kSVM\" for Kernel SVM \"PA\" for Passive Aggressive \"SGD\" for Stochastic Gradient Descent \"MLP\" for Multi-layer Perceptron metric: str, func, scorer, sequence or None, default=None Metric on which to fit the models. Choose from any of sklearn's SCORERS , a function with signature metric(y_true, y_pred) , a scorer object or a sequence of these. If multiple metrics are selected, only the first is used to optimize the BO. If None, a default metric is selected: \"f1\" for binary classification \"f1_weighted\" for multiclass classification \"r2\" for regression greater_is_better: bool or sequence, default=True Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_proba: bool or sequence, default=False Whether the metric function requires probability estimates out of a classifier. If True, make sure that every selected model has a predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_threshold: bool or sequence, default=False Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. skip_runs: int, default=0 Skip last skip_runs runs of the successive halving. n_calls: int or sequence, default=0 Maximum number of iterations of the BO. It includes the random points of n_initial_points . If 0, skip the BO and fit the model on its default parameters. If sequence, the n-th value applies to the n-th model. n_initial_points: int or sequence, default=5 Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value applies to the n-th model. est_params: dict, default=None Additional parameters for the estimators. See the corresponding documentation for the available options. For multiple models, use the acronyms as key (or 'all' for all models) and a dict of the parameters as value. Add _fit to the parameter's name to pass it to the fit method instead of the initializer. bo_params: dict, default=None Additional parameters to for the BO. These can include: base_estimator: str, default=\"GP\" Base estimator to use in the BO. Choose from: \"GP\" for Gaussian Process \"RF\" for Random Forest \"ET\" for Extra-Trees \"GBRT\" for Gradient Boosted Regression Trees max_time: int, default=np.inf Stop the optimization after max_time seconds. delta_x: int or float, default=0 Stop the optimization when |x1 - x2| < delta_x . delta_y: int or float, default=0) Stop the optimization if the 5 minima are within delta_y (the function is always minimized. cv: int, default=1 Number of folds for the cross-validation. If 1, the training set is randomly split in a subtrain and validation set. early stopping: int, float or None, default=None Training will stop if the model didn't improve in last early_stopping rounds. If < 1, fraction of rounds from the total. If None, no early stopping is performed. Only available for models that allow in-training evaluation. callback: callable or list of callables, default=None Callbacks for the BO. dimensions: dict, list or None, default=None Custom hyperparameter space for the bayesian optimization. Can be a list to share dimensions across models or a dict with the model's name as key (or 'all' for all models). If None, ATOM's predefined dimensions are used. plot: bool, default=False Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Additional keyword arguments for skopt's optimizer. bootstrap: int or sequence, default=0 Number of data sets (bootstrapped from the training set) to use in the bootstrap algorithm. If 0, no bootstrap is performed. If sequence, the n-th value will apply to the n-th model. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs . Beware that using multiple processes on the same machine may cause memory issues for large datasets. gpu: bool or str, default=False Train models on GPU. Refer to the documentation to check which estimators are supported. If False: Always use CPU implementation. If True: Use GPU implementation if possible. If \"force\": Force GPU implementation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of the actions in python's warnings environment. Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go directly from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random .","title":"SuccessiveHalvingClassifier"},{"location":"API/training/successivehalvingclassifier/#magic-methods","text":"The class contains some magic methods to help you access some of its elements faster. __len__: Returns the length of the dataset. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a model, column or subset of the dataset.","title":"Magic methods"},{"location":"API/training/successivehalvingclassifier/#attributes","text":"","title":"Attributes"},{"location":"API/training/successivehalvingclassifier/#data-attributes","text":"The dataset can be accessed at any time through multiple attributes, e.g. calling trainer.train will return the training set. Updating one of the data attributes will automatically update the rest as well. Changing the branch will also change the response from these attributes accordingly. Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/training/successivehalvingclassifier/#utility-attributes","text":"Attributes: models: list Names of the models in the instance. metric: str or list Metric(s) used to fit the models. errors: dict Dictionary of the encountered exceptions (if any). winners: list of str Model names ordered by performance on the test set (either through the metric_test or mean_bootstrap attribute). winner: model Model subclass that performed best on the test set (either through the metric_test or mean_bootstrap attribute). results: pd.DataFrame Dataframe of the training results. Columns can include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/training/successivehalvingclassifier/#plot-attributes","text":"Attributes: style: str Plotting style. See seaborn's documentation . palette: str Color palette. See seaborn's documentation . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes.","title":"Plot attributes"},{"location":"API/training/successivehalvingclassifier/#methods","text":"available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. evaluate Get all models' scores for the provided metrics. get_class_weight Return class weights for a balanced dataset. get_params Get parameters for this estimator. log Save information to the logger and print to stdout. merge Merge another instance of the same class into this one. reset_aesthetics Reset the plot aesthetics to their default values. run Fit and evaluate the models. save Save the instance to a pickle file. set_params Set the parameters of this estimator. stacking Train a Stacking model. voting Train a Voting model. method available_models () [source] Give an overview of the available predefined models. Returns: pd.DataFrame Information about the predefined models available for the current task. Columns include: acronym: Model's acronym (used to call the model). fullname: Complete name of the model. estimator: The model's underlying estimator. module: The estimator's module. needs_scaling: Whether the model requires feature scaling. accepts_sparse: Whether the model has native support for sparse matrices. supports_gpu: Whether the model has GPU support. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters: nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. Deleted models are not removed from any active mlflow experiment. Parameters: models: str or sequence, default=None Models to delete. If None, delete them all. method evaluate (metric=None, dataset=\"test\", threshold=0.5) [source] Get all the models' scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. Returns: pd.DataFrame Scores of the models. method get_class_weights (dataset=\"train\") [source] Return class weights for a balanced data set. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters: dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns: dict Classes with the corresponding weights. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters: other: SuccessiveHalvingClassifier Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method run (*arrays) [source] Fit and evaluate the models. Parameters: *arrays: sequence of indexables Training and test set (and optionally a holdout set). Allowed formats are: train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Remember that the class contains the complete dataset as attribute, so the file can become large for big datasets! To avoid this, use save_data=False . Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the data as an attribute of the instance. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: SuccessiveHalvingClassifier Estimator instance. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters: name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: Stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's StackingClassifier instance. The predefined model's acronyms can be used for the final_estimator parameter. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters: name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: Vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's VotingClassifier instance.","title":"Methods"},{"location":"API/training/successivehalvingclassifier/#example","text":"from atom.training import SuccessiveHalvingClassifier # Run the pipeline trainer = SuccessiveHalvingClassifier ([ \"Tree\" , \"Bag\" , \"RF\" , \"ET\" ], metric = \"f1\" ) trainer . run ( train , test ) # Analyze the results trainer . plot_successive_halving () print ( trainer . results )","title":"Example"},{"location":"API/training/successivehalvingregressor/","text":"SuccessiveHalvingRegressor class atom.training. SuccessiveHalvingRegressor (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_runs=0, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, n_jobs=1, gpu=False, verbose=0, warnings=True, logger=None, experiment=None, random_state=None) [source] Fit and evaluate the models in a successive halving fashion. The pipeline applies the following steps per iteration: The optimal hyperparameters for the model are selected using a bayesian optimization algorithm (optional). The model is fitted on the training set using the best combination of hyperparameters found. After that, the model is evaluated on the tes set. Calculate various scores on the test set using a bootstrap algorithm (optional). You can predict , plot and call any model from the instance. Read more in the user guide . Parameters: models: str, estimator or sequence, default=None Models to fit to the data. Allowed inputs are: an acronym from any of ATOM's predefined models, an ATOMModel or a custom estimator as class or instance. If None, all the predefined models are used. Available predefined models are: \"GP\" for Gaussian Process \"OLS\" for Ordinary Least Squares \"Ridge\" for Ridge Regression \"Lasso\" for Lasso Regression \"EN\" for ElasticNet \"BR\" for Bayesian Ridge \"ARD\" for Automated Relevance Determination \"KNN\" for K-Nearest Neighbors \"RNN\" for Radius Nearest Neighbors \"Tree\" for a single Decision Tree \"Bag\" for Bagging \"ET\" for Extra-Trees \"RF\" for Random Forest \"AdaB\" for AdaBoost \"GBM\" for Gradient Boosting Machine \"XGB\" for XGBoost (only available if package is installed) \"LGB\" for LightGBM (only available if package is installed) \"CatB\" for CatBoost (only available if package is installed) \"lSVM\" for Linear SVM \"kSVM\" for Kernel SVM \"PA\" for Passive Aggressive \"SGD\" for Stochastic Gradient Descent \"MLP\" for Multi-layer Perceptron metric: str, func, scorer, sequence or None, default=None Metric on which to fit the models. Choose from any of sklearn's SCORERS , a function with signature metric(y_true, y_pred) , a scorer object or a sequence of these. If multiple metrics are selected, only the first is used to optimize the BO. If None, a default metric is selected: \"f1\" for binary classification \"f1_weighted\" for multiclass classification \"r2\" for regression greater_is_better: bool or sequence, default=True Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_proba: bool or sequence, default=False Whether the metric function requires probability estimates out of a classifier. If True, make sure that every selected model has a predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_threshold: bool or sequence, default=False Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. skip_runs: int, default=0 Skip last skip_runs runs of the successive halving. n_calls: int or sequence, default=0 Maximum number of iterations of the BO. It includes the random points of n_initial_points . If 0, skip the BO and fit the model on its default parameters. If sequence, the n-th value applies to the n-th model. n_initial_points: int or sequence, default=5 Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value applies to the n-th model. est_params: dict, default=None Additional parameters for the estimators. See the corresponding documentation for the available options. For multiple models, use the acronyms as key (or 'all' for all models) and a dict of the parameters as value. Add _fit to the parameter's name to pass it to the fit method instead of the initializer. bo_params: dict, default=None Additional parameters to for the BO. These can include: base_estimator: str, default=\"GP\" Base estimator to use in the BO. Choose from: \"GP\" for Gaussian Process \"RF\" for Random Forest \"ET\" for Extra-Trees \"GBRT\" for Gradient Boosted Regression Trees max_time: int, default=np.inf Stop the optimization after max_time seconds. delta_x: int or float, default=0 Stop the optimization when |x1 - x2| < delta_x . delta_y: int or float, default=0) Stop the optimization if the 5 minima are within delta_y (the function is always minimized. cv: int, default=1 Number of folds for the cross-validation. If 1, the training set is randomly split in a subtrain and validation set. early stopping: int, float or None, default=None Training will stop if the model didn't improve in last early_stopping rounds. If < 1, fraction of rounds from the total. If None, no early stopping is performed. Only available for models that allow in-training evaluation. callback: callable or list of callables, default=None Callbacks for the BO. dimensions: dict, list or None, default=None Custom hyperparameter space for the bayesian optimization. Can be a list to share dimensions across models or a dict with the model's name as key (or 'all' for all models). If None, ATOM's predefined dimensions are used. plot: bool, default=False Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Additional keyword arguments for skopt's optimizer. bootstrap: int or sequence, default=0 Number of data sets (bootstrapped from the training set) to use in the bootstrap algorithm. If 0, no bootstrap is performed. If sequence, the n-th value will apply to the n-th model. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs . Beware that using multiple processes on the same machine may cause memory issues for large datasets. gpu: bool or str, default=False Train models on GPU. Refer to the documentation to check which estimators are supported. If False: Always use CPU implementation. If True: Use GPU implementation if possible. If \"force\": Force GPU implementation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of the actions in python's warnings environment. Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go directly from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random . Magic methods The class contains some magic methods to help you access some of its elements faster. __len__: Returns the length of the dataset. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a model, column or subset of the dataset. Attributes Data attributes The dataset can be accessed at any time through multiple attributes, e.g. calling trainer.train will return the training set. Updating one of the data attributes will automatically update the rest as well. Changing the branch will also change the response from these attributes accordingly. Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: models: list Names of the models in the instance. metric: str or list Metric(s) used to fit the models. errors: dict Dictionary of the encountered exceptions (if any). winners: list of str Model names ordered by performance on the test set (either through the metric_test or mean_bootstrap attribute). winner: model Model subclass that performed best on the test set (either through the metric_test or mean_bootstrap attribute). results: pd.DataFrame Dataframe of the training results. Columns can include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Plot attributes Attributes: style: str Plotting style. See seaborn's documentation . palette: str Color palette. See seaborn's documentation . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes. Methods available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. evaluate Get all models' scores for the provided metrics. get_params Get parameters for this estimator. log Save information to the logger and print to stdout. merge Merge another instance of the same class into this one. reset_aesthetics Reset the plot aesthetics to their default values. run Fit and evaluate the models. save Save the instance to a pickle file. set_params Set the parameters of this estimator. stacking Train a Stacking model. voting Train a Voting model. method available_models () [source] Give an overview of the available predefined models. Returns: pd.DataFrame Information about the predefined models available for the current task. Columns include: acronym: Model's acronym (used to call the model). fullname: Complete name of the model. estimator: The model's underlying estimator. module: The estimator's module. needs_scaling: Whether the model requires feature scaling. accepts_sparse: Whether the model has native support for sparse matrices. supports_gpu: Whether the model has GPU support. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters: nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. Deleted models are not removed from any active mlflow experiment. Parameters: models: str or sequence, default=None Models to delete. If None, delete them all. method evaluate (metric=None, dataset=\"test\") [source] Get all the models' scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". Returns: pd.DataFrame Scores of the models. method get_class_weights (dataset=\"train\") [source] Return class weights for a balanced data set. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters: dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns: dict Classes with the corresponding weights. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters: other: SuccessiveHalvingRegressor Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method run (*arrays) [source] Fit and evaluate the models. Parameters: *arrays: sequence of indexables Training and test set (and optionally a holdout set). Allowed formats are: train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Remember that the class contains the complete dataset as attribute, so the file can become large for big datasets! To avoid this, use save_data=False . Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the data as an attribute of the instance. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: SuccessiveHalvingRegressor Estimator instance. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters: name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: Stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's StackingRegressor instance. The predefined model's acronyms can be used for the final_estimator parameter. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters: name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: Vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's VotingRegressor instance. Example from atom.training import SuccessiveHalvingRegressor # Run the pipeline trainer = SuccessiveHalvingRegressor ([ \"Tree\" , \"Bag\" , \"RF\" , \"ET\" ], metric = \"f1\" ) trainer . run ( train , test ) # Analyze the results trainer . plot_successive_halving () print ( trainer . results )","title":"SuccessiveHalvingRegressor"},{"location":"API/training/successivehalvingregressor/#successivehalvingregressor","text":"class atom.training. SuccessiveHalvingRegressor (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_runs=0, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, n_jobs=1, gpu=False, verbose=0, warnings=True, logger=None, experiment=None, random_state=None) [source] Fit and evaluate the models in a successive halving fashion. The pipeline applies the following steps per iteration: The optimal hyperparameters for the model are selected using a bayesian optimization algorithm (optional). The model is fitted on the training set using the best combination of hyperparameters found. After that, the model is evaluated on the tes set. Calculate various scores on the test set using a bootstrap algorithm (optional). You can predict , plot and call any model from the instance. Read more in the user guide . Parameters: models: str, estimator or sequence, default=None Models to fit to the data. Allowed inputs are: an acronym from any of ATOM's predefined models, an ATOMModel or a custom estimator as class or instance. If None, all the predefined models are used. Available predefined models are: \"GP\" for Gaussian Process \"OLS\" for Ordinary Least Squares \"Ridge\" for Ridge Regression \"Lasso\" for Lasso Regression \"EN\" for ElasticNet \"BR\" for Bayesian Ridge \"ARD\" for Automated Relevance Determination \"KNN\" for K-Nearest Neighbors \"RNN\" for Radius Nearest Neighbors \"Tree\" for a single Decision Tree \"Bag\" for Bagging \"ET\" for Extra-Trees \"RF\" for Random Forest \"AdaB\" for AdaBoost \"GBM\" for Gradient Boosting Machine \"XGB\" for XGBoost (only available if package is installed) \"LGB\" for LightGBM (only available if package is installed) \"CatB\" for CatBoost (only available if package is installed) \"lSVM\" for Linear SVM \"kSVM\" for Kernel SVM \"PA\" for Passive Aggressive \"SGD\" for Stochastic Gradient Descent \"MLP\" for Multi-layer Perceptron metric: str, func, scorer, sequence or None, default=None Metric on which to fit the models. Choose from any of sklearn's SCORERS , a function with signature metric(y_true, y_pred) , a scorer object or a sequence of these. If multiple metrics are selected, only the first is used to optimize the BO. If None, a default metric is selected: \"f1\" for binary classification \"f1_weighted\" for multiclass classification \"r2\" for regression greater_is_better: bool or sequence, default=True Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_proba: bool or sequence, default=False Whether the metric function requires probability estimates out of a classifier. If True, make sure that every selected model has a predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_threshold: bool or sequence, default=False Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. skip_runs: int, default=0 Skip last skip_runs runs of the successive halving. n_calls: int or sequence, default=0 Maximum number of iterations of the BO. It includes the random points of n_initial_points . If 0, skip the BO and fit the model on its default parameters. If sequence, the n-th value applies to the n-th model. n_initial_points: int or sequence, default=5 Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value applies to the n-th model. est_params: dict, default=None Additional parameters for the estimators. See the corresponding documentation for the available options. For multiple models, use the acronyms as key (or 'all' for all models) and a dict of the parameters as value. Add _fit to the parameter's name to pass it to the fit method instead of the initializer. bo_params: dict, default=None Additional parameters to for the BO. These can include: base_estimator: str, default=\"GP\" Base estimator to use in the BO. Choose from: \"GP\" for Gaussian Process \"RF\" for Random Forest \"ET\" for Extra-Trees \"GBRT\" for Gradient Boosted Regression Trees max_time: int, default=np.inf Stop the optimization after max_time seconds. delta_x: int or float, default=0 Stop the optimization when |x1 - x2| < delta_x . delta_y: int or float, default=0) Stop the optimization if the 5 minima are within delta_y (the function is always minimized. cv: int, default=1 Number of folds for the cross-validation. If 1, the training set is randomly split in a subtrain and validation set. early stopping: int, float or None, default=None Training will stop if the model didn't improve in last early_stopping rounds. If < 1, fraction of rounds from the total. If None, no early stopping is performed. Only available for models that allow in-training evaluation. callback: callable or list of callables, default=None Callbacks for the BO. dimensions: dict, list or None, default=None Custom hyperparameter space for the bayesian optimization. Can be a list to share dimensions across models or a dict with the model's name as key (or 'all' for all models). If None, ATOM's predefined dimensions are used. plot: bool, default=False Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Additional keyword arguments for skopt's optimizer. bootstrap: int or sequence, default=0 Number of data sets (bootstrapped from the training set) to use in the bootstrap algorithm. If 0, no bootstrap is performed. If sequence, the n-th value will apply to the n-th model. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs . Beware that using multiple processes on the same machine may cause memory issues for large datasets. gpu: bool or str, default=False Train models on GPU. Refer to the documentation to check which estimators are supported. If False: Always use CPU implementation. If True: Use GPU implementation if possible. If \"force\": Force GPU implementation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of the actions in python's warnings environment. Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go directly from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random .","title":"SuccessiveHalvingRegressor"},{"location":"API/training/successivehalvingregressor/#magic-methods","text":"The class contains some magic methods to help you access some of its elements faster. __len__: Returns the length of the dataset. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a model, column or subset of the dataset.","title":"Magic methods"},{"location":"API/training/successivehalvingregressor/#attributes","text":"","title":"Attributes"},{"location":"API/training/successivehalvingregressor/#data-attributes","text":"The dataset can be accessed at any time through multiple attributes, e.g. calling trainer.train will return the training set. Updating one of the data attributes will automatically update the rest as well. Changing the branch will also change the response from these attributes accordingly. Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/training/successivehalvingregressor/#utility-attributes","text":"Attributes: models: list Names of the models in the instance. metric: str or list Metric(s) used to fit the models. errors: dict Dictionary of the encountered exceptions (if any). winners: list of str Model names ordered by performance on the test set (either through the metric_test or mean_bootstrap attribute). winner: model Model subclass that performed best on the test set (either through the metric_test or mean_bootstrap attribute). results: pd.DataFrame Dataframe of the training results. Columns can include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/training/successivehalvingregressor/#plot-attributes","text":"Attributes: style: str Plotting style. See seaborn's documentation . palette: str Color palette. See seaborn's documentation . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes.","title":"Plot attributes"},{"location":"API/training/successivehalvingregressor/#methods","text":"available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. evaluate Get all models' scores for the provided metrics. get_params Get parameters for this estimator. log Save information to the logger and print to stdout. merge Merge another instance of the same class into this one. reset_aesthetics Reset the plot aesthetics to their default values. run Fit and evaluate the models. save Save the instance to a pickle file. set_params Set the parameters of this estimator. stacking Train a Stacking model. voting Train a Voting model. method available_models () [source] Give an overview of the available predefined models. Returns: pd.DataFrame Information about the predefined models available for the current task. Columns include: acronym: Model's acronym (used to call the model). fullname: Complete name of the model. estimator: The model's underlying estimator. module: The estimator's module. needs_scaling: Whether the model requires feature scaling. accepts_sparse: Whether the model has native support for sparse matrices. supports_gpu: Whether the model has GPU support. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters: nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. Deleted models are not removed from any active mlflow experiment. Parameters: models: str or sequence, default=None Models to delete. If None, delete them all. method evaluate (metric=None, dataset=\"test\") [source] Get all the models' scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". Returns: pd.DataFrame Scores of the models. method get_class_weights (dataset=\"train\") [source] Return class weights for a balanced data set. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters: dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns: dict Classes with the corresponding weights. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters: other: SuccessiveHalvingRegressor Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method run (*arrays) [source] Fit and evaluate the models. Parameters: *arrays: sequence of indexables Training and test set (and optionally a holdout set). Allowed formats are: train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Remember that the class contains the complete dataset as attribute, so the file can become large for big datasets! To avoid this, use save_data=False . Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the data as an attribute of the instance. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: SuccessiveHalvingRegressor Estimator instance. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters: name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: Stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's StackingRegressor instance. The predefined model's acronyms can be used for the final_estimator parameter. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters: name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: Vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's VotingRegressor instance.","title":"Methods"},{"location":"API/training/successivehalvingregressor/#example","text":"from atom.training import SuccessiveHalvingRegressor # Run the pipeline trainer = SuccessiveHalvingRegressor ([ \"Tree\" , \"Bag\" , \"RF\" , \"ET\" ], metric = \"f1\" ) trainer . run ( train , test ) # Analyze the results trainer . plot_successive_halving () print ( trainer . results )","title":"Example"},{"location":"API/training/trainsizingclassifier/","text":"TrainSizingClassifier class atom.training. TrainSizingClassifier (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=5, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, n_jobs=1, gpu=False, verbose=0, warnings=True, logger=None, experiment=None, random_state=None) [source] Fit and evaluate the models in a train sizing fashion. The following steps are applied to every model (per iteration): Hyperparameter tuning is performed using a Bayesian Optimization approach (optional). The model is fitted on the training set using the best combination of hyperparameters found. The model is evaluated on the test set. The model is trained on various bootstrapped samples of the training set and scored again on the test set (optional). You can predict , plot and call any model from the instance. Read more in the user guide . Parameters: models: str, estimator or sequence, default=None Models to fit to the data. Allowed inputs are: an acronym from any of ATOM's predefined models, an ATOMModel or a custom estimator as class or instance. If None, all the predefined models are used. Available predefined models are: \"GP\" for Gaussian Process \"GNB\" for Gaussian Naive Bayes \"MNB\" for Multinomial Naive Bayes \"BNB\" for Bernoulli Naive Bayes \"CatNB\" for Categorical Naive Bayes \"CNB\" for Complement Naive Bayes \"Ridge\" for Ridge Classification \"LR\" for Logistic Regression \"LDA\" for Linear Discriminant Analysis \"QDA\" for Quadratic Discriminant Analysis \"KNN\" for K-Nearest Neighbors \"RNN\" for Radius Nearest Neighbors \"Tree\" for a single Decision Tree \"Bag\" for Bagging \"ET\" for Extra-Trees \"RF\" for Random Forest \"AdaB\" for AdaBoost \"GBM\" for Gradient Boosting Machine \"XGB\" for XGBoost (only available if package is installed) \"LGB\" for LightGBM (only available if package is installed) \"CatB\" for CatBoost (only available if package is installed) \"lSVM\" for Linear SVM \"kSVM\" for Kernel SVM \"PA\" for Passive Aggressive \"SGD\" for Stochastic Gradient Descent \"MLP\" for Multi-layer Perceptron metric: str, func, scorer, sequence or None, default=None Metric on which to fit the models. Choose from any of sklearn's SCORERS , a function with signature metric(y_true, y_pred) , a scorer object or a sequence of these. If multiple metrics are selected, only the first is used to optimize the BO. If None, a default metric is selected: \"f1\" for binary classification \"f1_weighted\" for multiclass classification \"r2\" for regression greater_is_better: bool or sequence, default=True Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_proba: bool or sequence, default=False Whether the metric function requires probability estimates out of a classifier. If True, make sure that every selected model has a predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_threshold: bool or sequence, default=False Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. train_sizes: int or sequence, default=5 Sequence of training set sizes used to run the trainings. If int: Number of equally distributed splits, i.e. for a value N it's equal to np.linspace(1.0/N, 1.0, N). If sequence: Fraction of the training set when < =1, else total number of samples. n_calls: int or sequence, default=0 Maximum number of iterations of the BO. It includes the random points of n_initial_points . If 0, skip the BO and fit the model on its default parameters. If sequence, the n-th value applies to the n-th model. n_initial_points: int or sequence, default=5 Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value applies to the n-th model. est_params: dict, default=None Additional parameters for the estimators. See the corresponding documentation for the available options. For multiple models, use the acronyms as key (or 'all' for all models) and a dict of the parameters as value. Add _fit to the parameter's name to pass it to the fit method instead of the initializer. bo_params: dict, default=None Additional parameters to for the BO. These can include: base_estimator: str, default=\"GP\" Base estimator to use in the BO. Choose from: \"GP\" for Gaussian Process \"RF\" for Random Forest \"ET\" for Extra-Trees \"GBRT\" for Gradient Boosted Regression Trees max_time: int, default=np.inf Stop the optimization after max_time seconds. delta_x: int or float, default=0 Stop the optimization when |x1 - x2| < delta_x . delta_y: int or float, default=0) Stop the optimization if the 5 minima are within delta_y (the function is always minimized. cv: int, default=1 Number of folds for the cross-validation. If 1, the training set is randomly split in a subtrain and validation set. early stopping: int, float or None, default=None Training will stop if the model didn't improve in last early_stopping rounds. If < 1, fraction of rounds from the total. If None, no early stopping is performed. Only available for models that allow in-training evaluation. callback: callable or list of callables, default=None Callbacks for the BO. dimensions: dict, list or None, default=None Custom hyperparameter space for the bayesian optimization. Can be a list to share dimensions across models or a dict with the model's name as key (or 'all' for all models). If None, ATOM's predefined dimensions are used. plot: bool, default=False Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Additional keyword arguments for skopt's optimizer. bootstrap: int or sequence, default=0 Number of data sets (bootstrapped from the training set) to use in the bootstrap algorithm. If 0, no bootstrap is performed. If sequence, the n-th value will apply to the n-th model. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs . Beware that using multiple processes on the same machine may cause memory issues for large datasets. gpu: bool or str, default=False Train models on GPU. Refer to the documentation to check which estimators are supported. If False: Always use CPU implementation. If True: Use GPU implementation if possible. If \"force\": Force GPU implementation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of the actions in python's warnings environment. Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go directly from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random . Magic methods The class contains some magic methods to help you access some of its elements faster. __len__: Returns the length of the dataset. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a model, column or subset of the dataset. Attributes Data attributes The dataset can be accessed at any time through multiple attributes, e.g. calling trainer.train will return the training set. Updating one of the data attributes will automatically update the rest as well. Changing the branch will also change the response from these attributes accordingly. Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: models: list Names of the models in the instance. metric: str or list Metric(s) used to fit the models. errors: dict Dictionary of the encountered exceptions (if any). winners: list of str Model names ordered by performance on the test set (either through the metric_test or mean_bootstrap attribute). winner: model Model subclass that performed best on the test set (either through the metric_test or mean_bootstrap attribute). results: pd.DataFrame Dataframe of the training results. Columns can include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Plot attributes Attributes: style: str Plotting style. See seaborn's documentation . palette: str Color palette. See seaborn's documentation . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes. Methods available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. evaluate Get all models' scores for the provided metrics. get_class_weight Return class weights for a balanced dataset. get_params Get parameters for this estimator. log Save information to the logger and print to stdout. merge Merge another instance of the same class into this one. reset_aesthetics Reset the plot aesthetics to their default values. run Fit and evaluate the models. save Save the instance to a pickle file. set_params Set the parameters of this estimator. stacking Train a Stacking model. voting Train a Voting model. method available_models () [source] Give an overview of the available predefined models. Returns: pd.DataFrame Information about the predefined models available for the current task. Columns include: acronym: Model's acronym (used to call the model). fullname: Complete name of the model. estimator: The model's underlying estimator. module: The estimator's module. needs_scaling: Whether the model requires feature scaling. accepts_sparse: Whether the model has native support for sparse matrices. supports_gpu: Whether the model has GPU support. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters: nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. Deleted models are not removed from any active mlflow experiment. Parameters: models: str or sequence, default=None Models to delete. If None, delete them all. method evaluate (metric=None, dataset=\"test\", threshold=0.5) [source] Get all the models' scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. Returns: pd.DataFrame Scores of the models. method get_class_weights (dataset=\"train\") [source] Return class weights for a balanced data set. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters: dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns: dict Classes with the corresponding weights. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters: other: TrainSizingClassifier Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method run (*arrays) [source] Fit and evaluate the models. Parameters: *arrays: sequence of indexables Training and test set (and optionally a holdout set). Allowed formats are: train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Remember that the class contains the complete dataset as attribute, so the file can become large for big datasets! To avoid this, use save_data=False . Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the data as an attribute of the instance. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: TrainSizingClassifier Estimator instance. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters: name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: Stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's StackingClassifier instance. The predefined model's acronyms can be used for the final_estimator parameter. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters: name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: Vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's VotingClassifier instance. Example from atom.training import TrainSizingClassifier # Run the pipeline trainer = TrainSizingClassifier ( \"RF\" , n_calls = 5 , n_initial_points = 3 ) trainer . run ( train , test ) # Analyze the results trainer . plot_learning_curve () print ( trainer . results )","title":"TrainSizingClassifier"},{"location":"API/training/trainsizingclassifier/#trainsizingclassifier","text":"class atom.training. TrainSizingClassifier (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=5, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, n_jobs=1, gpu=False, verbose=0, warnings=True, logger=None, experiment=None, random_state=None) [source] Fit and evaluate the models in a train sizing fashion. The following steps are applied to every model (per iteration): Hyperparameter tuning is performed using a Bayesian Optimization approach (optional). The model is fitted on the training set using the best combination of hyperparameters found. The model is evaluated on the test set. The model is trained on various bootstrapped samples of the training set and scored again on the test set (optional). You can predict , plot and call any model from the instance. Read more in the user guide . Parameters: models: str, estimator or sequence, default=None Models to fit to the data. Allowed inputs are: an acronym from any of ATOM's predefined models, an ATOMModel or a custom estimator as class or instance. If None, all the predefined models are used. Available predefined models are: \"GP\" for Gaussian Process \"GNB\" for Gaussian Naive Bayes \"MNB\" for Multinomial Naive Bayes \"BNB\" for Bernoulli Naive Bayes \"CatNB\" for Categorical Naive Bayes \"CNB\" for Complement Naive Bayes \"Ridge\" for Ridge Classification \"LR\" for Logistic Regression \"LDA\" for Linear Discriminant Analysis \"QDA\" for Quadratic Discriminant Analysis \"KNN\" for K-Nearest Neighbors \"RNN\" for Radius Nearest Neighbors \"Tree\" for a single Decision Tree \"Bag\" for Bagging \"ET\" for Extra-Trees \"RF\" for Random Forest \"AdaB\" for AdaBoost \"GBM\" for Gradient Boosting Machine \"XGB\" for XGBoost (only available if package is installed) \"LGB\" for LightGBM (only available if package is installed) \"CatB\" for CatBoost (only available if package is installed) \"lSVM\" for Linear SVM \"kSVM\" for Kernel SVM \"PA\" for Passive Aggressive \"SGD\" for Stochastic Gradient Descent \"MLP\" for Multi-layer Perceptron metric: str, func, scorer, sequence or None, default=None Metric on which to fit the models. Choose from any of sklearn's SCORERS , a function with signature metric(y_true, y_pred) , a scorer object or a sequence of these. If multiple metrics are selected, only the first is used to optimize the BO. If None, a default metric is selected: \"f1\" for binary classification \"f1_weighted\" for multiclass classification \"r2\" for regression greater_is_better: bool or sequence, default=True Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_proba: bool or sequence, default=False Whether the metric function requires probability estimates out of a classifier. If True, make sure that every selected model has a predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_threshold: bool or sequence, default=False Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. train_sizes: int or sequence, default=5 Sequence of training set sizes used to run the trainings. If int: Number of equally distributed splits, i.e. for a value N it's equal to np.linspace(1.0/N, 1.0, N). If sequence: Fraction of the training set when < =1, else total number of samples. n_calls: int or sequence, default=0 Maximum number of iterations of the BO. It includes the random points of n_initial_points . If 0, skip the BO and fit the model on its default parameters. If sequence, the n-th value applies to the n-th model. n_initial_points: int or sequence, default=5 Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value applies to the n-th model. est_params: dict, default=None Additional parameters for the estimators. See the corresponding documentation for the available options. For multiple models, use the acronyms as key (or 'all' for all models) and a dict of the parameters as value. Add _fit to the parameter's name to pass it to the fit method instead of the initializer. bo_params: dict, default=None Additional parameters to for the BO. These can include: base_estimator: str, default=\"GP\" Base estimator to use in the BO. Choose from: \"GP\" for Gaussian Process \"RF\" for Random Forest \"ET\" for Extra-Trees \"GBRT\" for Gradient Boosted Regression Trees max_time: int, default=np.inf Stop the optimization after max_time seconds. delta_x: int or float, default=0 Stop the optimization when |x1 - x2| < delta_x . delta_y: int or float, default=0) Stop the optimization if the 5 minima are within delta_y (the function is always minimized. cv: int, default=1 Number of folds for the cross-validation. If 1, the training set is randomly split in a subtrain and validation set. early stopping: int, float or None, default=None Training will stop if the model didn't improve in last early_stopping rounds. If < 1, fraction of rounds from the total. If None, no early stopping is performed. Only available for models that allow in-training evaluation. callback: callable or list of callables, default=None Callbacks for the BO. dimensions: dict, list or None, default=None Custom hyperparameter space for the bayesian optimization. Can be a list to share dimensions across models or a dict with the model's name as key (or 'all' for all models). If None, ATOM's predefined dimensions are used. plot: bool, default=False Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Additional keyword arguments for skopt's optimizer. bootstrap: int or sequence, default=0 Number of data sets (bootstrapped from the training set) to use in the bootstrap algorithm. If 0, no bootstrap is performed. If sequence, the n-th value will apply to the n-th model. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs . Beware that using multiple processes on the same machine may cause memory issues for large datasets. gpu: bool or str, default=False Train models on GPU. Refer to the documentation to check which estimators are supported. If False: Always use CPU implementation. If True: Use GPU implementation if possible. If \"force\": Force GPU implementation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of the actions in python's warnings environment. Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go directly from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random .","title":"TrainSizingClassifier"},{"location":"API/training/trainsizingclassifier/#magic-methods","text":"The class contains some magic methods to help you access some of its elements faster. __len__: Returns the length of the dataset. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a model, column or subset of the dataset.","title":"Magic methods"},{"location":"API/training/trainsizingclassifier/#attributes","text":"","title":"Attributes"},{"location":"API/training/trainsizingclassifier/#data-attributes","text":"The dataset can be accessed at any time through multiple attributes, e.g. calling trainer.train will return the training set. Updating one of the data attributes will automatically update the rest as well. Changing the branch will also change the response from these attributes accordingly. Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/training/trainsizingclassifier/#utility-attributes","text":"Attributes: models: list Names of the models in the instance. metric: str or list Metric(s) used to fit the models. errors: dict Dictionary of the encountered exceptions (if any). winners: list of str Model names ordered by performance on the test set (either through the metric_test or mean_bootstrap attribute). winner: model Model subclass that performed best on the test set (either through the metric_test or mean_bootstrap attribute). results: pd.DataFrame Dataframe of the training results. Columns can include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/training/trainsizingclassifier/#plot-attributes","text":"Attributes: style: str Plotting style. See seaborn's documentation . palette: str Color palette. See seaborn's documentation . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes.","title":"Plot attributes"},{"location":"API/training/trainsizingclassifier/#methods","text":"available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. evaluate Get all models' scores for the provided metrics. get_class_weight Return class weights for a balanced dataset. get_params Get parameters for this estimator. log Save information to the logger and print to stdout. merge Merge another instance of the same class into this one. reset_aesthetics Reset the plot aesthetics to their default values. run Fit and evaluate the models. save Save the instance to a pickle file. set_params Set the parameters of this estimator. stacking Train a Stacking model. voting Train a Voting model. method available_models () [source] Give an overview of the available predefined models. Returns: pd.DataFrame Information about the predefined models available for the current task. Columns include: acronym: Model's acronym (used to call the model). fullname: Complete name of the model. estimator: The model's underlying estimator. module: The estimator's module. needs_scaling: Whether the model requires feature scaling. accepts_sparse: Whether the model has native support for sparse matrices. supports_gpu: Whether the model has GPU support. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters: nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. Deleted models are not removed from any active mlflow experiment. Parameters: models: str or sequence, default=None Models to delete. If None, delete them all. method evaluate (metric=None, dataset=\"test\", threshold=0.5) [source] Get all the models' scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". threshold: float, default=0.5 Threshold between 0 and 1 to convert predicted probabilities to class labels. Only used when: The task is binary classification. The model has a predict_proba method. The metric evaluates predicted target values. Returns: pd.DataFrame Scores of the models. method get_class_weights (dataset=\"train\") [source] Return class weights for a balanced data set. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters: dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns: dict Classes with the corresponding weights. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters: other: TrainSizingClassifier Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method run (*arrays) [source] Fit and evaluate the models. Parameters: *arrays: sequence of indexables Training and test set (and optionally a holdout set). Allowed formats are: train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Remember that the class contains the complete dataset as attribute, so the file can become large for big datasets! To avoid this, use save_data=False . Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the data as an attribute of the instance. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: TrainSizingClassifier Estimator instance. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters: name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: Stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's StackingClassifier instance. The predefined model's acronyms can be used for the final_estimator parameter. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters: name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: Vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's VotingClassifier instance.","title":"Methods"},{"location":"API/training/trainsizingclassifier/#example","text":"from atom.training import TrainSizingClassifier # Run the pipeline trainer = TrainSizingClassifier ( \"RF\" , n_calls = 5 , n_initial_points = 3 ) trainer . run ( train , test ) # Analyze the results trainer . plot_learning_curve () print ( trainer . results )","title":"Example"},{"location":"API/training/trainsizingregressor/","text":"TrainSizingRegressor class atom.training. TrainSizingRegressor (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=5, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, n_jobs=1, gpu=False, verbose=0, warnings=True, logger=None, experiment=None, random_state=None) [source] Fit and evaluate the models in a train sizing fashion. The following steps are applied to every model (per iteration): Hyperparameter tuning is performed using a Bayesian Optimization approach (optional). The model is fitted on the training set using the best combination of hyperparameters found. The model is evaluated on the test set. The model is trained on various bootstrapped samples of the training set and scored again on the test set (optional). You can predict , plot and call any model from the instance. Read more in the user guide . Parameters: models: str, estimator or sequence, default=None Models to fit to the data. Allowed inputs are: an acronym from any of ATOM's predefined models, an ATOMModel or a custom estimator as class or instance. If None, all the predefined models are used. Available predefined models are: \"GP\" for Gaussian Process \"OLS\" for Ordinary Least Squares \"Ridge\" for Ridge Regression \"Lasso\" for Lasso Regression \"EN\" for ElasticNet \"BR\" for Bayesian Ridge \"ARD\" for Automated Relevance Determination \"KNN\" for K-Nearest Neighbors \"RNN\" for Radius Nearest Neighbors \"Tree\" for a single Decision Tree \"Bag\" for Bagging \"ET\" for Extra-Trees \"RF\" for Random Forest \"AdaB\" for AdaBoost \"GBM\" for Gradient Boosting Machine \"XGB\" for XGBoost (only available if package is installed) \"LGB\" for LightGBM (only available if package is installed) \"CatB\" for CatBoost (only available if package is installed) \"lSVM\" for Linear SVM \"kSVM\" for Kernel SVM \"PA\" for Passive Aggressive \"SGD\" for Stochastic Gradient Descent \"MLP\" for Multi-layer Perceptron metric: str, func, scorer, sequence or None, default=None Metric on which to fit the models. Choose from any of sklearn's SCORERS , a function with signature metric(y_true, y_pred) , a scorer object or a sequence of these. If multiple metrics are selected, only the first is used to optimize the BO. If None, a default metric is selected: \"f1\" for binary classification \"f1_weighted\" for multiclass classification \"r2\" for regression greater_is_better: bool or sequence, default=True Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_proba: bool or sequence, default=False Whether the metric function requires probability estimates out of a classifier. If True, make sure that every selected model has a predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_threshold: bool or sequence, default=False Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. train_sizes: int or sequence, default=5 Sequence of training set sizes used to run the trainings. If int: Number of equally distributed splits, i.e. for a value N it's equal to np.linspace(1.0/N, 1.0, N). If sequence: Fraction of the training set when < =1, else total number of samples. n_calls: int or sequence, default=0 Maximum number of iterations of the BO. It includes the random points of n_initial_points . If 0, skip the BO and fit the model on its default parameters. If sequence, the n-th value applies to the n-th model. n_initial_points: int or sequence, default=5 Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value applies to the n-th model. est_params: dict, default=None Additional parameters for the estimators. See the corresponding documentation for the available options. For multiple models, use the acronyms as key (or 'all' for all models) and a dict of the parameters as value. Add _fit to the parameter's name to pass it to the fit method instead of the initializer. bo_params: dict, default=None Additional parameters to for the BO. These can include: base_estimator: str, default=\"GP\" Base estimator to use in the BO. Choose from: \"GP\" for Gaussian Process \"RF\" for Random Forest \"ET\" for Extra-Trees \"GBRT\" for Gradient Boosted Regression Trees max_time: int, default=np.inf Stop the optimization after max_time seconds. delta_x: int or float, default=0 Stop the optimization when |x1 - x2| < delta_x . delta_y: int or float, default=0) Stop the optimization if the 5 minima are within delta_y (the function is always minimized. cv: int, default=1 Number of folds for the cross-validation. If 1, the training set is randomly split in a subtrain and validation set. early stopping: int, float or None, default=None Training will stop if the model didn't improve in last early_stopping rounds. If < 1, fraction of rounds from the total. If None, no early stopping is performed. Only available for models that allow in-training evaluation. callback: callable or list of callables, default=None Callbacks for the BO. dimensions: dict, list or None, default=None Custom hyperparameter space for the bayesian optimization. Can be a list to share dimensions across models or a dict with the model's name as key (or 'all' for all models). If None, ATOM's predefined dimensions are used. plot: bool, default=False Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Additional keyword arguments for skopt's optimizer. bootstrap: int or sequence, default=0 Number of data sets (bootstrapped from the training set) to use in the bootstrap algorithm. If 0, no bootstrap is performed. If sequence, the n-th value will apply to the n-th model. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs . Beware that using multiple processes on the same machine may cause memory issues for large datasets. gpu: bool or str, default=False Train models on GPU. Refer to the documentation to check which estimators are supported. If False: Always use CPU implementation. If True: Use GPU implementation if possible. If \"force\": Force GPU implementation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of the actions in python's warnings environment. Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go directly from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random . Magic methods The class contains some magic methods to help you access some of its elements faster. __len__: Returns the length of the dataset. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a model, column or subset of the dataset. Attributes Data attributes The dataset can be accessed at any time through multiple attributes, e.g. calling trainer.train will return the training set. Updating one of the data attributes will automatically update the rest as well. Changing the branch will also change the response from these attributes accordingly. Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column. Utility attributes Attributes: models: list Names of the models in the instance. metric: str or list Metric(s) used to fit the models. errors: dict Dictionary of the encountered exceptions (if any). winners: list of str Model names ordered by performance on the test set (either through the metric_test or mean_bootstrap attribute). winner: model Model subclass that performed best on the test set (either through the metric_test or mean_bootstrap attribute). results: pd.DataFrame Dataframe of the training results. Columns can include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run. Plot attributes Attributes: style: str Plotting style. See seaborn's documentation . palette: str Color palette. See seaborn's documentation . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes. Methods available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. evaluate Get all models' scores for the provided metrics. get_params Get parameters for this estimator. log Save information to the logger and print to stdout. merge Merge another instance of the same class into this one. reset_aesthetics Reset the plot aesthetics to their default values. run Fit and evaluate the models. save Save the instance to a pickle file. set_params Set the parameters of this estimator. stacking Train a Stacking model. voting Train a Voting model. method available_models () [source] Give an overview of the available predefined models. Returns: pd.DataFrame Information about the predefined models available for the current task. Columns include: acronym: Model's acronym (used to call the model). fullname: Complete name of the model. estimator: The model's underlying estimator. module: The estimator's module. needs_scaling: Whether the model requires feature scaling. accepts_sparse: Whether the model has native support for sparse matrices. supports_gpu: Whether the model has GPU support. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters: nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. Deleted models are not removed from any active mlflow experiment. Parameters: models: str or sequence, default=None Models to delete. If None, delete them all. method evaluate (metric=None, dataset=\"test\") [source] Get all the models' scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". Returns: pd.DataFrame Scores of the models. method get_class_weights (dataset=\"train\") [source] Return class weights for a balanced data set. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters: dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns: dict Classes with the corresponding weights. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters: other: TrainSizingRegressor Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method run (*arrays) [source] Fit and evaluate the models. Parameters: *arrays: sequence of indexables Training and test set (and optionally a holdout set). Allowed formats are: train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Remember that the class contains the complete dataset as attribute, so the file can become large for big datasets! To avoid this, use save_data=False . Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the data as an attribute of the instance. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: TrainSizingRegressor Estimator instance. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters: name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: Stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's StackingRegressor instance. The predefined model's acronyms can be used for the final_estimator parameter. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters: name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: Vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's VotingRegressor instance. Example from atom.training import TrainSizingRegressor # Run the pipeline trainer = TrainSizingRegressor ( \"RF\" , n_calls = 5 , n_initial_points = 3 ) trainer . run ( train , test ) # Analyze the results trainer . plot_learning_curve () print ( trainer . results )","title":"TrainSizingRegressor"},{"location":"API/training/trainsizingregressor/#trainsizingregressor","text":"class atom.training. TrainSizingRegressor (models=None, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=5, n_calls=0, n_initial_points=5, est_params=None, bo_params=None, n_bootstrap=0, n_jobs=1, gpu=False, verbose=0, warnings=True, logger=None, experiment=None, random_state=None) [source] Fit and evaluate the models in a train sizing fashion. The following steps are applied to every model (per iteration): Hyperparameter tuning is performed using a Bayesian Optimization approach (optional). The model is fitted on the training set using the best combination of hyperparameters found. The model is evaluated on the test set. The model is trained on various bootstrapped samples of the training set and scored again on the test set (optional). You can predict , plot and call any model from the instance. Read more in the user guide . Parameters: models: str, estimator or sequence, default=None Models to fit to the data. Allowed inputs are: an acronym from any of ATOM's predefined models, an ATOMModel or a custom estimator as class or instance. If None, all the predefined models are used. Available predefined models are: \"GP\" for Gaussian Process \"OLS\" for Ordinary Least Squares \"Ridge\" for Ridge Regression \"Lasso\" for Lasso Regression \"EN\" for ElasticNet \"BR\" for Bayesian Ridge \"ARD\" for Automated Relevance Determination \"KNN\" for K-Nearest Neighbors \"RNN\" for Radius Nearest Neighbors \"Tree\" for a single Decision Tree \"Bag\" for Bagging \"ET\" for Extra-Trees \"RF\" for Random Forest \"AdaB\" for AdaBoost \"GBM\" for Gradient Boosting Machine \"XGB\" for XGBoost (only available if package is installed) \"LGB\" for LightGBM (only available if package is installed) \"CatB\" for CatBoost (only available if package is installed) \"lSVM\" for Linear SVM \"kSVM\" for Kernel SVM \"PA\" for Passive Aggressive \"SGD\" for Stochastic Gradient Descent \"MLP\" for Multi-layer Perceptron metric: str, func, scorer, sequence or None, default=None Metric on which to fit the models. Choose from any of sklearn's SCORERS , a function with signature metric(y_true, y_pred) , a scorer object or a sequence of these. If multiple metrics are selected, only the first is used to optimize the BO. If None, a default metric is selected: \"f1\" for binary classification \"f1_weighted\" for multiclass classification \"r2\" for regression greater_is_better: bool or sequence, default=True Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_proba: bool or sequence, default=False Whether the metric function requires probability estimates out of a classifier. If True, make sure that every selected model has a predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. needs_threshold: bool or sequence, default=False Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. This parameter is ignored if the metric is a string or a scorer. If sequence, the n-th value applies to the n-th metric. train_sizes: int or sequence, default=5 Sequence of training set sizes used to run the trainings. If int: Number of equally distributed splits, i.e. for a value N it's equal to np.linspace(1.0/N, 1.0, N). If sequence: Fraction of the training set when < =1, else total number of samples. n_calls: int or sequence, default=0 Maximum number of iterations of the BO. It includes the random points of n_initial_points . If 0, skip the BO and fit the model on its default parameters. If sequence, the n-th value applies to the n-th model. n_initial_points: int or sequence, default=5 Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value applies to the n-th model. est_params: dict, default=None Additional parameters for the estimators. See the corresponding documentation for the available options. For multiple models, use the acronyms as key (or 'all' for all models) and a dict of the parameters as value. Add _fit to the parameter's name to pass it to the fit method instead of the initializer. bo_params: dict, default=None Additional parameters to for the BO. These can include: base_estimator: str, default=\"GP\" Base estimator to use in the BO. Choose from: \"GP\" for Gaussian Process \"RF\" for Random Forest \"ET\" for Extra-Trees \"GBRT\" for Gradient Boosted Regression Trees max_time: int, default=np.inf Stop the optimization after max_time seconds. delta_x: int or float, default=0 Stop the optimization when |x1 - x2| < delta_x . delta_y: int or float, default=0) Stop the optimization if the 5 minima are within delta_y (the function is always minimized. cv: int, default=1 Number of folds for the cross-validation. If 1, the training set is randomly split in a subtrain and validation set. early stopping: int, float or None, default=None Training will stop if the model didn't improve in last early_stopping rounds. If < 1, fraction of rounds from the total. If None, no early stopping is performed. Only available for models that allow in-training evaluation. callback: callable or list of callables, default=None Callbacks for the BO. dimensions: dict, list or None, default=None Custom hyperparameter space for the bayesian optimization. Can be a list to share dimensions across models or a dict with the model's name as key (or 'all' for all models). If None, ATOM's predefined dimensions are used. plot: bool, default=False Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Additional keyword arguments for skopt's optimizer. bootstrap: int or sequence, default=0 Number of data sets (bootstrapped from the training set) to use in the bootstrap algorithm. If 0, no bootstrap is performed. If sequence, the n-th value will apply to the n-th model. n_jobs: int, default=1 Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs . Beware that using multiple processes on the same machine may cause memory issues for large datasets. gpu: bool or str, default=False Train models on GPU. Refer to the documentation to check which estimators are supported. If False: Always use CPU implementation. If True: Use GPU implementation if possible. If \"force\": Force GPU implementation. verbose: int, default=0 Verbosity level of the class. Choose from: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, default=False If True: Default warning action (equal to \"default\"). If False: Suppress all warnings (equal to \"ignore\"). If str: One of the actions in python's warnings environment. Changing this parameter affects the PYTHONWARNINGS environment. ATOM can't manage warnings that go directly from C/C++ code to stdout. logger: str, Logger or None, default=None If None: Doesn't save a logging file. If str: Name of the log file. Use \"auto\" for automatic naming. Else: Python logging.Logger instance. experiment: str or None, default=None Name of the mlflow experiment to use for tracking. If None, no mlflow tracking is performed. random_state: int or None, default=None Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random .","title":"TrainSizingRegressor"},{"location":"API/training/trainsizingregressor/#magic-methods","text":"The class contains some magic methods to help you access some of its elements faster. __len__: Returns the length of the dataset. __contains__: Checks if the provided item is a column in the dataset. __getitem__: Access a model, column or subset of the dataset.","title":"Magic methods"},{"location":"API/training/trainsizingregressor/#attributes","text":"","title":"Attributes"},{"location":"API/training/trainsizingregressor/#data-attributes","text":"The dataset can be accessed at any time through multiple attributes, e.g. calling trainer.train will return the training set. Updating one of the data attributes will automatically update the rest as well. Changing the branch will also change the response from these attributes accordingly. Attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. shape: tuple Dataset's shape: (n_rows x n_columns). columns: pd.Index Names of the columns in the dataset. n_columns: int Number of columns in the dataset. features: pd.Index Names of the features in the dataset. n_features: int Number of features in the dataset. target: str Name of the target column.","title":"Data attributes"},{"location":"API/training/trainsizingregressor/#utility-attributes","text":"Attributes: models: list Names of the models in the instance. metric: str or list Metric(s) used to fit the models. errors: dict Dictionary of the encountered exceptions (if any). winners: list of str Model names ordered by performance on the test set (either through the metric_test or mean_bootstrap attribute). winner: model Model subclass that performed best on the test set (either through the metric_test or mean_bootstrap attribute). results: pd.DataFrame Dataframe of the training results. Columns can include: metric_bo: Best score achieved during the BO. time_bo: Time spent on the BO. metric_train: Metric score on the training set. metric_test: Metric score on the test set. time_fit: Time spent fitting and evaluating. mean_bootstrap: Mean score of the bootstrap results. std_bootstrap: Standard deviation score of the bootstrap results. time_bootstrap: Time spent on the bootstrap algorithm. time: Total time spent on the whole run.","title":"Utility attributes"},{"location":"API/training/trainsizingregressor/#plot-attributes","text":"Attributes: style: str Plotting style. See seaborn's documentation . palette: str Color palette. See seaborn's documentation . title_fontsize: int Fontsize for the plot's title. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for the ticks along the plot's axes.","title":"Plot attributes"},{"location":"API/training/trainsizingregressor/#methods","text":"available_models Give an overview of the available predefined models. canvas Create a figure with multiple plots. clear Clear attributes from all models. delete Delete models. evaluate Get all models' scores for the provided metrics. get_params Get parameters for this estimator. log Save information to the logger and print to stdout. merge Merge another instance of the same class into this one. reset_aesthetics Reset the plot aesthetics to their default values. run Fit and evaluate the models. save Save the instance to a pickle file. set_params Set the parameters of this estimator. stacking Train a Stacking model. voting Train a Voting model. method available_models () [source] Give an overview of the available predefined models. Returns: pd.DataFrame Information about the predefined models available for the current task. Columns include: acronym: Model's acronym (used to call the model). fullname: Complete name of the model. estimator: The model's underlying estimator. module: The estimator's module. needs_scaling: Whether the model requires feature scaling. accepts_sparse: Whether the model has native support for sparse matrices. supports_gpu: Whether the model has GPU support. method canvas (nrows=1, ncols=2, title=None, figsize=None, filename=None, display=True) [source] This @contextmanager allows you to draw many plots in one figure. The default option is to add two plots side by side. See the user guide for an example. Parameters: nrows: int, default=1 Number of plots in length. ncols: int, default=2 Number of plots in width. title: str or None, default=None Plot's title. If None, no title is displayed. figsize: tuple or None, default=None Figure's size, format as (x, y). If None, it adapts the size to the number of plots in the canvas. filename: str or None, default=None Name of the file. Use \"auto\" for automatic naming. If None, the figure is not saved. display: bool, default=True Whether to render the plot. method clear () [source] Reset all model attributes to their initial state, deleting potentially large data arrays. Use this method to free some memory before saving the class. The cleared attributes per model are: Prediction attributes . Metrics scores . Shap values . Dashboard instance . method delete (models=None) [source] Delete models. If all models are removed, the metric is reset. Use this method to drop unwanted models from the pipeline or to free some memory before saving. Deleted models are not removed from any active mlflow experiment. Parameters: models: str or sequence, default=None Models to delete. If None, delete them all. method evaluate (metric=None, dataset=\"test\") [source] Get all the models' scores for the provided metrics. Parameters: metric: str, func, scorer, sequence or None, default=None Metrics to calculate. If None, a selection of the most common metrics per task are used. dataset: str, default=\"test\" Data set on which to calculate the metric. Choose from: \"train\", \"test\" or \"holdout\". Returns: pd.DataFrame Scores of the models. method get_class_weights (dataset=\"train\") [source] Return class weights for a balanced data set. Statistically, the class weights re-balance the data set so that the sampled data set represents the target population as closely as possible. The returned weights are inversely proportional to the class frequencies in the selected data set. Parameters: dataset: str, default=\"train\" Data set from which to get the weights. Choose from: \"train\", \"test\" or \"dataset\". Returns: dict Classes with the corresponding weights. method get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: dict Parameter names mapped to their values. method log (msg, level=0) [source] Write a message to the logger and print it to stdout. Parameters: msg: str Message to write to the logger and print to stdout. level: int, default=0 Minimum verbosity level to print the message. method merge (other, suffix=\"2\") [source] Merge another instance of the same class into this one. Branches, models, metrics and attributes of the other instance are merged into this one. If there are branches and/or models with the same name, they are merged adding the suffix parameter to their name. The errors and missing attributes are extended with those of the other instance. It's only possible to merge two instances if they are initialized with the same dataset and trained with the same metric. Parameters: other: TrainSizingRegressor Instance with which to merge. Should be of the same class as self. suffix: str, default=\"2\" Conflicting branches and models are merged adding suffix to the end of their names. method reset_aesthetics () [source] Reset the plot aesthetics to their default values. method run (*arrays) [source] Fit and evaluate the models. Parameters: *arrays: sequence of indexables Training and test set (and optionally a holdout set). Allowed formats are: train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) method save (filename=\"auto\", save_data=True) [source] Save the instance to a pickle file. Remember that the class contains the complete dataset as attribute, so the file can become large for big datasets! To avoid this, use save_data=False . Parameters: filename: str, default=\"auto\" Name of the file. Use \"auto\" for automatic naming. save_data: bool, default=True Whether to save the data as an attribute of the instance. If False, remember to add the data to ATOMLoader when loading the file. method set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: TrainSizingRegressor Estimator instance. method stacking (name=\"Stack\", models=None, **kwargs) [source] Add a Stacking model to the pipeline. Parameters: name: str, default=\"Stack\" Name of the model. The name is always presided with the model's acronym: Stack . models: sequence or None, default=None Models that feed the stacking estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's StackingRegressor instance. The predefined model's acronyms can be used for the final_estimator parameter. method voting (name=\"Vote\", models=None, **kwargs) [source] Add a Voting model to the pipeline. Parameters: name: str, default=\"Vote\" Name of the model. The name is always presided with the model's acronym: Vote . models: sequence or None, default=None Models that feed the voting estimator. If None, it selects all non-ensemble models trained on the current branch. **kwargs Additional keyword arguments for sklearn's VotingRegressor instance.","title":"Methods"},{"location":"API/training/trainsizingregressor/#example","text":"from atom.training import TrainSizingRegressor # Run the pipeline trainer = TrainSizingRegressor ( \"RF\" , n_calls = 5 , n_initial_points = 3 ) trainer . run ( train , test ) # Analyze the results trainer . plot_learning_curve () print ( trainer . results )","title":"Example"},{"location":"changelog/v4.x.x/","text":"Release history Version 4.14.1 Fixed an installation issue with conda . Version 4.14.0 Refactor of the Cleaner and Vectorizer classes. Refactor of the cross_validate method. The plot_pipeline method now supports drawing multiple pipelines. Renamed the Normalizer class to TextNormalizer . Renamed the Gauss class to Normalizer . Added the inverse_transform method to the Scaler , Normalizer and Cleaner classes. Added the winners property to the trainers (note the extra s ). Added the feature_names_in_ and n_features_in_ attributes to transformers. The default value of the warnings parameter is set to False. Improvements for multicollinearity removal in FeatureSelector . Renamed default feature names to x0 , x1 , etc... for consistency with sklearn's API. Renamed component names in FeatureSelector to pca0 , pca1 , etc... for consistency with sklearn's API. Significant speed up in pipeline transformations. Fixed a bug where mlflow runs could be ended unexpectedly. Version 4.13.1 Fixed an installation issue. Version 4.13.0 Added GPU support. Read more in the user guide . Added advanced feature selection strategies. Added the return_sparse parameter to the Vectorizer class. Added the quantile hyperparameter to the Dummy model. The data attributes now return pandas objects where possible. Fixed a bug where the BO could crash after balancing the data. Fixed a bug where saving the FeatureGenerator class could fail for certain operators. Fixed a bug where the FeatureSelector class displayed the wrong output. Fixed a bug where the mapping attribute was not reordered. Version 4.12.0 Support for Python 3.10 . New Discretizer class to bin numerical features. Refactor of the FeatureGenerator class. The mapping attribute now shows all encoded features. Added the sample_weight parameter to the evaluate method. ATOMClassifier has now a stratify parameter to split the data sets in a stratified fashion . Possibility to exclude hyperparameters from the BO adding ! before the name. Added memory usage to the stats method. Fixed a bug where decision_plot could fail when only one row was plotted. Added versioning to the documentation. Version 4.11.0 Full support for sparse matrices. Read more in the user guide . The shrink method now also handles sparse features. Refactor of the distribution method. Added three new linear models: Lars , Huber and Perc . Dimensions can be shared across models using the key 'all' in bo_params[\"dimensions\"] . Assign hyperparameters to tune using the predefined dimensions. It's now possible to tune a custom number of layers for the MLP model. If multiple BO calls share the best score, the one with the shortest training time is selected as winner (instead of the first). Fixed a bug where the BO could fail when custom dimensions where defined. Fixed a bug where FeatureSelector could fail after repeated calls to fit. Fixed a bug where FeatureGenerator didn't pass the correct data indices to its output. Performance improvements for the custom pipeline. Minor documentation fixes. Version 4.10.0 Added the holdout data set to have an extra way of assessing a model's performance on a completely independent dataset. Read more in the user_guide . Complete rework of the ensemble models. Support for dataframe indexing. Read more in the user guide . New plot_parshap plot to detect overfitting features. The new dashboard method makes analyzing the models even easier using a dashboard app. The plot_feature_importance plot now also accepts estimators with coefficients. Added the transform method for models. Added the threshold parameter to the evaluate method. The reset_predictions method is deprecated in favour of the new clear method. Refactor of the model's full_train method. The merge method is available for all trainers. Improvements in the trainer's pipeline. Training scores are now also saved to the mlflow run. Trying to change the data in a branch after fitting a model with it now raises an exception. Fixed a bug where the columns of array inputs were not ordered correctly. Fixed a bug where branches did not correctly act case-insensitive. Fixed a bug where the export_pipeline method for models would not export the transformers in the correct branch. Version 4.9.1 Changed the default cross-validation for hyperparameter tuning from 5 to 1 to avoid errors with deep learning models . Added clearer exception messages when a model's run failed. Fixed a bug where custom dimensions didn't show during hyperparameter tuning . Documentation improvements. Version 4.9.0 Drop support of Python 3.6 . Added the HistGBM model. Improved print layout for hyperparameter tuning . The new available_models method returns an overview of the available predefined models. The calibrate and cross_validate methods can no longer be accessed from the trainers. The pipeline parameter for the prediction methods is deprecated. Improved visualization of the plot_rfecv , plot_successive_halving and plot_learning_curve methods. Sparse matrices are now accepted as input. Duplicate BO calls are no longer calculated. Improvement in performance of the RNN model. Refactor of the model's bo attribute. Predefined hyperparameters have been updated to be consistent with sklearn's API. Fixed a bug where custom scalers were ignored by the models. Fixed a bug where the BO of certain models would crash with custom hyperparameters. Fixed a bug where duplicate column names could be generated from a custom transformer. Documentation improvements. Version 4.8.0 The Encoder class now directly handles unknown categories encountered during fitting. The Balancer and Encoder classes now accept custom estimators for the strategy parameter. The new merge method enables the user to merge multiple atom instances into one. The dtype shrinking is moved from atom's initializers to the shrink method. ATOM's custom pipeline now handles transformers fitted on a subset of the dataset. The column parameter in the distribution method is renamed to columns for continuity of the API. The mae criterion for the GBM model hyperparameter tuning is deprecated to be consistent with sklearn's API. Branches are now case-insensitive. Renaming a branch using an existing name now raises an exception. Fixed a bug where columns of type category broke the Imputer class. Fixed a bug where predictions of the Stacking ensemble crashed for branches with multiple transformers. The tables in the documentation now adapt to dark mode. Version 4.7.3 Fixed a bug where the conda-forge recipe couldn't install properly. Version 4.7.2 Fixed a bug where the pipeline failed for custom transformers that returned sparse matrices. Package requirements files are added to the installer. Version 4.7.1 Fixed a bug where the pip installer failed. Fixed a bug where categorical columns also selected datetime columns. Version 4.7.0 Launched our new slack channel! The new FeatureExtractor class extracts useful features from datetime columns. The new plot_det method plots a binary classifier's detection error tradeoff curve. The partial dependence plot is able to draw Individual Conditional Expectation (ICE) lines. The full traceback of exceptions encountered during training are now saved to the logger. ATOMClassifier and ATOMRegressor now convert the dtypes of the input data to the minimal allowed type for memory efficiency. The scoring method is renamed to evaluate to clarify its purpose. The column parameter in the apply method is renamed to columns for continuity of the API. Minor documentation improvements. Version 4.6.0 Added the full_train method to retrieve an estimator trained on the complete dataset. The score method is now also able to calculate custom metrics on new data. Refactor of the Imputer class. Refactor of the Encoder class to avoid errors for unknown classes and allow the input of missing values. The clean method no longer automatically encodes the target column for regression tasks. Creating a branch using a models' acronym as name now raises an exception. Fixed a bug where CatBoost failed when early_stopping < 1. Fixed a bug where created pipelines had duplicated names. Version 4.5.0 Support of NLP pipelines. Read more in the user guide . Integration of mlflow to track all models in the pipeline. Read more in the user guide . The new Normalizer class transforms features to a more Gaussian-like distribution. New cross_validate method to evaluate the robustness of a pipeline using cross_validation. New reset method to go back to atom's initial state. Added the Dummy model to compare other models with a simple baseline. New plot_wordcloud and plot_ngrams methods for text visualization. Plots now can return the figure object when display=None . The Pruner class can now able to drop outliers based on the selection of multiple strategies. The new shuffle parameter in atom's initializer determines whether to shuffle the dataset. The trainers no longer require you to specify a model using the models parameter. If left to default, all predefined models for that task are used. The apply method now accepts args and kwargs for the function. Refactor of the evaluate method. Refactor of the export_pipeline method. The parameters in the Cleaner class have been refactored to better describe their function. The train_sizes parameter in train_sizing now accepts integer values to automatically create equally distributed splits in the training set. Refactor of plot_pipeline to show models in the diagram as well. Refactor of the bagging parameter to the (more appropriate) name n_bootstrap . New option to exclude columns from a transformer adding ! before their name. Fixed a bug where the Pruner class failed if there were categorical columns in the dataset. Completely reworked documentation website. Version 4.4.0 The drop method now allows the user to drop columns as part of the pipeline. New apply method to perform data transformations as function to the pipeline Added the status method to save an overview of atom's branches and models to the logger. Improved the output messages for the Imputer class. The dataset's columns can now be called directly from atom. The distribution and plot_distribution methods now ignore missing values. Fixed a bug where transformations could fail when columns were added to the dataset after initializing the pipeline. Fixed a bug where the Cleaner class didn't drop columns consisting entirely of missing values when drop_min_cardinality=True . Fixed a bug where the winning model wasn't displayed correctly. Refactored the way transformers are added or removed from predicting methods. Improved documentation. Version 4.3.0 Possibility to add custom transformers to the pipeline. The export_pipeline utility method exports atom's current pipeline to a sklearn object. Use AutoML to automate the search for an optimized pipeline. New magic methods makes atom behave similarly to sklearn's Pipeline . All training approaches can now be combined in the same atom instance. New plot_scatter_matrix , plot_distribution and plot_qq plots for data inspection. Complete rework of all the shap plots to be consistent with their new API. Improvements for the Scaler and Pruner classes. The acronym for custom models now defaults to the capital letters in the class' __name__. Possibility to apply transformations on only a subset of the columns. Plots and methods now accept winner as model name. Fixed a bug where custom metrics didn't show the correct name. Fixed a bug where timers were not displayed correctly. Further compatibility with deep learning datasets. Large refactoring for performance optimization. Cleaner output of messages to the logger. Plots no longer show a default title. Added the AutoML example notebook. Minor bug fixes. Version 4.2.1 Bug fix where there was memory leakage in successive halving and train sizing pipelines. The XGBoost , LightGBM and CatBoost packages can now be installed through the installer's extras_require under the name models , e.g. pip install -U atom-ml[models] . Improved documentation. Version 4.2.0 Possibility to add custom models to the pipeline using ATOMModel . Compatibility with deep learning models. New branch system for different data pipelines. Read more in the user guide . Use the canvas contextmanager to draw multiple plots in one figure. New voting and stacking ensemble techniques. New get_class_weight utility method. New Sequential Feature Selection strategy for the FeatureSelector . Added the sample_weight parameter to the score method. New ways to initialize the data in the training instances. The n_rows parameter in ATOMLoader is deprecated in favour of the new input formats. The test_size parameter now also allows integer values. Renamed categories to classes to be consistent with sklearn's API. The class property now returns a pd.DataFrame of the number of rows per target class in the train, test and complete dataset. Possibility to add custom parameters to an estimator's fit method through est_params . The successive halving and train sizing approaches now both allow subsequent runs from atom without losing the information from previous runs. Bug fix where ATOMLoader wouldn't encode the target column during transformation. Added the Deep learning , Ensembles and Utilities example notebooks. Support for python 3.9 . Version 4.1.0 New est_params parameter to customize the parameters in every model's estimator. Following skopt's API, the n_random_starts parameter to specify the number of random trials is deprecated in favour of n_initial_points . The Balancer class now allows you to use any of the strategies from imblearn . New utility attributes to inspect the dataset. Four new models: CatNB , CNB , ARD and RNN . Added the models section to the documentation. Small changes in log outputs. Bug fixes and performance improvements. Version 4.0.1 Bug fix where the FeatureGenerator was not deterministic for a fixed random state. Bug fix where subsequent runs with the same metric failed. Added the license file to the package's installer. Typo fixes in documentation. Version 4.0.0 Bayesian optimization package changed from GpyOpt to skopt . Complete revision of the model's hyperparameters. Four SHAP plots can now be called directly from an ATOM pipeline. Two new plots for regression tasks. New plot_pipeline and pipeline attribute to access all transformers. Possibility to determine transformer parameters per method. New calibration method and plot . Metrics can now be added as scorers or functions with signature metric(y, y_pred, **kwargs). Implementation of multi-metric runs. Possibility to choose which metric to plot. Early stopping for models that allow in-training evaluation. Added the ATOMLoader function to load any saved pickle instance. The \"remove\" strategy in the data cleaning parameters is deprecated in favour of \"drop\". Implemented the dfs strategy in FeatureGenerator . All training classes now inherit from BaseEstimator. Added multiple new example notebooks. Tests coverage up to 100%. Completely new documentation page. Bug fixes and performance improvements.","title":"v4.x.x"},{"location":"changelog/v4.x.x/#release-history","text":"","title":"Release history"},{"location":"changelog/v4.x.x/#version-4141","text":"Fixed an installation issue with conda .","title":"Version 4.14.1"},{"location":"changelog/v4.x.x/#version-4140","text":"Refactor of the Cleaner and Vectorizer classes. Refactor of the cross_validate method. The plot_pipeline method now supports drawing multiple pipelines. Renamed the Normalizer class to TextNormalizer . Renamed the Gauss class to Normalizer . Added the inverse_transform method to the Scaler , Normalizer and Cleaner classes. Added the winners property to the trainers (note the extra s ). Added the feature_names_in_ and n_features_in_ attributes to transformers. The default value of the warnings parameter is set to False. Improvements for multicollinearity removal in FeatureSelector . Renamed default feature names to x0 , x1 , etc... for consistency with sklearn's API. Renamed component names in FeatureSelector to pca0 , pca1 , etc... for consistency with sklearn's API. Significant speed up in pipeline transformations. Fixed a bug where mlflow runs could be ended unexpectedly.","title":"Version 4.14.0"},{"location":"changelog/v4.x.x/#version-4131","text":"Fixed an installation issue.","title":"Version 4.13.1"},{"location":"changelog/v4.x.x/#version-4130","text":"Added GPU support. Read more in the user guide . Added advanced feature selection strategies. Added the return_sparse parameter to the Vectorizer class. Added the quantile hyperparameter to the Dummy model. The data attributes now return pandas objects where possible. Fixed a bug where the BO could crash after balancing the data. Fixed a bug where saving the FeatureGenerator class could fail for certain operators. Fixed a bug where the FeatureSelector class displayed the wrong output. Fixed a bug where the mapping attribute was not reordered.","title":"Version 4.13.0"},{"location":"changelog/v4.x.x/#version-4120","text":"Support for Python 3.10 . New Discretizer class to bin numerical features. Refactor of the FeatureGenerator class. The mapping attribute now shows all encoded features. Added the sample_weight parameter to the evaluate method. ATOMClassifier has now a stratify parameter to split the data sets in a stratified fashion . Possibility to exclude hyperparameters from the BO adding ! before the name. Added memory usage to the stats method. Fixed a bug where decision_plot could fail when only one row was plotted. Added versioning to the documentation.","title":"Version 4.12.0"},{"location":"changelog/v4.x.x/#version-4110","text":"Full support for sparse matrices. Read more in the user guide . The shrink method now also handles sparse features. Refactor of the distribution method. Added three new linear models: Lars , Huber and Perc . Dimensions can be shared across models using the key 'all' in bo_params[\"dimensions\"] . Assign hyperparameters to tune using the predefined dimensions. It's now possible to tune a custom number of layers for the MLP model. If multiple BO calls share the best score, the one with the shortest training time is selected as winner (instead of the first). Fixed a bug where the BO could fail when custom dimensions where defined. Fixed a bug where FeatureSelector could fail after repeated calls to fit. Fixed a bug where FeatureGenerator didn't pass the correct data indices to its output. Performance improvements for the custom pipeline. Minor documentation fixes.","title":"Version 4.11.0"},{"location":"changelog/v4.x.x/#version-4100","text":"Added the holdout data set to have an extra way of assessing a model's performance on a completely independent dataset. Read more in the user_guide . Complete rework of the ensemble models. Support for dataframe indexing. Read more in the user guide . New plot_parshap plot to detect overfitting features. The new dashboard method makes analyzing the models even easier using a dashboard app. The plot_feature_importance plot now also accepts estimators with coefficients. Added the transform method for models. Added the threshold parameter to the evaluate method. The reset_predictions method is deprecated in favour of the new clear method. Refactor of the model's full_train method. The merge method is available for all trainers. Improvements in the trainer's pipeline. Training scores are now also saved to the mlflow run. Trying to change the data in a branch after fitting a model with it now raises an exception. Fixed a bug where the columns of array inputs were not ordered correctly. Fixed a bug where branches did not correctly act case-insensitive. Fixed a bug where the export_pipeline method for models would not export the transformers in the correct branch.","title":"Version 4.10.0"},{"location":"changelog/v4.x.x/#version-491","text":"Changed the default cross-validation for hyperparameter tuning from 5 to 1 to avoid errors with deep learning models . Added clearer exception messages when a model's run failed. Fixed a bug where custom dimensions didn't show during hyperparameter tuning . Documentation improvements.","title":"Version 4.9.1"},{"location":"changelog/v4.x.x/#version-490","text":"Drop support of Python 3.6 . Added the HistGBM model. Improved print layout for hyperparameter tuning . The new available_models method returns an overview of the available predefined models. The calibrate and cross_validate methods can no longer be accessed from the trainers. The pipeline parameter for the prediction methods is deprecated. Improved visualization of the plot_rfecv , plot_successive_halving and plot_learning_curve methods. Sparse matrices are now accepted as input. Duplicate BO calls are no longer calculated. Improvement in performance of the RNN model. Refactor of the model's bo attribute. Predefined hyperparameters have been updated to be consistent with sklearn's API. Fixed a bug where custom scalers were ignored by the models. Fixed a bug where the BO of certain models would crash with custom hyperparameters. Fixed a bug where duplicate column names could be generated from a custom transformer. Documentation improvements.","title":"Version 4.9.0"},{"location":"changelog/v4.x.x/#version-480","text":"The Encoder class now directly handles unknown categories encountered during fitting. The Balancer and Encoder classes now accept custom estimators for the strategy parameter. The new merge method enables the user to merge multiple atom instances into one. The dtype shrinking is moved from atom's initializers to the shrink method. ATOM's custom pipeline now handles transformers fitted on a subset of the dataset. The column parameter in the distribution method is renamed to columns for continuity of the API. The mae criterion for the GBM model hyperparameter tuning is deprecated to be consistent with sklearn's API. Branches are now case-insensitive. Renaming a branch using an existing name now raises an exception. Fixed a bug where columns of type category broke the Imputer class. Fixed a bug where predictions of the Stacking ensemble crashed for branches with multiple transformers. The tables in the documentation now adapt to dark mode.","title":"Version 4.8.0"},{"location":"changelog/v4.x.x/#version-473","text":"Fixed a bug where the conda-forge recipe couldn't install properly.","title":"Version 4.7.3"},{"location":"changelog/v4.x.x/#version-472","text":"Fixed a bug where the pipeline failed for custom transformers that returned sparse matrices. Package requirements files are added to the installer.","title":"Version 4.7.2"},{"location":"changelog/v4.x.x/#version-471","text":"Fixed a bug where the pip installer failed. Fixed a bug where categorical columns also selected datetime columns.","title":"Version 4.7.1"},{"location":"changelog/v4.x.x/#version-470","text":"Launched our new slack channel! The new FeatureExtractor class extracts useful features from datetime columns. The new plot_det method plots a binary classifier's detection error tradeoff curve. The partial dependence plot is able to draw Individual Conditional Expectation (ICE) lines. The full traceback of exceptions encountered during training are now saved to the logger. ATOMClassifier and ATOMRegressor now convert the dtypes of the input data to the minimal allowed type for memory efficiency. The scoring method is renamed to evaluate to clarify its purpose. The column parameter in the apply method is renamed to columns for continuity of the API. Minor documentation improvements.","title":"Version 4.7.0"},{"location":"changelog/v4.x.x/#version-460","text":"Added the full_train method to retrieve an estimator trained on the complete dataset. The score method is now also able to calculate custom metrics on new data. Refactor of the Imputer class. Refactor of the Encoder class to avoid errors for unknown classes and allow the input of missing values. The clean method no longer automatically encodes the target column for regression tasks. Creating a branch using a models' acronym as name now raises an exception. Fixed a bug where CatBoost failed when early_stopping < 1. Fixed a bug where created pipelines had duplicated names.","title":"Version 4.6.0"},{"location":"changelog/v4.x.x/#version-450","text":"Support of NLP pipelines. Read more in the user guide . Integration of mlflow to track all models in the pipeline. Read more in the user guide . The new Normalizer class transforms features to a more Gaussian-like distribution. New cross_validate method to evaluate the robustness of a pipeline using cross_validation. New reset method to go back to atom's initial state. Added the Dummy model to compare other models with a simple baseline. New plot_wordcloud and plot_ngrams methods for text visualization. Plots now can return the figure object when display=None . The Pruner class can now able to drop outliers based on the selection of multiple strategies. The new shuffle parameter in atom's initializer determines whether to shuffle the dataset. The trainers no longer require you to specify a model using the models parameter. If left to default, all predefined models for that task are used. The apply method now accepts args and kwargs for the function. Refactor of the evaluate method. Refactor of the export_pipeline method. The parameters in the Cleaner class have been refactored to better describe their function. The train_sizes parameter in train_sizing now accepts integer values to automatically create equally distributed splits in the training set. Refactor of plot_pipeline to show models in the diagram as well. Refactor of the bagging parameter to the (more appropriate) name n_bootstrap . New option to exclude columns from a transformer adding ! before their name. Fixed a bug where the Pruner class failed if there were categorical columns in the dataset. Completely reworked documentation website.","title":"Version 4.5.0"},{"location":"changelog/v4.x.x/#version-440","text":"The drop method now allows the user to drop columns as part of the pipeline. New apply method to perform data transformations as function to the pipeline Added the status method to save an overview of atom's branches and models to the logger. Improved the output messages for the Imputer class. The dataset's columns can now be called directly from atom. The distribution and plot_distribution methods now ignore missing values. Fixed a bug where transformations could fail when columns were added to the dataset after initializing the pipeline. Fixed a bug where the Cleaner class didn't drop columns consisting entirely of missing values when drop_min_cardinality=True . Fixed a bug where the winning model wasn't displayed correctly. Refactored the way transformers are added or removed from predicting methods. Improved documentation.","title":"Version 4.4.0"},{"location":"changelog/v4.x.x/#version-430","text":"Possibility to add custom transformers to the pipeline. The export_pipeline utility method exports atom's current pipeline to a sklearn object. Use AutoML to automate the search for an optimized pipeline. New magic methods makes atom behave similarly to sklearn's Pipeline . All training approaches can now be combined in the same atom instance. New plot_scatter_matrix , plot_distribution and plot_qq plots for data inspection. Complete rework of all the shap plots to be consistent with their new API. Improvements for the Scaler and Pruner classes. The acronym for custom models now defaults to the capital letters in the class' __name__. Possibility to apply transformations on only a subset of the columns. Plots and methods now accept winner as model name. Fixed a bug where custom metrics didn't show the correct name. Fixed a bug where timers were not displayed correctly. Further compatibility with deep learning datasets. Large refactoring for performance optimization. Cleaner output of messages to the logger. Plots no longer show a default title. Added the AutoML example notebook. Minor bug fixes.","title":"Version 4.3.0"},{"location":"changelog/v4.x.x/#version-421","text":"Bug fix where there was memory leakage in successive halving and train sizing pipelines. The XGBoost , LightGBM and CatBoost packages can now be installed through the installer's extras_require under the name models , e.g. pip install -U atom-ml[models] . Improved documentation.","title":"Version 4.2.1"},{"location":"changelog/v4.x.x/#version-420","text":"Possibility to add custom models to the pipeline using ATOMModel . Compatibility with deep learning models. New branch system for different data pipelines. Read more in the user guide . Use the canvas contextmanager to draw multiple plots in one figure. New voting and stacking ensemble techniques. New get_class_weight utility method. New Sequential Feature Selection strategy for the FeatureSelector . Added the sample_weight parameter to the score method. New ways to initialize the data in the training instances. The n_rows parameter in ATOMLoader is deprecated in favour of the new input formats. The test_size parameter now also allows integer values. Renamed categories to classes to be consistent with sklearn's API. The class property now returns a pd.DataFrame of the number of rows per target class in the train, test and complete dataset. Possibility to add custom parameters to an estimator's fit method through est_params . The successive halving and train sizing approaches now both allow subsequent runs from atom without losing the information from previous runs. Bug fix where ATOMLoader wouldn't encode the target column during transformation. Added the Deep learning , Ensembles and Utilities example notebooks. Support for python 3.9 .","title":"Version 4.2.0"},{"location":"changelog/v4.x.x/#version-410","text":"New est_params parameter to customize the parameters in every model's estimator. Following skopt's API, the n_random_starts parameter to specify the number of random trials is deprecated in favour of n_initial_points . The Balancer class now allows you to use any of the strategies from imblearn . New utility attributes to inspect the dataset. Four new models: CatNB , CNB , ARD and RNN . Added the models section to the documentation. Small changes in log outputs. Bug fixes and performance improvements.","title":"Version 4.1.0"},{"location":"changelog/v4.x.x/#version-401","text":"Bug fix where the FeatureGenerator was not deterministic for a fixed random state. Bug fix where subsequent runs with the same metric failed. Added the license file to the package's installer. Typo fixes in documentation.","title":"Version 4.0.1"},{"location":"changelog/v4.x.x/#version-400","text":"Bayesian optimization package changed from GpyOpt to skopt . Complete revision of the model's hyperparameters. Four SHAP plots can now be called directly from an ATOM pipeline. Two new plots for regression tasks. New plot_pipeline and pipeline attribute to access all transformers. Possibility to determine transformer parameters per method. New calibration method and plot . Metrics can now be added as scorers or functions with signature metric(y, y_pred, **kwargs). Implementation of multi-metric runs. Possibility to choose which metric to plot. Early stopping for models that allow in-training evaluation. Added the ATOMLoader function to load any saved pickle instance. The \"remove\" strategy in the data cleaning parameters is deprecated in favour of \"drop\". Implemented the dfs strategy in FeatureGenerator . All training classes now inherit from BaseEstimator. Added multiple new example notebooks. Tests coverage up to 100%. Completely new documentation page. Bug fixes and performance improvements.","title":"Version 4.0.0"},{"location":"changelog/v5.x.x/","text":"Release history Version 5.0.0 New features Accelerate your pipelines on CPU with sklearnex . Read more in the user guide . New FeatureGrouper class to extract statistical features from similar groups. New create_app method to create a nice front-end for model predictions. New inverse_transform method for atom and models. API changes The gpu parameter is deprecated in favor of device and engine . Refactor of the Cleaner , Discretizer , Encoder and FeatureSelector classes. Refactor of all shap plots . Refactor of the apply method. Multidimensional datasets are no longer supported. Check the deep learning section of the user guide for guidance with such datasets. The drop method is removed from atom. Use the new apply method instead. The prediction methods can no longer be called from atom. The dashboard method for models is now called create_dashboard . Enhancements The models parameter in plot and utility methods now accepts model indices. The transform method now also transforms only y when X has a default value. The prediction methods now return pandas objects. Dependency versions are checked with originals after unpickling. Automatic generation of documentation from docstrings. Improvements in documentation display for mobile phones. New feature_importance attribute for models. Bug fixes The FeatureExtractor class no longer raises a warning for highly fragmented dataframes. Fixed a bug where models could not call the score function. The Encoder class no longer fails when the user provides ordinal values that are not present during fitting. Fixed a bug with the max_nan_rows parameter in the Imputer class. Fixed a bug where Tokenizer could fail when no ngrams were found.","title":"v5.x.x"},{"location":"changelog/v5.x.x/#release-history","text":"","title":"Release history"},{"location":"changelog/v5.x.x/#version-500","text":"New features Accelerate your pipelines on CPU with sklearnex . Read more in the user guide . New FeatureGrouper class to extract statistical features from similar groups. New create_app method to create a nice front-end for model predictions. New inverse_transform method for atom and models. API changes The gpu parameter is deprecated in favor of device and engine . Refactor of the Cleaner , Discretizer , Encoder and FeatureSelector classes. Refactor of all shap plots . Refactor of the apply method. Multidimensional datasets are no longer supported. Check the deep learning section of the user guide for guidance with such datasets. The drop method is removed from atom. Use the new apply method instead. The prediction methods can no longer be called from atom. The dashboard method for models is now called create_dashboard . Enhancements The models parameter in plot and utility methods now accepts model indices. The transform method now also transforms only y when X has a default value. The prediction methods now return pandas objects. Dependency versions are checked with originals after unpickling. Automatic generation of documentation from docstrings. Improvements in documentation display for mobile phones. New feature_importance attribute for models. Bug fixes The FeatureExtractor class no longer raises a warning for highly fragmented dataframes. Fixed a bug where models could not call the score function. The Encoder class no longer fails when the user provides ordinal values that are not present during fitting. Fixed a bug with the max_nan_rows parameter in the Imputer class. Fixed a bug where Tokenizer could fail when no ngrams were found.","title":"Version 5.0.0"},{"location":"user_guide/accelerating/","text":"Accelerating pipelines CPU acceleration ATOM uses sklearnex to accelerate sklearn applications and still have full conformance with its API. This tool can bring over 10-100X acceleration across a variety of transformers and models. Prerequisites Operating System: Linux (Ubuntu, Fedora, etc...) Windows 8.1+ macOS CPU: Processor must have x86 architecture. Processor must support at least one of SSE2, AVX, AVX2, AVX512 instruction sets. ARM* architecture is not supported. Libraries: sklearnex >=2021.6.3 (automatically installed with atom) Note Intel\u00ae processors provide better performance than other CPUs. Supported estimators Transformers Pruner (only for strategy=\"dbscan\") FeatureSelector (only for strategy=\"pca\" and dense datasets) Models ElasticNet K-Nearest Neighbors Kernel SVM Lasso Logistic Regression Ordinary Least Squares Random Forest Ridge (only for regression tasks) GPU acceleration Graphics Processing Units (GPUs) can significantly accelerate calculations for preprocessing step or training machine learning models. Training models involves compute-intensive matrix multiplications and other operations that can take advantage of a GPU's massively parallel architecture. Training on large datasets can take hours to run on a single processor. However, if you offload those tasks to a GPU, you can reduce training time to minutes instead. Training transformers and models in atom using a GPU is as easy as initializing the instance with parameter device=\"gpu\" . The device parameter accepts any string that follows the SYCL_DEVICE_FILTER filter selector. Examples are: device=\"cpu\" (use CPU) device=\"gpu\" (use default GPU) device=\"gpu:1\" (use second GPU) Use the engine parameter to choose between the cuML and sklearnex execution engines. The XGB , LGB and CatB models come with their own GPU engine. Setting device=\"gpu\" is sufficient to accelerate them with GPU, regardless of the engine parameter. Warning GPU accelerated estimators almost never support sparse datasets . Refer to their respective documentation to check which ones do. GPU accelerated estimators use slightly different hyperparameters than their CPU counterparts. ATOM does not support multi-GPU training. If there is more than one GPU on the machine and the device parameter does not specify which one to use, the first one is used by default. Example Train a model on a GPU yourself using Google Colab. Just click on the badge above and follow the notebook. Note two things: Make sure you've been allocated a Tesla T4, P4, or P100. If this is not the case (check it using !nvidia-smi ), reset the runtime (Runtime -> Factory reset runtime) until you get one. Setting up the environment and installing the necessary libraries may take quite some time (usually up to 15min). Prerequisites Operating System: Ubuntu 18.04/20.04 or CentOS 7/8 with gcc/++ 9.0+ Windows 10+ with WSL2 (see here a tutorial) GPU: For sklearnex: All Intel\u00ae integrated and discrete GPUs. For cuML: NVIDIA Pascal\u2122 or better with compute capability 6.0+ Drivers: For cuML: CUDA & NVIDIA Drivers of versions 11.0, 11.2, 11.4 or 11.5 For sklearnex: Intel\u00ae GPU drivers. Libraries: sklearnex >=2021.6.3 (automatically installed with atom) cuML >=22.08 Supported estimators Transformers Cleaner (only for cuML with encode_target=True) Discretizer (only for cuML with strategy!=\"custom\") Imputer (only for cuML with strat_num=\"knn\") Pruner (only for strategy=\"dbscan\") Scaler (only for cuML) Vectorizer (only for cuML) FeatureSelector (only for strategy=\"pca\" and dense datasets) Models Bernoulli Naive Bayes (only for cuML) CatBoost Categorical Naive Bayes (only for cuML) ElasticNet (only for cuML) Gaussian Naive Bayes (only for cuML) K-Nearest Neighbors Kernel SVM Lasso (only for cuML) LightGBM (requires extra installations ) Linear SVM (only for cuML) Logistic Regression Multinomial Naive Bayes (only for cuML) Ordinary Least Squares Random Forest Ridge (only for regression tasks) XGBoost","title":"Accelerating pipelines"},{"location":"user_guide/accelerating/#accelerating-pipelines","text":"","title":"Accelerating pipelines"},{"location":"user_guide/accelerating/#cpu-acceleration","text":"ATOM uses sklearnex to accelerate sklearn applications and still have full conformance with its API. This tool can bring over 10-100X acceleration across a variety of transformers and models.","title":"CPU acceleration"},{"location":"user_guide/accelerating/#prerequisites","text":"Operating System: Linux (Ubuntu, Fedora, etc...) Windows 8.1+ macOS CPU: Processor must have x86 architecture. Processor must support at least one of SSE2, AVX, AVX2, AVX512 instruction sets. ARM* architecture is not supported. Libraries: sklearnex >=2021.6.3 (automatically installed with atom) Note Intel\u00ae processors provide better performance than other CPUs.","title":"Prerequisites"},{"location":"user_guide/accelerating/#supported-estimators","text":"Transformers Pruner (only for strategy=\"dbscan\") FeatureSelector (only for strategy=\"pca\" and dense datasets) Models ElasticNet K-Nearest Neighbors Kernel SVM Lasso Logistic Regression Ordinary Least Squares Random Forest Ridge (only for regression tasks)","title":"Supported estimators"},{"location":"user_guide/accelerating/#gpu-acceleration","text":"Graphics Processing Units (GPUs) can significantly accelerate calculations for preprocessing step or training machine learning models. Training models involves compute-intensive matrix multiplications and other operations that can take advantage of a GPU's massively parallel architecture. Training on large datasets can take hours to run on a single processor. However, if you offload those tasks to a GPU, you can reduce training time to minutes instead. Training transformers and models in atom using a GPU is as easy as initializing the instance with parameter device=\"gpu\" . The device parameter accepts any string that follows the SYCL_DEVICE_FILTER filter selector. Examples are: device=\"cpu\" (use CPU) device=\"gpu\" (use default GPU) device=\"gpu:1\" (use second GPU) Use the engine parameter to choose between the cuML and sklearnex execution engines. The XGB , LGB and CatB models come with their own GPU engine. Setting device=\"gpu\" is sufficient to accelerate them with GPU, regardless of the engine parameter. Warning GPU accelerated estimators almost never support sparse datasets . Refer to their respective documentation to check which ones do. GPU accelerated estimators use slightly different hyperparameters than their CPU counterparts. ATOM does not support multi-GPU training. If there is more than one GPU on the machine and the device parameter does not specify which one to use, the first one is used by default. Example Train a model on a GPU yourself using Google Colab. Just click on the badge above and follow the notebook. Note two things: Make sure you've been allocated a Tesla T4, P4, or P100. If this is not the case (check it using !nvidia-smi ), reset the runtime (Runtime -> Factory reset runtime) until you get one. Setting up the environment and installing the necessary libraries may take quite some time (usually up to 15min).","title":"GPU acceleration"},{"location":"user_guide/accelerating/#prerequisites_1","text":"Operating System: Ubuntu 18.04/20.04 or CentOS 7/8 with gcc/++ 9.0+ Windows 10+ with WSL2 (see here a tutorial) GPU: For sklearnex: All Intel\u00ae integrated and discrete GPUs. For cuML: NVIDIA Pascal\u2122 or better with compute capability 6.0+ Drivers: For cuML: CUDA & NVIDIA Drivers of versions 11.0, 11.2, 11.4 or 11.5 For sklearnex: Intel\u00ae GPU drivers. Libraries: sklearnex >=2021.6.3 (automatically installed with atom) cuML >=22.08","title":"Prerequisites"},{"location":"user_guide/accelerating/#supported-estimators_1","text":"Transformers Cleaner (only for cuML with encode_target=True) Discretizer (only for cuML with strategy!=\"custom\") Imputer (only for cuML with strat_num=\"knn\") Pruner (only for strategy=\"dbscan\") Scaler (only for cuML) Vectorizer (only for cuML) FeatureSelector (only for strategy=\"pca\" and dense datasets) Models Bernoulli Naive Bayes (only for cuML) CatBoost Categorical Naive Bayes (only for cuML) ElasticNet (only for cuML) Gaussian Naive Bayes (only for cuML) K-Nearest Neighbors Kernel SVM Lasso (only for cuML) LightGBM (requires extra installations ) Linear SVM (only for cuML) Logistic Regression Multinomial Naive Bayes (only for cuML) Ordinary Least Squares Random Forest Ridge (only for regression tasks) XGBoost","title":"Supported estimators"},{"location":"user_guide/data_cleaning/","text":"Data cleaning More often than not, you'll need to do some data cleaning before fitting your dataset to a model. Usually, this involves importing different libraries and writing many lines of code. Since ATOM is all about fast exploration and experimentation, it provides various data cleaning classes to apply the most common transformations fast and easy. Note All of atom's data cleaning methods automatically adopt the relevant transformer attributes ( n_jobs , verbose , logger , random_state ) from atom. A different choice can be added as parameter to the method call, e.g. atom.scale(verbose=2) . Note Like the add method, the data cleaning methods accept the columns parameter to only transform a subset of the dataset's features, e.g. atom.scale(columns=[0, 1]) . Balancing the data One of the common issues found in datasets that are used for classification is imbalanced classes. Data imbalance usually reflects an unequal distribution of classes within a dataset. For example, in a credit card fraud detection dataset, most of the transactions are non-fraud, and a very few cases are fraud. This leaves us with a very unbalanced ratio of fraud vs non-fraud cases. The Balancer class can oversample the minority class or undersample the majority class using any of the transformers implemented in the imblearn package. It can be accessed from atom through the balance method. Standard data cleaning There are many data cleaning steps that are useful to perform on any dataset before modelling. These are general rules that apply almost on every use-case and every task. The Cleaner class is a convenient tool to apply such steps. It can be accessed from atom through the clean method. Use the class' parameters to choose which transformations to perform. The available steps are: Drop columns with specific data types. Strip categorical features from white spaces. Drop duplicate rows. Drop rows with missing values in the target column. Encode the target column. Binning numerical features Discretization (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes. Discretization is similar to constructing histograms for continuous data. However, histograms focus on counting features which fall into particular bins, whereas discretization focuses on assigning feature values to these bins. The Discretizer class can be used to bin continuous data into intervals. It can be accessed from atom through the discretize method. Encoding categorical features Many datasets contain categorical features. Their variables are typically stored as text values which represent various classes. Some examples include color (\u201cRed\u201d, \u201cYellow\u201d, \u201cBlue\u201d), size (\u201cSmall\u201d, \u201cMedium\u201d, \u201cLarge\u201d) or geographic designations (city or country). Regardless of what the value is used for, the challenge is determining how to use this data in the analysis. The majority of sklearn's models don't support direct manipulation of this kind of data. Use the Encoder class to encode categorical features to numerical values. It can be accessed from atom through the encode method. There are many strategies to encode categorical columns. The Encoder class applies one strategy or another depending on the number of classes in the column to be encoded. When there are only two, the values are encoded with 0 or 1. When there are more than two, the columns can be encoded using one-hot encoding or any other strategy of the category-encoders package, depending on the value of the max_onehot parameter. One-hot encodes the column making a dummy feature for every class. This approach preserves all the information but increases the size of the dataset considerably, making it often an undesirable strategy for high cardinality features. Other strategies like LeaveOneOut transform the column in place. Imputing missing values For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with ATOM's models which assume that all values in an array are numerical, and that all have and hold meaning. The Imputer class handles missing values in the dataset by either dropping or imputing the value. It can be accessed from atom through the impute method. Normalizing the feature set Use the Normalizer class to transform the feature set to follow a Normal (Gaussian)-like distribution. In general, data must be transformed when using models that assume normality in the residuals. Examples of such models are Logistic Regression , Linear Discriminant Analysis and Gaussian Naive Bayes . The class can be accessed from atom through the normalize method. Handling outliers When modelling, it is important to clean the data sample to ensure that the observations best represent the problem. Sometimes a dataset can contain extreme values that are outside the range of what is expected and unlike the other data. These are called outliers. Often, machine learning modelling and model skill in general can be improved by understanding and even removing these outlier samples. The Pruner class offers 7 different strategies to detect outliers (described hereunder). It can be accessed from atom through the prune method. z-score The z-score of a value in the dataset is defined as the number of standard deviations by which the value is above or below the mean of the column. Values above or below a certain threshold (specified with the parameter max_sigma ) are considered outliers. Note that, contrary to the rest of the strategies, this approach selects outlier values, not outlier samples! Because of this, it is possible to replace the outlier value instead of dropping the entire sample. Isolation Forest Uses a tree-based anomaly detection algorithm. It is based on modeling the normal data in such a way as to isolate anomalies that are both few and different in the feature space. Read more in sklearn's documentation . Elliptic Envelope If the input variables have a Gaussian distribution, then simple statistical methods can be used to detect outliers. For example, if the dataset has two input variables and both are Gaussian, the feature space forms a multidimensional Gaussian, and knowledge of this distribution can be used to identify values far from the distribution. This approach can be generalized by defining a hypersphere (ellipsoid) that covers the normal data, and data that falls outside this shape is considered an outlier. Read more in sklearn's documentation . Local Outlier Factor A simple approach to identifying outliers is to locate those examples that are far from the other examples in the feature space. This can work well for feature spaces with low dimensionality (few features) but becomes less reliable as the number of features is increased. The local outlier factor is a technique that attempts to harness the idea of nearest neighbors for outlier detection. Each example is assigned a score of how isolated or how likely it is to be outliers based on the size of its local neighborhood. Those examples with the largest score are more likely to be outliers. Read more in sklearn's documentation . One-class SVM The support vector machine algorithm, initially developed for binary classification tasks, can also be used for one-class classification. When modeling one class, the algorithm captures the density of the majority class and classifies examples on the extremes of the density function as outliers. This modification of SVM is referred to as One-Class SVM. Read more in sklearn's documentation . DBSCAN The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. Samples that lie outside any cluster are considered outliers. Read more in sklearn's documentation . OPTICS The OPTICS algorithm shares many similarities with the DBSCAN algorithm, and can be considered a generalization of DBSCAN that relaxes the eps requirement from a single value to a value range. The key difference between DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability graph, and a spot within the cluster ordering. These two attributes are assigned when the model is fitted, and are used to determine cluster membership. Read more in sklearn's documentation . Scaling the feature set Standardization of a dataset is a common requirement for many machine learning estimators; they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with zero mean and unit variance). The Scaler class let you quickly scale atom's dataset using one of sklearn's scalers. It can be accessed from atom through the scale method. Info All strategies can utilize GPU speed-up. Click here for further information about GPU implementation.","title":"Data cleaning"},{"location":"user_guide/data_cleaning/#data-cleaning","text":"More often than not, you'll need to do some data cleaning before fitting your dataset to a model. Usually, this involves importing different libraries and writing many lines of code. Since ATOM is all about fast exploration and experimentation, it provides various data cleaning classes to apply the most common transformations fast and easy. Note All of atom's data cleaning methods automatically adopt the relevant transformer attributes ( n_jobs , verbose , logger , random_state ) from atom. A different choice can be added as parameter to the method call, e.g. atom.scale(verbose=2) . Note Like the add method, the data cleaning methods accept the columns parameter to only transform a subset of the dataset's features, e.g. atom.scale(columns=[0, 1]) .","title":"Data cleaning"},{"location":"user_guide/data_cleaning/#balancing-the-data","text":"One of the common issues found in datasets that are used for classification is imbalanced classes. Data imbalance usually reflects an unequal distribution of classes within a dataset. For example, in a credit card fraud detection dataset, most of the transactions are non-fraud, and a very few cases are fraud. This leaves us with a very unbalanced ratio of fraud vs non-fraud cases. The Balancer class can oversample the minority class or undersample the majority class using any of the transformers implemented in the imblearn package. It can be accessed from atom through the balance method.","title":"Balancing the data"},{"location":"user_guide/data_cleaning/#standard-data-cleaning","text":"There are many data cleaning steps that are useful to perform on any dataset before modelling. These are general rules that apply almost on every use-case and every task. The Cleaner class is a convenient tool to apply such steps. It can be accessed from atom through the clean method. Use the class' parameters to choose which transformations to perform. The available steps are: Drop columns with specific data types. Strip categorical features from white spaces. Drop duplicate rows. Drop rows with missing values in the target column. Encode the target column.","title":"Standard data cleaning"},{"location":"user_guide/data_cleaning/#binning-numerical-features","text":"Discretization (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes. Discretization is similar to constructing histograms for continuous data. However, histograms focus on counting features which fall into particular bins, whereas discretization focuses on assigning feature values to these bins. The Discretizer class can be used to bin continuous data into intervals. It can be accessed from atom through the discretize method.","title":"Binning numerical features"},{"location":"user_guide/data_cleaning/#encoding-categorical-features","text":"Many datasets contain categorical features. Their variables are typically stored as text values which represent various classes. Some examples include color (\u201cRed\u201d, \u201cYellow\u201d, \u201cBlue\u201d), size (\u201cSmall\u201d, \u201cMedium\u201d, \u201cLarge\u201d) or geographic designations (city or country). Regardless of what the value is used for, the challenge is determining how to use this data in the analysis. The majority of sklearn's models don't support direct manipulation of this kind of data. Use the Encoder class to encode categorical features to numerical values. It can be accessed from atom through the encode method. There are many strategies to encode categorical columns. The Encoder class applies one strategy or another depending on the number of classes in the column to be encoded. When there are only two, the values are encoded with 0 or 1. When there are more than two, the columns can be encoded using one-hot encoding or any other strategy of the category-encoders package, depending on the value of the max_onehot parameter. One-hot encodes the column making a dummy feature for every class. This approach preserves all the information but increases the size of the dataset considerably, making it often an undesirable strategy for high cardinality features. Other strategies like LeaveOneOut transform the column in place.","title":"Encoding categorical features"},{"location":"user_guide/data_cleaning/#imputing-missing-values","text":"For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with ATOM's models which assume that all values in an array are numerical, and that all have and hold meaning. The Imputer class handles missing values in the dataset by either dropping or imputing the value. It can be accessed from atom through the impute method.","title":"Imputing missing values"},{"location":"user_guide/data_cleaning/#normalizing-the-feature-set","text":"Use the Normalizer class to transform the feature set to follow a Normal (Gaussian)-like distribution. In general, data must be transformed when using models that assume normality in the residuals. Examples of such models are Logistic Regression , Linear Discriminant Analysis and Gaussian Naive Bayes . The class can be accessed from atom through the normalize method.","title":"Normalizing the feature set"},{"location":"user_guide/data_cleaning/#handling-outliers","text":"When modelling, it is important to clean the data sample to ensure that the observations best represent the problem. Sometimes a dataset can contain extreme values that are outside the range of what is expected and unlike the other data. These are called outliers. Often, machine learning modelling and model skill in general can be improved by understanding and even removing these outlier samples. The Pruner class offers 7 different strategies to detect outliers (described hereunder). It can be accessed from atom through the prune method. z-score The z-score of a value in the dataset is defined as the number of standard deviations by which the value is above or below the mean of the column. Values above or below a certain threshold (specified with the parameter max_sigma ) are considered outliers. Note that, contrary to the rest of the strategies, this approach selects outlier values, not outlier samples! Because of this, it is possible to replace the outlier value instead of dropping the entire sample. Isolation Forest Uses a tree-based anomaly detection algorithm. It is based on modeling the normal data in such a way as to isolate anomalies that are both few and different in the feature space. Read more in sklearn's documentation . Elliptic Envelope If the input variables have a Gaussian distribution, then simple statistical methods can be used to detect outliers. For example, if the dataset has two input variables and both are Gaussian, the feature space forms a multidimensional Gaussian, and knowledge of this distribution can be used to identify values far from the distribution. This approach can be generalized by defining a hypersphere (ellipsoid) that covers the normal data, and data that falls outside this shape is considered an outlier. Read more in sklearn's documentation . Local Outlier Factor A simple approach to identifying outliers is to locate those examples that are far from the other examples in the feature space. This can work well for feature spaces with low dimensionality (few features) but becomes less reliable as the number of features is increased. The local outlier factor is a technique that attempts to harness the idea of nearest neighbors for outlier detection. Each example is assigned a score of how isolated or how likely it is to be outliers based on the size of its local neighborhood. Those examples with the largest score are more likely to be outliers. Read more in sklearn's documentation . One-class SVM The support vector machine algorithm, initially developed for binary classification tasks, can also be used for one-class classification. When modeling one class, the algorithm captures the density of the majority class and classifies examples on the extremes of the density function as outliers. This modification of SVM is referred to as One-Class SVM. Read more in sklearn's documentation . DBSCAN The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. Samples that lie outside any cluster are considered outliers. Read more in sklearn's documentation . OPTICS The OPTICS algorithm shares many similarities with the DBSCAN algorithm, and can be considered a generalization of DBSCAN that relaxes the eps requirement from a single value to a value range. The key difference between DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability graph, and a spot within the cluster ordering. These two attributes are assigned when the model is fitted, and are used to determine cluster membership. Read more in sklearn's documentation .","title":"Handling outliers"},{"location":"user_guide/data_cleaning/#scaling-the-feature-set","text":"Standardization of a dataset is a common requirement for many machine learning estimators; they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with zero mean and unit variance). The Scaler class let you quickly scale atom's dataset using one of sklearn's scalers. It can be accessed from atom through the scale method. Info All strategies can utilize GPU speed-up. Click here for further information about GPU implementation.","title":"Scaling the feature set"},{"location":"user_guide/data_management/","text":"Data management Data sets ATOM is designed to work around one single dataset: the one with which atom is initialized. This is the dataset you want to explore, transform, and use for model training and validation. ATOM differentiates three different data sets: The training set is usually the largest of the data sets. As the name suggests, this set is used to train the pipeline. During hyperparameter tuning , only the training set is used to fit and evaluate the estimator in every call. The training set in the current branch can be accessed through the train attribute. It's features and target can be accessed through X_train and y_train respectively. The test set is used to evaluate the models. The model scores on this set give an indication on how the model performs on new data. The test set can be accessed through the test attribute. It's features and target can be accessed through X_test and y_test respectively. The holdout set is an optional, separate set that should only be used to evaluate the final model's performance. Create this set when you are going to use the test set for an intermediate validation step. The holdout set is immediately set apart during initialization and is not considered part of atom's dataset (the dataset attribute only returns the train and test sets). The holdout set is left untouched until predictions are made on it, i.e. it does not undergo any pipeline transformations. The holdout set is stored in atom's holdout attribute. It's features and target can not be accessed separately. See here an example that shows how to use the holdout data set. The data can be provided in different formats. If the data sets are not specified beforehand, you can input the features and target separately or together: X X, y Remember to use the y parameter to indicate the target column in X when using the first option. If not specified, the last column in X is used as target. In both these cases, the size of the sets are defined using the test_size and holdout_size parameters. Note that the splits are made after the subsample of the dataset with the n_rows parameter (when not left to its default value). If you already have the separate data sets, provide them using one of the following formats: train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) The input data is always converted internally to a pandas dataframe , if it isn't one already. The column names should always be strings. If they are not, atom changes their type at initialization. If no column names are provided, default names are given of the form X[N-1] , where N stands for the n-th feature in the dataset. Indexing By default, atom resets the dataframe's index after initialization and after every transformation in the pipeline. To avoid this, specify the index parameter. If the dataset has an 'identifier' column, it is useful to use it as index for two reasons: An identifier doesn't usually contain any useful information on the target column, and should therefore be removed before training. Predictions of specific rows can be accessed through their index. Sparse datasets If atom is initialized using a scipy sparse matrix, it is converted internally to a dataframe of sparse columns. Read more about pandas' sparse data structures here . The same conversion takes place when a transformer returns a sparse matrix, like for example, the Vectorizer . Note that ATOM considers a dataset to be sparse if any of the columns is sparse. A dataset can only benefit from sparsity when all its columns are sparse, hence mixing sparse and non-sparse columns is not recommended and can cause estimators to decrease their training speed or even crash. Use the shrink method to convert dense features to sparse and the available_models method to check which models have native support for sparse matrices. Click here to see an example that uses sparse data. Branches You might want to compare how a model performs on a dataset transformed through multiple pipelines, each using different transformers. For example, on one pipeline with an undersampling strategy and the other with an oversampling strategy. To be able to do this, ATOM has the branching system. The branching system helps the user to manage multiple data pipelines within the same atom instance. Branches are created and accessed through atom's branch property. A branch contains a specific pipeline, the dataset transformed through that pipeline, and all data and utility attributes that refer to that dataset. Transformers and models called from atom use the dataset in the current branch, as well as data attributes such as atom.dataset . Use the branch's __repr__ to get an overview of the transformers in the branch. It's not allowed to change the data in a branch after fitting a model with it. Doing this would cause unexpected model behaviour and break down the plotting methods. Instead, create a new branch for every unique pipeline. By default, atom starts with one branch called \"master\". To start a new branch, set a new name to the property, e.g. atom.branch = \"undersample\" . This will create a new branch from the current one. To create a branch from any other branch type \"_from_\" between the new name and the branch from which to split, e.g. atom.branch = \"oversample_from_master\" will create branch \"oversample\" from branch \"master\", even if the current branch is \"undersample\". To switch between existing branches, just type the name of the desired branch, e.g. atom.branch = \"master\" brings you back to the master branch. Note that every branch contains a unique copy of the whole dataset! Creating many branches can cause memory issues for large datasets. See the Imbalanced datasets or Feature engineering examples for branching use cases. Warning Always create a new branch if you want to change the dataset after fitting a model! Figure 1. Diagram of a possible branch system to compare an oversampling with an undersampling pipeline. The branch class has the following methods. Memory considerations An atom instance stores one copy of the dataframe in each branch, and one copy of the initial dataset with which the instance is initialized (this copy is necessary to avoid data leakage during hyperparameter tuning and for some specific methods like cross_validate and reset ). This initial copy is created as soon as there are no branches in the initial state (usually after calling the first data transformation) and it's stored in an internal branch called og (original). The og branch is not accessible by the user. If the dataset is occupying too much memory, consider using the shrink method to convert the dtypes to their smallest possible matching dtype. Apart from the dataset itself, a model's prediction attributes (e.g. atom.lr.predict_proba_train ), metric scores and shap values are also stored as attributes of the model to avoid having to recalculate them every time they are needed. This data can occupy a considerable amount of memory for large datasets. You can delete all these attributes using the clear method in order to free some memory before saving the class. Data transformations Performing data transformations is a common requirement of many datasets before they are ready to be ingested by a model. ATOM provides various classes to apply data cleaning and feature engineering transformations to the data. This tooling should be able to help you apply most of the typically needed transformations to get the data ready for modelling. For further fine-tuning, it's also possible to transform the data using custom transformers (see the add method) or through a function (see the apply method). Remember that all transformations are only applied to the dataset in the current branch.","title":"Data management"},{"location":"user_guide/data_management/#data-management","text":"","title":"Data management"},{"location":"user_guide/data_management/#data-sets","text":"ATOM is designed to work around one single dataset: the one with which atom is initialized. This is the dataset you want to explore, transform, and use for model training and validation. ATOM differentiates three different data sets: The training set is usually the largest of the data sets. As the name suggests, this set is used to train the pipeline. During hyperparameter tuning , only the training set is used to fit and evaluate the estimator in every call. The training set in the current branch can be accessed through the train attribute. It's features and target can be accessed through X_train and y_train respectively. The test set is used to evaluate the models. The model scores on this set give an indication on how the model performs on new data. The test set can be accessed through the test attribute. It's features and target can be accessed through X_test and y_test respectively. The holdout set is an optional, separate set that should only be used to evaluate the final model's performance. Create this set when you are going to use the test set for an intermediate validation step. The holdout set is immediately set apart during initialization and is not considered part of atom's dataset (the dataset attribute only returns the train and test sets). The holdout set is left untouched until predictions are made on it, i.e. it does not undergo any pipeline transformations. The holdout set is stored in atom's holdout attribute. It's features and target can not be accessed separately. See here an example that shows how to use the holdout data set. The data can be provided in different formats. If the data sets are not specified beforehand, you can input the features and target separately or together: X X, y Remember to use the y parameter to indicate the target column in X when using the first option. If not specified, the last column in X is used as target. In both these cases, the size of the sets are defined using the test_size and holdout_size parameters. Note that the splits are made after the subsample of the dataset with the n_rows parameter (when not left to its default value). If you already have the separate data sets, provide them using one of the following formats: train, test train, test, holdout X_train, X_test, y_train, y_test X_train, X_test, X_holdout, y_train, y_test, y_holdout (X_train, y_train), (X_test, y_test) (X_train, y_train), (X_test, y_test), (X_holdout, y_holdout) The input data is always converted internally to a pandas dataframe , if it isn't one already. The column names should always be strings. If they are not, atom changes their type at initialization. If no column names are provided, default names are given of the form X[N-1] , where N stands for the n-th feature in the dataset.","title":"Data sets"},{"location":"user_guide/data_management/#indexing","text":"By default, atom resets the dataframe's index after initialization and after every transformation in the pipeline. To avoid this, specify the index parameter. If the dataset has an 'identifier' column, it is useful to use it as index for two reasons: An identifier doesn't usually contain any useful information on the target column, and should therefore be removed before training. Predictions of specific rows can be accessed through their index.","title":"Indexing"},{"location":"user_guide/data_management/#sparse-datasets","text":"If atom is initialized using a scipy sparse matrix, it is converted internally to a dataframe of sparse columns. Read more about pandas' sparse data structures here . The same conversion takes place when a transformer returns a sparse matrix, like for example, the Vectorizer . Note that ATOM considers a dataset to be sparse if any of the columns is sparse. A dataset can only benefit from sparsity when all its columns are sparse, hence mixing sparse and non-sparse columns is not recommended and can cause estimators to decrease their training speed or even crash. Use the shrink method to convert dense features to sparse and the available_models method to check which models have native support for sparse matrices. Click here to see an example that uses sparse data.","title":"Sparse datasets"},{"location":"user_guide/data_management/#branches","text":"You might want to compare how a model performs on a dataset transformed through multiple pipelines, each using different transformers. For example, on one pipeline with an undersampling strategy and the other with an oversampling strategy. To be able to do this, ATOM has the branching system. The branching system helps the user to manage multiple data pipelines within the same atom instance. Branches are created and accessed through atom's branch property. A branch contains a specific pipeline, the dataset transformed through that pipeline, and all data and utility attributes that refer to that dataset. Transformers and models called from atom use the dataset in the current branch, as well as data attributes such as atom.dataset . Use the branch's __repr__ to get an overview of the transformers in the branch. It's not allowed to change the data in a branch after fitting a model with it. Doing this would cause unexpected model behaviour and break down the plotting methods. Instead, create a new branch for every unique pipeline. By default, atom starts with one branch called \"master\". To start a new branch, set a new name to the property, e.g. atom.branch = \"undersample\" . This will create a new branch from the current one. To create a branch from any other branch type \"_from_\" between the new name and the branch from which to split, e.g. atom.branch = \"oversample_from_master\" will create branch \"oversample\" from branch \"master\", even if the current branch is \"undersample\". To switch between existing branches, just type the name of the desired branch, e.g. atom.branch = \"master\" brings you back to the master branch. Note that every branch contains a unique copy of the whole dataset! Creating many branches can cause memory issues for large datasets. See the Imbalanced datasets or Feature engineering examples for branching use cases. Warning Always create a new branch if you want to change the dataset after fitting a model! Figure 1. Diagram of a possible branch system to compare an oversampling with an undersampling pipeline. The branch class has the following methods.","title":"Branches"},{"location":"user_guide/data_management/#memory-considerations","text":"An atom instance stores one copy of the dataframe in each branch, and one copy of the initial dataset with which the instance is initialized (this copy is necessary to avoid data leakage during hyperparameter tuning and for some specific methods like cross_validate and reset ). This initial copy is created as soon as there are no branches in the initial state (usually after calling the first data transformation) and it's stored in an internal branch called og (original). The og branch is not accessible by the user. If the dataset is occupying too much memory, consider using the shrink method to convert the dtypes to their smallest possible matching dtype. Apart from the dataset itself, a model's prediction attributes (e.g. atom.lr.predict_proba_train ), metric scores and shap values are also stored as attributes of the model to avoid having to recalculate them every time they are needed. This data can occupy a considerable amount of memory for large datasets. You can delete all these attributes using the clear method in order to free some memory before saving the class.","title":"Memory considerations"},{"location":"user_guide/data_management/#data-transformations","text":"Performing data transformations is a common requirement of many datasets before they are ready to be ingested by a model. ATOM provides various classes to apply data cleaning and feature engineering transformations to the data. This tooling should be able to help you apply most of the typically needed transformations to get the data ready for modelling. For further fine-tuning, it's also possible to transform the data using custom transformers (see the add method) or through a function (see the apply method). Remember that all transformations are only applied to the dataset in the current branch.","title":"Data transformations"},{"location":"user_guide/feature_engineering/","text":"Feature engineering Feature engineering is the process of creating new features from the existing ones, in order to capture relationships with the target column that the first set of features didn't have on their own. This process is very important to improve the performance of machine learning algorithms. Although feature engineering works best when the data scientist applies use-case specific transformations, there are ways to do this in an automated manner, without prior domain knowledge. One of the problems of creating new features without human expert intervention, is that many of the newly created features can be useless, i.e. they do not help the algorithm to make better predictions. Even worse, having useless features can drop your performance. To avoid this, we perform feature selection, a process in which we select the relevant features in the dataset. See the Feature engineering example. Note All of atom's feature engineering methods automatically adopt the relevant transformer attributes ( n_jobs , verbose , logger , random_state ) from atom. A different choice can be added as parameter to the method call, e.g. atom.feature_selection(\"pca\", n_features=10, random_state=2) . Note Like the add method, the feature engineering methods accept the columns parameter to only transform a subset of the dataset's features, e.g. atom.feature_selection(\"pca\", n_features=10, columns=slice(5, 15)) . Extracting datetime features Features that contain dates or timestamps can not be directly ingested by models since they are not strictly numerical. Encoding them as categorical features is not an option since the encoding does not capture the relationship between the different moments in time. The FeatureExtractor class creates new features extracting datetime elements (e.g. day, month, year, hour...) from the columns. It can be accessed from atom through the feature_extraction method. The new features are named equally to the column from which they are extracted, followed by an underscore and the datetime element they create, e.g. x0_day for the day element of x0 . Note that many time features have a cyclic pattern, e.g. after Sunday comes Monday. This means that if we would encode the days of the week from 0 to 6, we would lose that relation. A common method used to encode cyclical features is to transform the data into two dimensions using a sine and cosine transformation: \\[ x_{sin} = sin\\left(\\frac{2\\pi * x}{max(x)}\\right) \\] \\[ x_{cos} = cos\\left(\\frac{2\\pi * x}{max(x)}\\right) \\] The resulting features have their names followed by sin or cos, e.g. x0_day_sin and x0_day_cos . The datetime elements that can be encoded in a cyclic fashion are: microsecond, second, minute, hour, weekday, day, day_of_year, month and quarter. Note that decision trees based algorithms build their split rules according to one feature at a time. This means that they will fail to correctly process cyclic features since the sin/cos values are expected to be considered as one single coordinate system. Use the fmt parameter to specify your feature's format in case the column is categorical. The FeatureExtractor class will convert the column to the datetime dtype before extracting the specified features. Click here for an overview of the available formats. Generating new features The FeatureGenerator class creates new non-linear features based on the original feature set. It can be accessed from atom through the feature_generation method. You can choose between two strategies: Deep Feature Synthesis and Genetic Feature Generation. Deep Feature Synthesis Deep feature synthesis (DFS) applies the selected operators on the features in the dataset. For example, if the operator is \"log\", it will create the new feature LOG(old_feature) and if the operator is \"mul\", it will create the new feature old_feature_1 x old_feature_2 . The operators can be chosen through the operators parameter. Choose from: add : Take the sum of two features. sub: Subtract two features from each other. mul: Multiply two features with each other. div: Divide two features with each other. abs: Calculate the absolute value of a feature. srqt: Calculate the square root of a feature. log: Calculate the natural logarithm of a feature. sin: Calculate the sine of a feature. cos: Calculate the cosine of a feature. tan: Calculate the tangent of a feature. ATOM's implementation of DFS uses the featuretools package. Genetic Feature Generation Genetic feature generation (GFG) uses genetic programming , a branch of evolutionary programming, to determine which features are successful and create new ones based on those. Where dfs can be seen as some kind of \"brute force\" for feature engineering, gfg tries to improve its features with every generation of the algorithm. gfg uses the same operators as dfs, but instead of only applying the transformations once, it evolves them further, creating nested structures of combinations of features. The new features are given the name feature_n , where n stands for the n-th feature in the dataset. You can access the genetic feature's fitness and description (how they are calculated) through the genetic_features attribute. ATOM uses the SymbolicTransformer class from the gplearn package for the genetic algorithm. Read more about this implementation here . Grouping similar features When your dataset contains many similar features corresponding to a certain natural group or entity, it's possible to replace these features for a handful of them, that should capture the relations of the group, in order to lose as little information as possible. To achieve this, the FeatureGrouper class computes certain statistical properties that describe the group's distribution, like the mean or the median, and replaces the columns with the result of these statistical calculations over every row in the dataset. The goal of this approach is to reduce the number of columns in the dataset, avoiding the curse of dimensionality . Selecting useful features The FeatureSelector class provides tooling to select the relevant features from a dataset. It can be accessed from atom through the feature_selection method. Standard strategies Univariate Univariate feature selection works by selecting the best features based on univariate statistical F-test. The test is provided via the solver parameter. It takes any function taking two arrays (X, y), and returning arrays (scores, p-values). Read more in sklearn's documentation . Principal Components Analysis Applying PCA reduces the dimensionality of the dataset by maximizing the variance of each dimension. The new features are called pca0 , pca1 , etc... PCA can be applied in three ways: If the data is dense (i.e. not sparse), the estimator used is PCA . Before fitting the transformer, the data is scaled to mean=0 and std=1 if it wasn't already. Read more in sklearn's documentation . If the data is [sparse][sparse datasets] (often the case for term-document matrices, see Vectorizer ), the estimator used is TruncatedSVD . Read more in sklearn's documentation . If engine=\"cuml\" is enabled, the estimator used is cuml's PCA . There's no cuml implementation available for sparse data. Selection from model SFM uses an estimator with feature_importances_ or coef_ attributes to select the best features in a dataset based on importance weights. The estimator is provided through the solver parameter and can be already fitted. ATOM allows you to use one its predefined models , e.g. solver=\"RF\" . If you didn't call the FeatureSelector through atom, don't forget to indicate the estimator's task adding _class or _reg after the name, e.g. RF_class to use a random forest classifier. Read more in sklearn's documentation . Sequential Feature Selection Sequential feature selection adds (forward selection) or removes (backward selection) features to form a feature subset in a greedy fashion. At each stage, this estimator chooses the best feature to add or remove based on the cross-validation score of an estimator. Read more in sklearn's documentation . Recursive Feature Elimination Select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features, and the importance of each feature is obtained either through a coef_ or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached. Note that, since RFE needs to fit the model again every iteration, this method can be fairly slow. RFECV applies the same algorithm as RFE but uses a cross-validated metric (under the scoring parameter, see RFECV ) to assess every step's performance. Also, where RFE returns the number of features selected by n_features , RFECV returns the number of features that achieved the optimal score on the specified metric. Note that this is not always equal to the amount specified by n_features . Read more in sklearn's documentation . Advanced strategies The following strategies are a collection of nature-inspired optimization algorithms that maximize an objective function. If not manually specified, the function calculates the cross-validated score of a model on the data. Use the scoring parameter (not present in description, part of kwargs) to specify the metric to optimize on. Particle Swarm Optimization Particle Swarm Optimization (PSO) optimizes a problem by having a population of candidate solutions (particles), and moving them around in the search-space according to simple mathematical formula over the particle's position and velocity. Each particle's movement is influenced by its local best known position, but is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions. Read more here . Harris Hawks Optimization Harris Hawks Optimization (HHO) mimics the action and reaction of Hawk's team collaboration hunting in nature and prey escaping to discover the solutions of the single-objective problem. Read more here . Grey Wolf Optimization The Grey Wolf Optimizer (GWO) mimics the leadership hierarchy and hunting mechanism of grey wolves in nature. Four types of grey wolves such as alpha, beta, delta, and omega are employed for simulating the leadership hierarchy. In addition, three main steps of hunting, searching for prey, encircling prey, and attacking prey, are implemented to perform optimization. Read more here . Dragonfly Optimization The Dragonfly Algorithm (DFO) algorithm originates from static and dynamic swarming behaviours. These two swarming behaviours are very similar to the two main phases of optimization using meta-heuristics: exploration and exploitation. Dragonflies create sub swarms and fly over different areas in a static swarm, which is the main objective of the exploration phase. In the static swarm, however, dragonflies fly in bigger swarms and along one direction, which is favourable in the exploitation phase. Read more here . Genetic Optimization Genetic Optimization is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms. Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection. Read more here . Other selection methods Removing features with low or high variance Variance is the expectation of the squared deviation of a random variable from its mean. Features with low variance have many values repeated, which means the model can't learn much from them. In a similar way, features with very high variance have very few values repeated, which makes it also difficult for a model to learn from this feature. FeatureSelector removes a categorical feature when the maximum number of occurrences for any value is below min_repeated or when the same value is repeated in at least max_repeated fraction of the rows. The default option is to remove a feature if all values in it are either different or exactly the same. Removing features with multi-collinearity Two features that are highly correlated are redundant, i.e. two will not contribute more to the model than only one of them. FeatureSelector will drop a feature that has a Pearson correlation coefficient larger than max_correlation with another feature. A correlation of 1 means the two columns are equal. A dataframe of the removed features and their correlation values can be accessed through the collinear attribute.","title":"Feature engineering"},{"location":"user_guide/feature_engineering/#feature-engineering","text":"Feature engineering is the process of creating new features from the existing ones, in order to capture relationships with the target column that the first set of features didn't have on their own. This process is very important to improve the performance of machine learning algorithms. Although feature engineering works best when the data scientist applies use-case specific transformations, there are ways to do this in an automated manner, without prior domain knowledge. One of the problems of creating new features without human expert intervention, is that many of the newly created features can be useless, i.e. they do not help the algorithm to make better predictions. Even worse, having useless features can drop your performance. To avoid this, we perform feature selection, a process in which we select the relevant features in the dataset. See the Feature engineering example. Note All of atom's feature engineering methods automatically adopt the relevant transformer attributes ( n_jobs , verbose , logger , random_state ) from atom. A different choice can be added as parameter to the method call, e.g. atom.feature_selection(\"pca\", n_features=10, random_state=2) . Note Like the add method, the feature engineering methods accept the columns parameter to only transform a subset of the dataset's features, e.g. atom.feature_selection(\"pca\", n_features=10, columns=slice(5, 15)) .","title":"Feature engineering"},{"location":"user_guide/feature_engineering/#extracting-datetime-features","text":"Features that contain dates or timestamps can not be directly ingested by models since they are not strictly numerical. Encoding them as categorical features is not an option since the encoding does not capture the relationship between the different moments in time. The FeatureExtractor class creates new features extracting datetime elements (e.g. day, month, year, hour...) from the columns. It can be accessed from atom through the feature_extraction method. The new features are named equally to the column from which they are extracted, followed by an underscore and the datetime element they create, e.g. x0_day for the day element of x0 . Note that many time features have a cyclic pattern, e.g. after Sunday comes Monday. This means that if we would encode the days of the week from 0 to 6, we would lose that relation. A common method used to encode cyclical features is to transform the data into two dimensions using a sine and cosine transformation: \\[ x_{sin} = sin\\left(\\frac{2\\pi * x}{max(x)}\\right) \\] \\[ x_{cos} = cos\\left(\\frac{2\\pi * x}{max(x)}\\right) \\] The resulting features have their names followed by sin or cos, e.g. x0_day_sin and x0_day_cos . The datetime elements that can be encoded in a cyclic fashion are: microsecond, second, minute, hour, weekday, day, day_of_year, month and quarter. Note that decision trees based algorithms build their split rules according to one feature at a time. This means that they will fail to correctly process cyclic features since the sin/cos values are expected to be considered as one single coordinate system. Use the fmt parameter to specify your feature's format in case the column is categorical. The FeatureExtractor class will convert the column to the datetime dtype before extracting the specified features. Click here for an overview of the available formats.","title":"Extracting datetime features"},{"location":"user_guide/feature_engineering/#generating-new-features","text":"The FeatureGenerator class creates new non-linear features based on the original feature set. It can be accessed from atom through the feature_generation method. You can choose between two strategies: Deep Feature Synthesis and Genetic Feature Generation. Deep Feature Synthesis Deep feature synthesis (DFS) applies the selected operators on the features in the dataset. For example, if the operator is \"log\", it will create the new feature LOG(old_feature) and if the operator is \"mul\", it will create the new feature old_feature_1 x old_feature_2 . The operators can be chosen through the operators parameter. Choose from: add : Take the sum of two features. sub: Subtract two features from each other. mul: Multiply two features with each other. div: Divide two features with each other. abs: Calculate the absolute value of a feature. srqt: Calculate the square root of a feature. log: Calculate the natural logarithm of a feature. sin: Calculate the sine of a feature. cos: Calculate the cosine of a feature. tan: Calculate the tangent of a feature. ATOM's implementation of DFS uses the featuretools package. Genetic Feature Generation Genetic feature generation (GFG) uses genetic programming , a branch of evolutionary programming, to determine which features are successful and create new ones based on those. Where dfs can be seen as some kind of \"brute force\" for feature engineering, gfg tries to improve its features with every generation of the algorithm. gfg uses the same operators as dfs, but instead of only applying the transformations once, it evolves them further, creating nested structures of combinations of features. The new features are given the name feature_n , where n stands for the n-th feature in the dataset. You can access the genetic feature's fitness and description (how they are calculated) through the genetic_features attribute. ATOM uses the SymbolicTransformer class from the gplearn package for the genetic algorithm. Read more about this implementation here .","title":"Generating new features"},{"location":"user_guide/feature_engineering/#grouping-similar-features","text":"When your dataset contains many similar features corresponding to a certain natural group or entity, it's possible to replace these features for a handful of them, that should capture the relations of the group, in order to lose as little information as possible. To achieve this, the FeatureGrouper class computes certain statistical properties that describe the group's distribution, like the mean or the median, and replaces the columns with the result of these statistical calculations over every row in the dataset. The goal of this approach is to reduce the number of columns in the dataset, avoiding the curse of dimensionality .","title":"Grouping similar features"},{"location":"user_guide/feature_engineering/#selecting-useful-features","text":"The FeatureSelector class provides tooling to select the relevant features from a dataset. It can be accessed from atom through the feature_selection method.","title":"Selecting useful features"},{"location":"user_guide/feature_engineering/#standard-strategies","text":"Univariate Univariate feature selection works by selecting the best features based on univariate statistical F-test. The test is provided via the solver parameter. It takes any function taking two arrays (X, y), and returning arrays (scores, p-values). Read more in sklearn's documentation . Principal Components Analysis Applying PCA reduces the dimensionality of the dataset by maximizing the variance of each dimension. The new features are called pca0 , pca1 , etc... PCA can be applied in three ways: If the data is dense (i.e. not sparse), the estimator used is PCA . Before fitting the transformer, the data is scaled to mean=0 and std=1 if it wasn't already. Read more in sklearn's documentation . If the data is [sparse][sparse datasets] (often the case for term-document matrices, see Vectorizer ), the estimator used is TruncatedSVD . Read more in sklearn's documentation . If engine=\"cuml\" is enabled, the estimator used is cuml's PCA . There's no cuml implementation available for sparse data. Selection from model SFM uses an estimator with feature_importances_ or coef_ attributes to select the best features in a dataset based on importance weights. The estimator is provided through the solver parameter and can be already fitted. ATOM allows you to use one its predefined models , e.g. solver=\"RF\" . If you didn't call the FeatureSelector through atom, don't forget to indicate the estimator's task adding _class or _reg after the name, e.g. RF_class to use a random forest classifier. Read more in sklearn's documentation . Sequential Feature Selection Sequential feature selection adds (forward selection) or removes (backward selection) features to form a feature subset in a greedy fashion. At each stage, this estimator chooses the best feature to add or remove based on the cross-validation score of an estimator. Read more in sklearn's documentation . Recursive Feature Elimination Select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features, and the importance of each feature is obtained either through a coef_ or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached. Note that, since RFE needs to fit the model again every iteration, this method can be fairly slow. RFECV applies the same algorithm as RFE but uses a cross-validated metric (under the scoring parameter, see RFECV ) to assess every step's performance. Also, where RFE returns the number of features selected by n_features , RFECV returns the number of features that achieved the optimal score on the specified metric. Note that this is not always equal to the amount specified by n_features . Read more in sklearn's documentation .","title":"Standard strategies"},{"location":"user_guide/feature_engineering/#advanced-strategies","text":"The following strategies are a collection of nature-inspired optimization algorithms that maximize an objective function. If not manually specified, the function calculates the cross-validated score of a model on the data. Use the scoring parameter (not present in description, part of kwargs) to specify the metric to optimize on. Particle Swarm Optimization Particle Swarm Optimization (PSO) optimizes a problem by having a population of candidate solutions (particles), and moving them around in the search-space according to simple mathematical formula over the particle's position and velocity. Each particle's movement is influenced by its local best known position, but is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions. Read more here . Harris Hawks Optimization Harris Hawks Optimization (HHO) mimics the action and reaction of Hawk's team collaboration hunting in nature and prey escaping to discover the solutions of the single-objective problem. Read more here . Grey Wolf Optimization The Grey Wolf Optimizer (GWO) mimics the leadership hierarchy and hunting mechanism of grey wolves in nature. Four types of grey wolves such as alpha, beta, delta, and omega are employed for simulating the leadership hierarchy. In addition, three main steps of hunting, searching for prey, encircling prey, and attacking prey, are implemented to perform optimization. Read more here . Dragonfly Optimization The Dragonfly Algorithm (DFO) algorithm originates from static and dynamic swarming behaviours. These two swarming behaviours are very similar to the two main phases of optimization using meta-heuristics: exploration and exploitation. Dragonflies create sub swarms and fly over different areas in a static swarm, which is the main objective of the exploration phase. In the static swarm, however, dragonflies fly in bigger swarms and along one direction, which is favourable in the exploitation phase. Read more here . Genetic Optimization Genetic Optimization is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms. Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection. Read more here .","title":"Advanced strategies"},{"location":"user_guide/feature_engineering/#other-selection-methods","text":"Removing features with low or high variance Variance is the expectation of the squared deviation of a random variable from its mean. Features with low variance have many values repeated, which means the model can't learn much from them. In a similar way, features with very high variance have very few values repeated, which makes it also difficult for a model to learn from this feature. FeatureSelector removes a categorical feature when the maximum number of occurrences for any value is below min_repeated or when the same value is repeated in at least max_repeated fraction of the rows. The default option is to remove a feature if all values in it are either different or exactly the same. Removing features with multi-collinearity Two features that are highly correlated are redundant, i.e. two will not contribute more to the model than only one of them. FeatureSelector will drop a feature that has a Pearson correlation coefficient larger than max_correlation with another feature. A correlation of 1 means the two columns are equal. A dataframe of the removed features and their correlation values can be accessed through the collinear attribute.","title":"Other selection methods"},{"location":"user_guide/introduction/","text":"Introduction There is no magic formula in data science that can tell us which type of machine learning estimator in combination with which pipeline will perform best for a given raw dataset. Different models are better suited for different types of data and different types of problems. You can follow some rough guide on how to approach problems with regard to which model to try, but these are incomplete at best. During the exploration phase of a machine learning project, a data scientist tries to find the optimal pipeline for his specific use case. This usually involves applying standard data cleaning steps, creating or selecting useful features, trying out different models, etc. Testing multiple pipelines requires many lines of code, and writing it all in the same notebook often makes it long and cluttered. On the other hand, using multiple notebooks makes it harder to compare the results and to keep an overview. On top of that, refactoring the code for every test can be quite time-consuming. How many times have you conducted the same action to pre-process a raw dataset? How many times have you copy-and-pasted code from an old repository to re-use it in a new use case? Although best practices tell us to start with a simple model and build up to more complicated ones, many data scientists just use the model best known to them in order to avoid the aforementioned problems. This can result in poor performance (because the model is just not the right one for the task) or in inefficient management of time and computing resources (because a simpler/faster model could have achieved a similar performance). ATOM is here to help solve these common issues. The package acts as a wrapper of the whole machine learning pipeline, helping the data scientist to rapidly find a good model for his problem. Avoid endless imports and documentation lookups. Avoid rewriting the same code over and over again. With just a few lines of code, it's now possible to perform basic data cleaning steps, select relevant features and compare the performance of multiple models on a given dataset, providing quick insights on which pipeline performs best for the task at hand. It is important to realize that ATOM is not here to replace all the work a data scientist has to do before getting his model into production. ATOM doesn't spit out production-ready models just by tuning some parameters in its API. After helping you determine the right pipeline, you will most probably need to fine-tune it using use-case specific features and data cleaning steps in order to achieve maximum performance. Example steps taken by ATOM's pipeline: Data Cleaning Handle missing values Encode categorical features Detect and remove outliers Balance the training set Feature engineering Create new non-linear features Select the most promising features Train and validate multiple models Apply hyperparameter tuning Fit the models on the training set Evaluate the results on the test set Analyze the results Get the scores on various metrics Make plots to compare the model performances Figure 1. Diagram of a possible pipeline created by ATOM.","title":"Introduction"},{"location":"user_guide/introduction/#introduction","text":"There is no magic formula in data science that can tell us which type of machine learning estimator in combination with which pipeline will perform best for a given raw dataset. Different models are better suited for different types of data and different types of problems. You can follow some rough guide on how to approach problems with regard to which model to try, but these are incomplete at best. During the exploration phase of a machine learning project, a data scientist tries to find the optimal pipeline for his specific use case. This usually involves applying standard data cleaning steps, creating or selecting useful features, trying out different models, etc. Testing multiple pipelines requires many lines of code, and writing it all in the same notebook often makes it long and cluttered. On the other hand, using multiple notebooks makes it harder to compare the results and to keep an overview. On top of that, refactoring the code for every test can be quite time-consuming. How many times have you conducted the same action to pre-process a raw dataset? How many times have you copy-and-pasted code from an old repository to re-use it in a new use case? Although best practices tell us to start with a simple model and build up to more complicated ones, many data scientists just use the model best known to them in order to avoid the aforementioned problems. This can result in poor performance (because the model is just not the right one for the task) or in inefficient management of time and computing resources (because a simpler/faster model could have achieved a similar performance). ATOM is here to help solve these common issues. The package acts as a wrapper of the whole machine learning pipeline, helping the data scientist to rapidly find a good model for his problem. Avoid endless imports and documentation lookups. Avoid rewriting the same code over and over again. With just a few lines of code, it's now possible to perform basic data cleaning steps, select relevant features and compare the performance of multiple models on a given dataset, providing quick insights on which pipeline performs best for the task at hand. It is important to realize that ATOM is not here to replace all the work a data scientist has to do before getting his model into production. ATOM doesn't spit out production-ready models just by tuning some parameters in its API. After helping you determine the right pipeline, you will most probably need to fine-tune it using use-case specific features and data cleaning steps in order to achieve maximum performance. Example steps taken by ATOM's pipeline: Data Cleaning Handle missing values Encode categorical features Detect and remove outliers Balance the training set Feature engineering Create new non-linear features Select the most promising features Train and validate multiple models Apply hyperparameter tuning Fit the models on the training set Evaluate the results on the test set Analyze the results Get the scores on various metrics Make plots to compare the model performances Figure 1. Diagram of a possible pipeline created by ATOM.","title":"Introduction"},{"location":"user_guide/logging/","text":"Logging & Tracking Logging To start logging your experiments, fill the logger parameter with the name or path to store the logging file. If automatic naming is used, the file is saved using the __name__ of the class followed by the timestamp of the logger's creation, e.g. ATOMClassifier_11May21_20h11m03s . The logging file contains method calls, all printed messages to stdout with maximum verbosity, and any exception raised during running. Tracking ATOM uses mlflow tracking as a backend API and UI for logging the models in its pipeline. Start tracking your experiments assigning a name to the experiment parameter. Every model is tracked using a separate run. When no backend is configured, the data is stored locally at ./mlruns . To configure the backend, use mlflow.set_tracking_uri in your notebook or IDE before initializing atom. This does not affect the currently active run (if one exists), but takes effect for successive runs. Info When using ATOM on Databricks , the experiment's name should include the complete path to the storage, e.g. /Users/username@domain.com/experiment_name . The following elements are tracked: Tags The runs are automatically tagged with the model's full name, the branch from which the model was trained, and the time it took to fit the model. Parameters All parameters used by the estimator at initialization are tracked (only if the estimator has a get_params method). Additional parameters passed to the fit method are not tracked. Model The model's estimator is stored as artifact. The estimator has to be compatible with the mlflow.sklearn , module. This option can be switched off using atom's log_model attribute, e.g. atom.log_model = False . Hyperparameter tuning If hyperparameter tuning is performed, every call of the BO is tracked as a nested run in the model's main run. This option can be switched off using atom's log_bo attribute, e.g. atom.log_bo = False . Metrics All metric results are tracked, not only during training, but also when the evaluate method is called at a later point. Metrics calculated during in-training evaluation are also logged. Dataset The train and test sets used to fit and evaluate the model can be stored as .csv files to the run's artifacts. This option can be switched on using atom's log_data attribute, e.g. atom.log_data = True . Pipeline The model's pipeline (returned from the export_pipeline method) can be stored as an artifact using atom's log_pipeline attribute, e.g. atom.log_pipeline = True . Plots By default, plots are stored as .png artifacts in all runs corresponding to the models that are showed in the plot. If the filename parameter is specified, they are stored under that name, else the plot's name is used. This option can be switched off using atom's log_plots attribute, e.g. atom.log_plots = False .","title":"Logging & Tracking"},{"location":"user_guide/logging/#logging-tracking","text":"","title":"Logging &amp; Tracking"},{"location":"user_guide/logging/#logging","text":"To start logging your experiments, fill the logger parameter with the name or path to store the logging file. If automatic naming is used, the file is saved using the __name__ of the class followed by the timestamp of the logger's creation, e.g. ATOMClassifier_11May21_20h11m03s . The logging file contains method calls, all printed messages to stdout with maximum verbosity, and any exception raised during running.","title":"Logging"},{"location":"user_guide/logging/#tracking","text":"ATOM uses mlflow tracking as a backend API and UI for logging the models in its pipeline. Start tracking your experiments assigning a name to the experiment parameter. Every model is tracked using a separate run. When no backend is configured, the data is stored locally at ./mlruns . To configure the backend, use mlflow.set_tracking_uri in your notebook or IDE before initializing atom. This does not affect the currently active run (if one exists), but takes effect for successive runs. Info When using ATOM on Databricks , the experiment's name should include the complete path to the storage, e.g. /Users/username@domain.com/experiment_name . The following elements are tracked: Tags The runs are automatically tagged with the model's full name, the branch from which the model was trained, and the time it took to fit the model. Parameters All parameters used by the estimator at initialization are tracked (only if the estimator has a get_params method). Additional parameters passed to the fit method are not tracked. Model The model's estimator is stored as artifact. The estimator has to be compatible with the mlflow.sklearn , module. This option can be switched off using atom's log_model attribute, e.g. atom.log_model = False . Hyperparameter tuning If hyperparameter tuning is performed, every call of the BO is tracked as a nested run in the model's main run. This option can be switched off using atom's log_bo attribute, e.g. atom.log_bo = False . Metrics All metric results are tracked, not only during training, but also when the evaluate method is called at a later point. Metrics calculated during in-training evaluation are also logged. Dataset The train and test sets used to fit and evaluate the model can be stored as .csv files to the run's artifacts. This option can be switched on using atom's log_data attribute, e.g. atom.log_data = True . Pipeline The model's pipeline (returned from the export_pipeline method) can be stored as an artifact using atom's log_pipeline attribute, e.g. atom.log_pipeline = True . Plots By default, plots are stored as .png artifacts in all runs corresponding to the models that are showed in the plot. If the filename parameter is specified, they are stored under that name, else the plot's name is used. This option can be switched off using atom's log_plots attribute, e.g. atom.log_plots = False .","title":"Tracking"},{"location":"user_guide/models/","text":"Models Predefined models ATOM provides many models for classification and regression tasks that can be used to fit the data in the pipeline. After fitting, a class containing the underlying estimator is attached to the trainer as an attribute. We refer to these \"subclasses\" as models. Apart from the estimator, the models contain a variety of attributes and methods to help you understand how the underlying estimator performed. They can be accessed using their acronyms, e.g. atom.LGB to access the LightGBM's model. The available models and their corresponding acronyms are: \"Dummy\" for Dummy Estimator \"GP\" for Gaussian Process \"GNB\" for Gaussian Naive Bayes \"MNB\" for Multinomial Naive Bayes \"BNB\" for Bernoulli Naive Bayes \"CatNB\" for Categorical Naive Bayes \"CNB\" for Complement Naive Bayes \"OLS\" for Ordinary Least Squares \"Ridge\" for Ridge Estimator \"Lasso\" for Lasso Regression \"EN\" for ElasticNet Regression \"Lars\" for Least Angle Regression \"BR\" for Bayesian Ridge \"ARD\" for Automated Relevance Determination \"Huber\" for Huber Regression \"Perc\" for Perceptron \"LR\" for Logistic Regression \"LDA\" for Linear Discriminant Analysis \"QDA\" for Quadratic Discriminant Analysis \"KNN\" for K-Nearest Neighbors \"RNN\" for Radius Nearest Neighbors \"Tree\" for Decision Tree \"Bag\" for Bagging \"ET\" for Extra-Trees \"RF\" for Random Forest \"AdaB\" for AdaBoost \"GBM\" for Gradient Boosting Machine \"hGBM\" for HistGBM \"XGB\" for XGBoost \"LGB\" for LightGBM \"CatB\" for CatBoost \"lSVM\" for Linear SVM \"kSVM\" for Kernel SVM \"PA\" for Passive Aggressive \"SGD\" for Stochastic Gradient Descent \"MLP\" for Multi-layer Perceptron Tip The acronyms are case-insensitive, e.g. atom.lgb also calls the LightGBM's model. Warning The models can not be initialized directly by the user! Use them only through the trainers. Custom models It is also possible to create your own models in ATOM's pipeline. For example, imagine we want to use sklearn's RANSACRegressor estimator (note that is not included in ATOM's predefined models ). There are two ways to achieve this: Using ATOMModel (recommended). With this approach you can pass the required model characteristics to the pipeline. from atom import ATOMRegressor , ATOMModel from sklearn.linear_model import RANSACRegressor ransac = ATOMModel ( models = RANSACRegressor , acronym = \"RANSAC\" , fullname = \"Random Sample Consensus\" , needs_scaling = True , ) atom = ATOMRegressor ( X , y ) atom . run ( ransac ) Using the estimator's class or an instance of the class. This approach will also call ATOMModel under the hood, but it will leave its parameters to their default values. from atom import ATOMRegressor from sklearn.linear_model import RANSACRegressor atom = ATOMRegressor ( X , y ) atom . run ( RANSACRegressor ) Additional things to take into account: Custom models can be accessed through their acronym like any other model, e.g. atom.ransac in the example above. Custom models are not restricted to sklearn estimators, but they should follow sklearn's API , i.e. have a fit and predict method. Parameter customization (for the initializer) is only possible for custom models which provide an estimator that has a set_params() method, i.e. it's a child class of BaseEstimator . Hyperparameter optimization for custom models is ignored unless appropriate dimensions are provided through bo_params . If the estimator has a n_jobs and/or random_state parameter that is left to its default value, it will automatically adopt the values from the trainer it's called from. Deep learning Deep learning models can be used through ATOM's custom models as long as they follow sklearn's API . For example, models implemented with the Keras package should use the scikeras wrappers KerasClassifier or KerasRegressor . Many deep learning use cases, for example in computer vision, use datasets with more than 2 dimensions, e.g. image data can have shape (n_samples, length, width, rgb). Luckily, scikeras has a workaround to be able to work with such datasets. Learn with this example how to use ATOM to train and validate a Convolutional Neural Network on an image dataset. Warning Models implemented with keras can only use custom hyperparameter tuning when n_jobs=1 or bo_params={\"cv\": 1} . Using n_jobs > 1 and cv > 1 raises a PicklingError due to incompatibilities of the APIs. Ensembles Ensemble models use multiple estimators to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. ATOM implements two ensemble techniques: voting and stacking. Click here to see an example that uses ensemble models. If the ensemble's underlying estimator is a model that used automated feature scaling , it's added as a Pipeline containing the scaler and estimator. If an mlflow experiment is active, the ensembles start their own run, just like the predefined models do. Warning Combining models trained on different branches into one ensemble is not allowed and will raise an exception. Voting The idea behind voting is to combine the predictions of conceptually different models to make new predictions. Such a technique can be useful for a set of equally well performing models in order to balance out their individual weaknesses. Read more in sklearn's documentation . A voting model is created from a trainer through the voting method. The voting model is added automatically to the list of models in the trainer, under the Vote acronym. The underlying estimator is a custom adaptation of VotingClassifier or VotingRegressor depending on the task. The differences between ATOM's and sklearn's implementation are: ATOM's implementation doesn't fit estimators if they're already fitted. ATOM's instance is considered fitted at initialization when all underlying estimators are. ATOM's VotingClassifier doesn't implement a LabelEncoder to encode the target column. The two estimators are customized in this way to save time and computational resources, since the classes are always initialized with fitted estimators. As a consequence of this, the VotingClassifier can not use sklearn's build-in LabelEncoder for the target column since it can't be fitted when initializing the class. For the vast majority of use cases, the changes will have no effect. If you want to export the estimator and retrain it on different data, just make sure to clone the underlying estimators first. Stacking Stacking is a method for combining estimators to reduce their biases. More precisely, the predictions of each individual estimator are stacked together and used as input to a final estimator to compute the prediction. Read more in sklearn's documentation . A stacking model is created from a trainer through the stacking method. The stacking model is added automatically to the list of models in the trainer, under the Stack acronym. The underlying estimator is a custom adaptation of StackingClassifier or StackingRegressor depending on the task. The only difference between ATOM's and sklearn's implementation is that ATOM's implementation doesn't fit estimators if they're already fitted. The two estimators are customized in this way to save time and computational resources, since the classes are always initialized with fitted estimators. For the vast majority of use cases, the changes will have no effect. If you want to export the estimator and retrain it on different data, just make sure to clone the underlying estimators first.","title":"Models"},{"location":"user_guide/models/#models","text":"","title":"Models"},{"location":"user_guide/models/#predefined-models","text":"ATOM provides many models for classification and regression tasks that can be used to fit the data in the pipeline. After fitting, a class containing the underlying estimator is attached to the trainer as an attribute. We refer to these \"subclasses\" as models. Apart from the estimator, the models contain a variety of attributes and methods to help you understand how the underlying estimator performed. They can be accessed using their acronyms, e.g. atom.LGB to access the LightGBM's model. The available models and their corresponding acronyms are: \"Dummy\" for Dummy Estimator \"GP\" for Gaussian Process \"GNB\" for Gaussian Naive Bayes \"MNB\" for Multinomial Naive Bayes \"BNB\" for Bernoulli Naive Bayes \"CatNB\" for Categorical Naive Bayes \"CNB\" for Complement Naive Bayes \"OLS\" for Ordinary Least Squares \"Ridge\" for Ridge Estimator \"Lasso\" for Lasso Regression \"EN\" for ElasticNet Regression \"Lars\" for Least Angle Regression \"BR\" for Bayesian Ridge \"ARD\" for Automated Relevance Determination \"Huber\" for Huber Regression \"Perc\" for Perceptron \"LR\" for Logistic Regression \"LDA\" for Linear Discriminant Analysis \"QDA\" for Quadratic Discriminant Analysis \"KNN\" for K-Nearest Neighbors \"RNN\" for Radius Nearest Neighbors \"Tree\" for Decision Tree \"Bag\" for Bagging \"ET\" for Extra-Trees \"RF\" for Random Forest \"AdaB\" for AdaBoost \"GBM\" for Gradient Boosting Machine \"hGBM\" for HistGBM \"XGB\" for XGBoost \"LGB\" for LightGBM \"CatB\" for CatBoost \"lSVM\" for Linear SVM \"kSVM\" for Kernel SVM \"PA\" for Passive Aggressive \"SGD\" for Stochastic Gradient Descent \"MLP\" for Multi-layer Perceptron Tip The acronyms are case-insensitive, e.g. atom.lgb also calls the LightGBM's model. Warning The models can not be initialized directly by the user! Use them only through the trainers.","title":"Predefined models"},{"location":"user_guide/models/#custom-models","text":"It is also possible to create your own models in ATOM's pipeline. For example, imagine we want to use sklearn's RANSACRegressor estimator (note that is not included in ATOM's predefined models ). There are two ways to achieve this: Using ATOMModel (recommended). With this approach you can pass the required model characteristics to the pipeline. from atom import ATOMRegressor , ATOMModel from sklearn.linear_model import RANSACRegressor ransac = ATOMModel ( models = RANSACRegressor , acronym = \"RANSAC\" , fullname = \"Random Sample Consensus\" , needs_scaling = True , ) atom = ATOMRegressor ( X , y ) atom . run ( ransac ) Using the estimator's class or an instance of the class. This approach will also call ATOMModel under the hood, but it will leave its parameters to their default values. from atom import ATOMRegressor from sklearn.linear_model import RANSACRegressor atom = ATOMRegressor ( X , y ) atom . run ( RANSACRegressor ) Additional things to take into account: Custom models can be accessed through their acronym like any other model, e.g. atom.ransac in the example above. Custom models are not restricted to sklearn estimators, but they should follow sklearn's API , i.e. have a fit and predict method. Parameter customization (for the initializer) is only possible for custom models which provide an estimator that has a set_params() method, i.e. it's a child class of BaseEstimator . Hyperparameter optimization for custom models is ignored unless appropriate dimensions are provided through bo_params . If the estimator has a n_jobs and/or random_state parameter that is left to its default value, it will automatically adopt the values from the trainer it's called from.","title":"Custom models"},{"location":"user_guide/models/#deep-learning","text":"Deep learning models can be used through ATOM's custom models as long as they follow sklearn's API . For example, models implemented with the Keras package should use the scikeras wrappers KerasClassifier or KerasRegressor . Many deep learning use cases, for example in computer vision, use datasets with more than 2 dimensions, e.g. image data can have shape (n_samples, length, width, rgb). Luckily, scikeras has a workaround to be able to work with such datasets. Learn with this example how to use ATOM to train and validate a Convolutional Neural Network on an image dataset. Warning Models implemented with keras can only use custom hyperparameter tuning when n_jobs=1 or bo_params={\"cv\": 1} . Using n_jobs > 1 and cv > 1 raises a PicklingError due to incompatibilities of the APIs.","title":"Deep learning"},{"location":"user_guide/models/#ensembles","text":"Ensemble models use multiple estimators to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. ATOM implements two ensemble techniques: voting and stacking. Click here to see an example that uses ensemble models. If the ensemble's underlying estimator is a model that used automated feature scaling , it's added as a Pipeline containing the scaler and estimator. If an mlflow experiment is active, the ensembles start their own run, just like the predefined models do. Warning Combining models trained on different branches into one ensemble is not allowed and will raise an exception.","title":"Ensembles"},{"location":"user_guide/models/#voting","text":"The idea behind voting is to combine the predictions of conceptually different models to make new predictions. Such a technique can be useful for a set of equally well performing models in order to balance out their individual weaknesses. Read more in sklearn's documentation . A voting model is created from a trainer through the voting method. The voting model is added automatically to the list of models in the trainer, under the Vote acronym. The underlying estimator is a custom adaptation of VotingClassifier or VotingRegressor depending on the task. The differences between ATOM's and sklearn's implementation are: ATOM's implementation doesn't fit estimators if they're already fitted. ATOM's instance is considered fitted at initialization when all underlying estimators are. ATOM's VotingClassifier doesn't implement a LabelEncoder to encode the target column. The two estimators are customized in this way to save time and computational resources, since the classes are always initialized with fitted estimators. As a consequence of this, the VotingClassifier can not use sklearn's build-in LabelEncoder for the target column since it can't be fitted when initializing the class. For the vast majority of use cases, the changes will have no effect. If you want to export the estimator and retrain it on different data, just make sure to clone the underlying estimators first.","title":"Voting"},{"location":"user_guide/models/#stacking","text":"Stacking is a method for combining estimators to reduce their biases. More precisely, the predictions of each individual estimator are stacked together and used as input to a final estimator to compute the prediction. Read more in sklearn's documentation . A stacking model is created from a trainer through the stacking method. The stacking model is added automatically to the list of models in the trainer, under the Stack acronym. The underlying estimator is a custom adaptation of StackingClassifier or StackingRegressor depending on the task. The only difference between ATOM's and sklearn's implementation is that ATOM's implementation doesn't fit estimators if they're already fitted. The two estimators are customized in this way to save time and computational resources, since the classes are always initialized with fitted estimators. For the vast majority of use cases, the changes will have no effect. If you want to export the estimator and retrain it on different data, just make sure to clone the underlying estimators first.","title":"Stacking"},{"location":"user_guide/nlp/","text":"Natural Language Processing Natural Language Processing (NLP) is the subfield of machine learning that works with human language data. The nlp module contains four classes that help to convert raw text to meaningful numeric values, ready to be ingested by a model. ATOM uses the nltk library for the majority of its NLP processes. The text documents are expected to be provided in a column of the dataframe named corpus (the name is case-insensitive). Only the corpus is changed by the transformers, leaving the rest of the columns as is. This mechanism allows atom to combine datasets containing a text corpus with other non-text features. If an array is provided as input, it should consist of only one feature containing the text (one document per row). ATOM will then automatically convert the array to a dataframe with the desired column name. Documents are expected to be strings or sequences of words. Click here for an example using text data. Note All of atom's NLP methods automatically adopt the relevant transformer attributes ( verbose , logger ) from atom. A different choice can be added as parameter to the method call, e.g. atom.tokenize(verbose=0) . Info ATOM doesn't do topic modelling! The module's goal is to help process text documents into features that can be used for supervised learning. Text cleaning Text data is rarely clean. Whether it's scraped from a website or inferred from paper documents, it's always populated with irrelevant information for the model, such as email addresses, HTML tags, numbers or punctuation marks. Use the TextCleaner class to clean the corpus from such noise. It can be accessed from atom through the textclean Use the class' parameters to choose which transformations to perform. The available steps are: Decode unicode characters to their ascii representations. Convert all characters to lower case. Drop email addresses from the text. Drop URL links from the text. Drop HTML tags from the text. Drop emojis from the text. Drop numbers from the text. Drop punctuations from the text. Tokenization Some text processing algorithms, like stemming or lemmatization, require the corpus to be made out of tokens, instead of strings, in order to know what to consider as words. Tokenization is used to achieve this. It separates every document into a sequence of smaller units. In this case, the words. Sometimes, words have a different meaning on their own than when combined with adjacent words. For example, the word new has a completely different meaning when the word york is directly after it than when it's not. These combinations of two words are called bigrams. When there are three words, they are called trigrams, and with four words quadgrams. The Tokenizer class converts a document into a sequence of words, and can create the most frequent bigrams, trigrams and quadgrams. It can be accessed from atom through the tokenize method. Text Normalization Normalization for texts is a process that converts a list of words to a more uniform standard. This is useful to reduce the amount of different information that the computer has to deal with, and therefore improves efficiency. The goal of normalization techniques like stemming and lemmatization is to reduce inflectional and related forms of a word to a common base form. Normalize the words in the corpus using the TextNormalizer class. It can be accessed from atom through the textnormalize method. Vectorization Text data cannot be fed directly to the algorithms themselves, as most of them expect numerical feature vectors with a fixed size, rather than words in the text documents with variable length. Vectorization is the general process of turning a collection of text documents into numerical feature vectors. You can apply it to the corpus using the Vectorizer class. It can be accessed from atom through the vectorize method. Info All strategies can utilize GPU speed-up. Click here for further information about GPU implementation. Bag of Words The Bag of Words (BOW) strategy applies tokenization, counting and normalization to the corpus. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document. The created columns are named with the words they are embedding with the prefix corpus_ . Read more in sklearn's documentation . TF-IDF In a large text corpus, some words will be very present (e.g. \u201cthe\u201d, \u201ca\u201d, \u201cis\u201d in English), hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier, those very frequent terms would shadow the frequencies of rarer, yet more interesting, terms. Use the TF-IDF strategy to re-weight the count features into floating point values. The created columns are named with the words they are embedding with the prefix corpus_ . Read more in sklearn's documentation . Hashing The larger the corpus, the larger the vocabulary will grow and thus increasing the number of features and memory use. Use the Hashing strategy to hash the words to a specified number of features. The created features are named hash0 , hash1 , etc... Read more in sklearn's documentation .","title":"NLP"},{"location":"user_guide/nlp/#natural-language-processing","text":"Natural Language Processing (NLP) is the subfield of machine learning that works with human language data. The nlp module contains four classes that help to convert raw text to meaningful numeric values, ready to be ingested by a model. ATOM uses the nltk library for the majority of its NLP processes. The text documents are expected to be provided in a column of the dataframe named corpus (the name is case-insensitive). Only the corpus is changed by the transformers, leaving the rest of the columns as is. This mechanism allows atom to combine datasets containing a text corpus with other non-text features. If an array is provided as input, it should consist of only one feature containing the text (one document per row). ATOM will then automatically convert the array to a dataframe with the desired column name. Documents are expected to be strings or sequences of words. Click here for an example using text data. Note All of atom's NLP methods automatically adopt the relevant transformer attributes ( verbose , logger ) from atom. A different choice can be added as parameter to the method call, e.g. atom.tokenize(verbose=0) . Info ATOM doesn't do topic modelling! The module's goal is to help process text documents into features that can be used for supervised learning.","title":"Natural Language Processing"},{"location":"user_guide/nlp/#text-cleaning","text":"Text data is rarely clean. Whether it's scraped from a website or inferred from paper documents, it's always populated with irrelevant information for the model, such as email addresses, HTML tags, numbers or punctuation marks. Use the TextCleaner class to clean the corpus from such noise. It can be accessed from atom through the textclean Use the class' parameters to choose which transformations to perform. The available steps are: Decode unicode characters to their ascii representations. Convert all characters to lower case. Drop email addresses from the text. Drop URL links from the text. Drop HTML tags from the text. Drop emojis from the text. Drop numbers from the text. Drop punctuations from the text.","title":"Text cleaning"},{"location":"user_guide/nlp/#tokenization","text":"Some text processing algorithms, like stemming or lemmatization, require the corpus to be made out of tokens, instead of strings, in order to know what to consider as words. Tokenization is used to achieve this. It separates every document into a sequence of smaller units. In this case, the words. Sometimes, words have a different meaning on their own than when combined with adjacent words. For example, the word new has a completely different meaning when the word york is directly after it than when it's not. These combinations of two words are called bigrams. When there are three words, they are called trigrams, and with four words quadgrams. The Tokenizer class converts a document into a sequence of words, and can create the most frequent bigrams, trigrams and quadgrams. It can be accessed from atom through the tokenize method.","title":"Tokenization"},{"location":"user_guide/nlp/#text-normalization","text":"Normalization for texts is a process that converts a list of words to a more uniform standard. This is useful to reduce the amount of different information that the computer has to deal with, and therefore improves efficiency. The goal of normalization techniques like stemming and lemmatization is to reduce inflectional and related forms of a word to a common base form. Normalize the words in the corpus using the TextNormalizer class. It can be accessed from atom through the textnormalize method.","title":"Text Normalization"},{"location":"user_guide/nlp/#vectorization","text":"Text data cannot be fed directly to the algorithms themselves, as most of them expect numerical feature vectors with a fixed size, rather than words in the text documents with variable length. Vectorization is the general process of turning a collection of text documents into numerical feature vectors. You can apply it to the corpus using the Vectorizer class. It can be accessed from atom through the vectorize method. Info All strategies can utilize GPU speed-up. Click here for further information about GPU implementation. Bag of Words The Bag of Words (BOW) strategy applies tokenization, counting and normalization to the corpus. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document. The created columns are named with the words they are embedding with the prefix corpus_ . Read more in sklearn's documentation . TF-IDF In a large text corpus, some words will be very present (e.g. \u201cthe\u201d, \u201ca\u201d, \u201cis\u201d in English), hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier, those very frequent terms would shadow the frequencies of rarer, yet more interesting, terms. Use the TF-IDF strategy to re-weight the count features into floating point values. The created columns are named with the words they are embedding with the prefix corpus_ . Read more in sklearn's documentation . Hashing The larger the corpus, the larger the vocabulary will grow and thus increasing the number of features and memory use. Use the Hashing strategy to hash the words to a specified number of features. The created features are named hash0 , hash1 , etc... Read more in sklearn's documentation .","title":"Vectorization"},{"location":"user_guide/nomenclature/","text":"Nomenclature This documentation consistently uses terms to refer to certain concepts related to this package. The most frequent terms are described hereunder. ATOM Refers to this package. atom Instance of the ATOMClassifier or ATOMRegressor classes (note that the examples use it as the default variable name). branch A pipeline , corresponding dataset and models fitted to that dataset. See the branches section of the user guide. categorical columns Refers to all columns of type object or category . class Unique value in a column, e.g. a binary classifier has 2 classes in the target column. dataframe-like Any type object from which a pd.DataFrame can be created. This includes an iterable , a dict whose values are 1d-arrays, a two-dimensional list , tuple , np.ndarray or sps.csr_matrix , and most commonly, a dataframe. This is the standard input format for any dataset. estimator An object which manages the estimation and decoding of an algorithm. The algorithm is estimated as a deterministic function of a set of parameters, a dataset and a random state. Should implement a fit method. Often used interchangeably with predictor because of user preference. missing values All values in the class' missing attribute, as well as None , NaN , +inf and -inf . model Instance of a model in the pipeline. Not to confuse with estimator . outliers Sample that contains one or more outlier values. Note that the Pruner class can use a different definition for outliers depending on the chosen strategy. outlier value Value that lies further than 3 times the standard deviation away from the mean of its column, i.e. |z-score| > 3. pipeline Sequence of transformers in a specific (usually the current) branch . predictor An estimator implementing a predict method. scorer A non-estimator callable object which evaluates an estimator on given test data, returning a number. Unlike evaluation metrics, a greater returned number must correspond with a better score. See sklearn's documentation . sequence A one-dimensional array of type list , tuple , np.ndarray or pd.Series . This is the standard input format for a dataset's target column. target The dependent variable in a supervised learning task. Passed as y to an estimator's fit method. task One of the three supervised machine learning approaches that ATOM supports: binary classification multiclass classification regression transformer An estimator implementing a transform method. This encompasses all data cleaning and feature engineering classes.","title":"Nomenclature"},{"location":"user_guide/nomenclature/#nomenclature","text":"This documentation consistently uses terms to refer to certain concepts related to this package. The most frequent terms are described hereunder. ATOM Refers to this package. atom Instance of the ATOMClassifier or ATOMRegressor classes (note that the examples use it as the default variable name). branch A pipeline , corresponding dataset and models fitted to that dataset. See the branches section of the user guide. categorical columns Refers to all columns of type object or category . class Unique value in a column, e.g. a binary classifier has 2 classes in the target column. dataframe-like Any type object from which a pd.DataFrame can be created. This includes an iterable , a dict whose values are 1d-arrays, a two-dimensional list , tuple , np.ndarray or sps.csr_matrix , and most commonly, a dataframe. This is the standard input format for any dataset. estimator An object which manages the estimation and decoding of an algorithm. The algorithm is estimated as a deterministic function of a set of parameters, a dataset and a random state. Should implement a fit method. Often used interchangeably with predictor because of user preference. missing values All values in the class' missing attribute, as well as None , NaN , +inf and -inf . model Instance of a model in the pipeline. Not to confuse with estimator . outliers Sample that contains one or more outlier values. Note that the Pruner class can use a different definition for outliers depending on the chosen strategy. outlier value Value that lies further than 3 times the standard deviation away from the mean of its column, i.e. |z-score| > 3. pipeline Sequence of transformers in a specific (usually the current) branch . predictor An estimator implementing a predict method. scorer A non-estimator callable object which evaluates an estimator on given test data, returning a number. Unlike evaluation metrics, a greater returned number must correspond with a better score. See sklearn's documentation . sequence A one-dimensional array of type list , tuple , np.ndarray or pd.Series . This is the standard input format for a dataset's target column. target The dependent variable in a supervised learning task. Passed as y to an estimator's fit method. task One of the three supervised machine learning approaches that ATOM supports: binary classification multiclass classification regression transformer An estimator implementing a transform method. This encompasses all data cleaning and feature engineering classes.","title":"Nomenclature"},{"location":"user_guide/plots/","text":"Plots ATOM provides many plotting methods to analyze the data or compare the model performances. Descriptions and examples can be found in the API section. ATOM uses the packages matplotlib , seaborn , shap , wordcloud and schemdraw for plotting. Plots that compare model performances (methods with the models parameter) can be called directly from atom, e.g. atom.plot_roc() , or from one of the models, e.g. atom.adab.plot_roc() . If called from atom, use the models parameter to specify which models to plot. If called from a specific model, it makes the plot only for that model and the models parameter becomes unavailable. Plots that analyze the data (methods without the models parameter) can only be called from atom, and not from the models. Parameters Apart from the plot-specific parameters, all plots have four parameters in common: The title parameter allows you to add a title to the plot. The figsize parameter adjust the plot's size. The filename parameter is used to save the plot. The display parameter determines whether to show or return the plot. Aesthetics The plot aesthetics can be customized using the plot attributes, e.g. atom.style = \"white\" . These attributes can be called from any instance with plotting methods. Note that the plot attributes are attached to the class and not the instance. This means that changing the attribute will also change it for all other instances in the module. Use the reset_aesthetics method to reset all the aesthetics to their default value. The default values are: style: \"darkgrid\" palette: \"GnBu_r_d\" title_fontsize: 20 label_fontsize: 16 tick_fontsize: 12 Canvas Sometimes it's desirable to draw multiple plots side by side in order to be able to compare them easier. Use the canvas method for this. The canvas method is a @contextmanager , i.e. it's used through the with command. Plots in a canvas will ignore the figsize, filename and display parameters. Instead, call these parameters from the canvas for the final figure. If a variable is assigned to the canvas (e.g. with atom.canvas() as fig ), it contains the resulting matplotlib figure. For example, we can use a canvas to compare the results of a XGBoost and LightGBM model on the train and test set. We could also draw the lines for both models in the same axes, but then the plot would become too cluttered. atom = ATOMClassifier ( X , y ) atom . run ([ \"xgb\" , \"lgb\" ], n_calls = 0 ) with atom . canvas ( 2 , 2 , title = \"XGBoost vs LightGBM\" , filename = \"canvas\" ): atom . xgb . plot_roc ( dataset = \"both\" , title = \"ROC - XGBoost\" ) atom . lgb . plot_roc ( dataset = \"both\" , title = \"ROC - LightGBM\" ) atom . xgb . plot_prc ( dataset = \"both\" , title = \"PRC - XGBoost\" ) atom . lgb . plot_prc ( dataset = \"both\" , title = \"PRC - LightGBM\" ) SHAP The SHAP (SHapley Additive exPlanations) python package uses a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. ATOM implements methods to plot 7 of SHAP's plotting functions directly from its API. Check the available shap plots here . Calculating the Shapley values is computationally expensive, especially for model agnostic explainers like Permutation . To avoid having to recalculate the values for every plot, ATOM stores the shapley values internally after the first calculation, and access them when needed again. Since the plots are not made by ATOM, we can't draw multiple models in the same figure. Selecting more than one model will raise an exception. To avoid this, call the plot directly from a model, e.g. atom.adab.plot_shap_force() . Available plots A list of available plots can be found hereunder. Note that not all plots can be called from every class and that their availability can depend on the task at hand. Feature selection plots plot_components Plot the explained variance ratio per component. plot_pca Plot the explained variance ratio vs number of components. plot_rfecv Plot the rfecv results. Data plots plot_correlation Plot a correlation matrix. plot_distribution Plot column distributions. plot_ngrams Plot n-gram frequencies. plot_qq Plot a quantile-quantile plot. plot_scatter_matrix Plot a matrix of scatter plots. plot_wordcloud Plot a wordcloud from the corpus. Model plots plot_calibration Plot the calibration curve for a binary classifier. plot_confusion_matrix Plot a model's confusion matrix. plot_det Plot the detection error tradeoff curve. plot_errors Plot a model's prediction errors. plot_evals Plot evaluation curves for the train and test set. plot_feature_importance Plot a model's feature importance. plot_gains Plot the cumulative gains curve. plot_learning_curve Plot the learning curve: score vs number of training samples. plot_lift Plot the lift curve. plot_parshap Plot the partial correlation of shap values. plot_partial_dependence Plot the partial dependence of features. plot_permutation_importance Plot the feature permutation importance of models. plot_pipeline Plot a diagram of the pipeline. plot_prc Plot the precision-recall curve. plot_probabilities Plot the probability distribution of the target classes. plot_residuals Plot a model's residuals. plot_results Plot of the model results after the evaluation. plot_roc Plot the Receiver Operating Characteristics curve. plot_successive_halving Plot scores per iteration of the successive halving. plot_threshold Plot metric performances against threshold values. plot_trials Plot the hyperparameter tuning trials. Shap plots plot_shap_bar Plot SHAP's bar plot. plot_shap_beeswarm Plot SHAP's beeswarm plot. plot_shap_decision Plot SHAP's decision plot. plot_shap_force Plot SHAP's force plot. plot_shap_heatmap Plot SHAP's heatmap plot. plot_shap_scatter Plot SHAP's scatter plot. plot_shap_waterfall Plot SHAP's waterfall plot.","title":"Plots"},{"location":"user_guide/plots/#plots","text":"ATOM provides many plotting methods to analyze the data or compare the model performances. Descriptions and examples can be found in the API section. ATOM uses the packages matplotlib , seaborn , shap , wordcloud and schemdraw for plotting. Plots that compare model performances (methods with the models parameter) can be called directly from atom, e.g. atom.plot_roc() , or from one of the models, e.g. atom.adab.plot_roc() . If called from atom, use the models parameter to specify which models to plot. If called from a specific model, it makes the plot only for that model and the models parameter becomes unavailable. Plots that analyze the data (methods without the models parameter) can only be called from atom, and not from the models.","title":"Plots"},{"location":"user_guide/plots/#parameters","text":"Apart from the plot-specific parameters, all plots have four parameters in common: The title parameter allows you to add a title to the plot. The figsize parameter adjust the plot's size. The filename parameter is used to save the plot. The display parameter determines whether to show or return the plot.","title":"Parameters"},{"location":"user_guide/plots/#aesthetics","text":"The plot aesthetics can be customized using the plot attributes, e.g. atom.style = \"white\" . These attributes can be called from any instance with plotting methods. Note that the plot attributes are attached to the class and not the instance. This means that changing the attribute will also change it for all other instances in the module. Use the reset_aesthetics method to reset all the aesthetics to their default value. The default values are: style: \"darkgrid\" palette: \"GnBu_r_d\" title_fontsize: 20 label_fontsize: 16 tick_fontsize: 12","title":"Aesthetics"},{"location":"user_guide/plots/#canvas","text":"Sometimes it's desirable to draw multiple plots side by side in order to be able to compare them easier. Use the canvas method for this. The canvas method is a @contextmanager , i.e. it's used through the with command. Plots in a canvas will ignore the figsize, filename and display parameters. Instead, call these parameters from the canvas for the final figure. If a variable is assigned to the canvas (e.g. with atom.canvas() as fig ), it contains the resulting matplotlib figure. For example, we can use a canvas to compare the results of a XGBoost and LightGBM model on the train and test set. We could also draw the lines for both models in the same axes, but then the plot would become too cluttered. atom = ATOMClassifier ( X , y ) atom . run ([ \"xgb\" , \"lgb\" ], n_calls = 0 ) with atom . canvas ( 2 , 2 , title = \"XGBoost vs LightGBM\" , filename = \"canvas\" ): atom . xgb . plot_roc ( dataset = \"both\" , title = \"ROC - XGBoost\" ) atom . lgb . plot_roc ( dataset = \"both\" , title = \"ROC - LightGBM\" ) atom . xgb . plot_prc ( dataset = \"both\" , title = \"PRC - XGBoost\" ) atom . lgb . plot_prc ( dataset = \"both\" , title = \"PRC - LightGBM\" )","title":"Canvas"},{"location":"user_guide/plots/#shap","text":"The SHAP (SHapley Additive exPlanations) python package uses a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. ATOM implements methods to plot 7 of SHAP's plotting functions directly from its API. Check the available shap plots here . Calculating the Shapley values is computationally expensive, especially for model agnostic explainers like Permutation . To avoid having to recalculate the values for every plot, ATOM stores the shapley values internally after the first calculation, and access them when needed again. Since the plots are not made by ATOM, we can't draw multiple models in the same figure. Selecting more than one model will raise an exception. To avoid this, call the plot directly from a model, e.g. atom.adab.plot_shap_force() .","title":"SHAP"},{"location":"user_guide/plots/#available-plots","text":"A list of available plots can be found hereunder. Note that not all plots can be called from every class and that their availability can depend on the task at hand.","title":"Available plots"},{"location":"user_guide/plots/#feature-selection-plots","text":"plot_components Plot the explained variance ratio per component. plot_pca Plot the explained variance ratio vs number of components. plot_rfecv Plot the rfecv results.","title":"Feature selection plots"},{"location":"user_guide/plots/#data-plots","text":"plot_correlation Plot a correlation matrix. plot_distribution Plot column distributions. plot_ngrams Plot n-gram frequencies. plot_qq Plot a quantile-quantile plot. plot_scatter_matrix Plot a matrix of scatter plots. plot_wordcloud Plot a wordcloud from the corpus.","title":"Data plots"},{"location":"user_guide/plots/#model-plots","text":"plot_calibration Plot the calibration curve for a binary classifier. plot_confusion_matrix Plot a model's confusion matrix. plot_det Plot the detection error tradeoff curve. plot_errors Plot a model's prediction errors. plot_evals Plot evaluation curves for the train and test set. plot_feature_importance Plot a model's feature importance. plot_gains Plot the cumulative gains curve. plot_learning_curve Plot the learning curve: score vs number of training samples. plot_lift Plot the lift curve. plot_parshap Plot the partial correlation of shap values. plot_partial_dependence Plot the partial dependence of features. plot_permutation_importance Plot the feature permutation importance of models. plot_pipeline Plot a diagram of the pipeline. plot_prc Plot the precision-recall curve. plot_probabilities Plot the probability distribution of the target classes. plot_residuals Plot a model's residuals. plot_results Plot of the model results after the evaluation. plot_roc Plot the Receiver Operating Characteristics curve. plot_successive_halving Plot scores per iteration of the successive halving. plot_threshold Plot metric performances against threshold values. plot_trials Plot the hyperparameter tuning trials.","title":"Model plots"},{"location":"user_guide/plots/#shap-plots","text":"plot_shap_bar Plot SHAP's bar plot. plot_shap_beeswarm Plot SHAP's beeswarm plot. plot_shap_decision Plot SHAP's decision plot. plot_shap_force Plot SHAP's force plot. plot_shap_heatmap Plot SHAP's heatmap plot. plot_shap_scatter Plot SHAP's scatter plot. plot_shap_waterfall Plot SHAP's waterfall plot.","title":"Shap plots"},{"location":"user_guide/predicting/","text":"Predicting Prediction methods After training a model, you probably want to make predictions on new, unseen data. Just like a sklearn estimator, you can call the prediction methods from the model, e.g. atom.tree.predict(X) . All prediction methods transform the provided data through the pipeline in the model's branch before making the predictions. Transformers that should only be applied on the training set are excluded from this step (e.g. outlier pruning or class balancing). The available prediction methods are the most common methods for estimators in sklearn's API: Prediction attributes The prediction methods can be calculated on the train, test and holdout set. You can access them through attributes of the form [method]_[data_set], e.g. atom.mnb.predict_train , atom.mnb.predict_test or atom.mnb.predict_holdout . The predictions for these attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to make (perhaps) expensive calculations that are never used, saving time and memory. Note Many of the plots use the prediction attributes. This can considerably increase the size of the instance for large datasets. Use the clear method if you need to free some memory. Predictions on rows in the dataset It's also possible to get the prediction for a specific row or rows in the dataset, providing the names or positions of their indices, e.g. atom.rf.predict(10) returns the random forest's prediction on the 10th row in the dataset, or atom.rf.predict_proba([\"index1\", \"index2\"]) returns the class probabilities for the rows in the dataset with indices index1 and index2 .","title":"Predicting"},{"location":"user_guide/predicting/#predicting","text":"","title":"Predicting"},{"location":"user_guide/predicting/#prediction-methods","text":"After training a model, you probably want to make predictions on new, unseen data. Just like a sklearn estimator, you can call the prediction methods from the model, e.g. atom.tree.predict(X) . All prediction methods transform the provided data through the pipeline in the model's branch before making the predictions. Transformers that should only be applied on the training set are excluded from this step (e.g. outlier pruning or class balancing). The available prediction methods are the most common methods for estimators in sklearn's API:","title":"Prediction methods"},{"location":"user_guide/predicting/#prediction-attributes","text":"The prediction methods can be calculated on the train, test and holdout set. You can access them through attributes of the form [method]_[data_set], e.g. atom.mnb.predict_train , atom.mnb.predict_test or atom.mnb.predict_holdout . The predictions for these attributes are not calculated until the attribute is called for the first time. This mechanism avoids having to make (perhaps) expensive calculations that are never used, saving time and memory. Note Many of the plots use the prediction attributes. This can considerably increase the size of the instance for large datasets. Use the clear method if you need to free some memory.","title":"Prediction attributes"},{"location":"user_guide/predicting/#predictions-on-rows-in-the-dataset","text":"It's also possible to get the prediction for a specific row or rows in the dataset, providing the names or positions of their indices, e.g. atom.rf.predict(10) returns the random forest's prediction on the 10th row in the dataset, or atom.rf.predict_proba([\"index1\", \"index2\"]) returns the class probabilities for the rows in the dataset with indices index1 and index2 .","title":"Predictions on rows in the dataset"},{"location":"user_guide/training/","text":"Training The training phase is where the models are fitted and evaluated. After this, the models are attached to the trainer, and you can use the plotting and predicting methods. The pipeline applies the following steps iteratively for all models: The optimal hyperparameters for the model are selected using a bayesian optimization algorithm (optional). The model is fitted on the training set using the best combination of hyperparameters found. After that, the model is evaluated on the tes set. Calculate various scores on the test set using a bootstrap algorithm (optional). There are three approaches to run the training. Direct training: DirectClassifier DirectRegressor Training via successive halving : SuccessiveHalvingClassifier SuccessiveHavingRegressor Training via train sizing : TrainSizingClassifier TrainSizingRegressor The direct fashion repeats the aforementioned steps only once, while the other two approaches repeats them more than once. Just like the data cleaning and feature engineering classes, it's discouraged to use these classes directly. Instead, every approach can be called directly from atom through the run , successive_halving and train_sizing methods respectively. Models are called through their acronyms , e.g. atom.run(models=\"RF\") will train a Random Forest . If you want to run the same model multiple times, add a tag after the acronym to differentiate them. atom . run ( models = [ \"RF1\" , \"RF2\" ], est_params = { \"RF1\" : { \"n_estimators\" : 100 }, \"RF2\" : { \"n_estimators\" : 200 }, } ) For example, this pipeline will fit two Random Forest models, one with 100 and the other with 200 decision trees. The models can be accessed through atom.rf1 and atom.rf2 . Use tagged models to test how the same model performs when fitted with different parameters or on different data sets. See the Imbalanced datasets example. Additional things to take into account: If an exception is encountered while fitting an estimator, the pipeline will automatically jump to the next model. The exceptions are stored in the errors attribute. Note that when a model is skipped, there is no model subclass for that estimator. When showing the final results, a ! indicates the highest score and a ~ indicates that the model is possibly overfitting (training set has a score at least 20% higher than the test set). The winning model (the one with the highest mean_bootstrap or metric_test ) can be accessed through the winner attribute. In case of a tie, the model that trained fastest is selected as winner. Metric ATOM uses sklearn's scorers for model evaluation. A scorer consists of a metric function and some parameters that define the scorer's properties , such as if a higher or lower score is better (score or loss function) or if the function needs probability estimates or rounded predictions (see the make_scorer function). ATOM lets you define the scorer for the pipeline in three ways: The metric parameter is the name of a predefined scorer . The metric parameter is a function with signature metric(y, y_pred). In this case, use the greater_is_better , needs_proba and needs_threshold parameters to specify the scorer's properties. The metric parameter is a scorer object. Note that all scorers follow the convention that higher return values are better than lower return values. Thus, metrics which measure the distance between the model and the data (i.e. loss functions), like max_error or mean_squared_error , will return the negated value of the metric. Predefined scorers ATOM accepts all sklearn's scorers as well as some custom acronyms and custom scorers. Since some of sklearn's scorers have quite long names and ATOM is all about lazy fast experimentation, the package provides acronyms for some of the most commonly used ones. These acronyms are case-insensitive and can be used in the metric parameter instead of the scorer's full name, e.g. atom.run(\"LR\", metric=\"BA\") will use balanced_accuracy . The available acronyms are: \"AP\" for \"average_precision\" \"BA\" for \"balanced_accuracy\" \"AUC\" for \"roc_auc\" \"LogLoss\" for \"neg_log_loss\" \"EV\" for \"explained_variance\" \"ME\" for \"max_error\" \"MAE\" for \"neg_mean_absolute_error\" \"MSE\" for \"neg_mean_squared_error\" \"RMSE\" for \"neg_root_mean_squared_error\" \"MSLE\" for \"neg_mean_squared_log_error\" \"MEDAE\" for \"neg_median_absolute_error\" \"MAPE\" for \"neg_mean_absolute_percentage_error\" \"POISSON\" for \"neg_mean_poisson_deviance\" \"GAMMA\" for \"neg_mean_gamma_deviance\" ATOM also provides some extra common metrics for binary classification tasks. \"TN\" for True Negatives \"FP\" for False Positives \"FN\" for False Negatives \"TP\" for True Positives \"FPR\" for False Positive rate (fall-out) \"TPR\" for True Positive Rate (sensitivity, recall) \"TNR\" for True Negative Rate (specificity) \"FNR\" for False Negative Rate (miss rate) \"MCC\" for Matthews Correlation Coefficient (also for multiclass classification) Multi-metric runs Sometimes it is useful to measure the performance of the models in more than one way. ATOM lets you run the pipeline with multiple metrics at the same time. To do so, provide the metric parameter with a list of desired metrics, e.g. atom.run(\"LDA\", metric=[\"r2\", \"mse\"]) . If you provide metric functions, don't forget to also provide a sequence of values to the greater_is_better , needs_proba and needs_threshold parameters, where the n-th value in corresponds to the n-th function. If you leave them as a single value, that value will apply to every provided metric. When fitting multi-metric runs, the resulting scores will return a list of metrics. For example, if you provided three metrics to the pipeline, atom.knn.metric_bo could return [0.8734, 0.6672, 0.9001]. Only the first metric of a multi-metric run is used to evaluate every step of the bayesian optimization and to select the winning model. Info Some plots let you choose which of the metrics to show using the metric parameter. Automated feature scaling Models that require feature scaling will automatically do so before training, unless the data is sparse or already scaled. The data is considered scaled if it has one of the following prerequisites: The mean value over the mean of all columns is <0.05 and the mean of the standard deviation over all columns lies between 0.9 and 1.1. There is a transformer in the pipeline whose __name__ contains the word scaler . The scaling is applied using a Scaler with default parameters. It can be accessed from the model through the scaler attribute. The scaled dataset can be examined through the model's data attributes . Use the available_models method to see which models require feature scaling. Parameter customization By default, the parameters every estimator uses are the same default parameters they get from their respective packages. To select different ones, use est_params . There are two ways to add custom parameters to the models: adding them directly to the dictionary as key-value pairs or through dictionaries. Adding the parameters directly to est_params (or using a dict with the key 'all') shares them across all models in the trainer. In this example, both the XGBoost and the LightGBM model use n_estimators=200 . Make sure all the models do have the specified parameters or an exception will be raised! atom . run ( models = [ \"XGB\" , \"LGB\" ], est_params = { \"n_estimators\" : 200 }) To specify parameters per model, use the model name as key and a dict of the parameters as value. In this example, the XGBoost model uses n_estimators=200 and the Multi-layer Perceptron uses one hidden layer with 75 neurons. atom . run ( models = [ \"XGB\" , \"MLP\" ], est_params = { \"XGB\" : { \"n_estimators\" : 200 }, \"MLP\" : { \"hidden_layer_sizes\" : ( 75 ,)}, } ) Some estimators allow you to pass extra parameters to the fit method (besides X and y). This can be done adding _fit at the end of the parameter. For example, to change XGBoost's verbosity, we can run: atom . run ( models = \"XGB\" , est_params = { \"verbose_fit\" : True }) Note If a parameter is specified through est_params , it is ignored by the bayesian optimization, even if it's added manually to bo_params[\"dimensions\"] ! Hyperparameter tuning In order to achieve maximum performance, it's important to tune an estimator's hyperparameters before training it. ATOM provides hyperparameter tuning using a bayesian optimization (BO) approach implemented with scikit-optimize . The BO is optimized on the first metric provided with the metric parameter. Each step is either computed by cross-validation on the complete training set or by randomly splitting the training set every iteration into a (sub) training and validation set. This process can create some minimum data leakage towards specific parameters (since the estimator is evaluated on data that is used to train the next estimator), but it ensures maximal use of the provided data. However, the leakage is not present in the independent test set, thus the final score of every model is unbiased. Note that, if the dataset is relatively small, the BO's best score can consistently be lower than the final score on the test set due to the considerable lower fraction of instances on which it is trained. After running the BO, the parameters that resulted in the best score (in case of a tie, the call with the shortest training time is selected) are used to train the model on the complete training set. There are many possibilities to tune the BO to your liking. Use n_calls and n_initial_points to determine the number of iterations that are performed randomly at the start (exploration) and the number of iterations spent optimizing (exploitation). If n_calls is equal to n_initial_points , every iteration of the BO will select its hyperparameters randomly. This means the algorithm is technically performing a random search . Extra things to take into account: The n_calls parameter includes the iterations in n_initial_points , i.e. calling atom.run(models=\"LR\", n_calls=20, n_intial_points=10) will run 20 iterations of which the first 10 are random. If n_initial_points=1 , the first call is equal to the estimator's default parameters. The train/validation splits are different per call but equal for all models. Re-evaluating the objective function at the same point automatically skips the calculation and returns the same score as the equivalent call. Tip The hyperparameter tuning output can become quite wide for models with many hyperparameters. If you are working in a Jupyter Notebook, you can change the output's width running the following code in a cell: from IPython.core.display import display , HTML display ( HTML ( \"<style>.container { width:100% !important; }</style>\" )) Other settings can be changed through the bo_params parameter, a dictionary where every key-value combination can be used to further customize the BO. By default, which hyperparameters are tuned and their corresponding dimensions are predefined by ATOM. Use the 'dimensions' key to customize these. Just like with est_params , you can share the same parameters across models or use a dictionary with the model name as key to specify the parameters for every individual model. Use the key 'all' to tune some hyperparameters for all models when you also want to tune other parameters only for specific ones. The following example tunes the n_estimators parameter for both models but the max_depth parameter only for the Random Forest. atom . run ( models = [ \"ET\" , \"RF\" ], n_calls = 30 , bo_params = { \"dimensions\" : { \"all\" : \"n_estimators\" , \"RF\" : \"max_depth\" }}, ) Like the columns parameter in atom's methods, you can exclude parameters from the BO adding ! before its name. It's possible to exclude multiple parameters, but not to combine inclusion and exclusion for the same model. For example, to optimize a Random Forest using all its predefined parameters except n_estimators , run: atom . run ( models = \"ET\" , n_calls = 15 , bo_params = { \"dimensions\" : \"!n_estimators\" }) If just the parameter name is provided, the predefined dimension space is used. It's also possible to provide custom dimension spaces, but make sure the dimensions are compliant with skopt's API . See every model's individual documentation in the API section for an overview of their hyperparameters and dimensions. from skopt.space.space import Categorical , Integer atom . run ( models = [ \"ET\" , \"RF\" ], n_calls = 30 , bo_params = { \"dimensions\" : { \"all\" : Integer ( 10 , 100 , name = \"n_estimators\" ), \"RF\" : [ Integer ( 1 , 10 , name = \"max_depth\" ), Categorical ([ None , \"sqrt\" , \"log2\" , 0.7 ], name = \"max_features\" ), ], }, }, ) Note When specifying dimension spaces manually, make sure to import the dimension types from scikit-optimize: from skopt.space.space import Real, Categorical, Integer . Warning Keras' models can only use hyperparameter tuning when n_jobs=1 or bo_params={\"cv\": 1} . Using n_jobs > 1 and cv > 1 raises a PicklingError due to incompatibilities of the APIs. Read here more about deep learning models. The majority of skopt's callbacks to stop the optimizer early can be accessed through bo_params . Other callbacks can be included through the callbacks key. atom . run ( models = \"LR\" , n_calls = 30 , bo_params = { \"callbacks\" : custom_callback ()}, ) It's also possible to include additional parameters for skopt's optimizer as key-value pairs. atom . run ( \"LR\" , n_calls = 10 , bo_params = { \"acq_func\" : \"EI\" }) Bootstrapping After fitting the estimator, you can assess the robustness of the model using the bootstrap technique. This technique creates several new data sets selecting random samples from the training set (with replacement) and evaluates them on the test set. This way we get a distribution of the performance of the model. The sets are the same for every model. The number of sets can be chosen through the n_bootstrap parameter. Tip Use the plot_results method to plot the boostrap scores in a boxplot. Early stopping XGBoost , LightGBM and CatBoost allow in-training evaluation. This means that the estimator is evaluated after every round of the training, and that the training is stopped early if it didn't improve in the last early_stopping rounds. This can save the pipeline much time that would otherwise be wasted on an estimator that is unlikely to improve further. Note that this technique is applied both during the BO and at the final fit on the complete training set. There are two ways to apply early stopping on these models: Through the early_stopping key in bo_params . This approach applies early stopping to all models in the trainer and allows the input of a fraction of the total number of rounds. Filling the early_stopping_rounds parameter directly in est_params . Don't forget to add _fit to the parameter to call it from the fit method. After fitting, the model gets the evals attribute, a dictionary of the train and test performances per round (also if early stopping wasn't applied). Click here for an example using early stopping. Tip Use the plot_evals method to plot the in-training evaluation on the train and test set. Successive halving Successive halving is a bandit-based algorithm that fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason, we recommend only to use this technique with similar models, e.g. only using tree-based models. Use successive halving through the SuccessiveHalvingClassifier / SuccessiveHalvingRegressor classes or from atom via the successive_halving method. Consecutive runs of the same model are saved with the model's acronym followed by the number of models in the run. For example, a Random Forest in a run with 4 models would become model RF4 . Click here for a successive halving example. Tip Use the plot_successive_halving method to see every model's performance per iteration of the successive halving. Train sizing When training models, there is usually a trade-off between model performance and computation time, that is regulated by the number of samples in the training set. Train sizing can be used to create insights in this trade-off, and help determine the optimal size of the training set. The models are fitted multiple times, ever-increasing the number of samples in the training set. Use train sizing through the TrainSizingClassifier / TrainSizingRegressor classes or from atom via the train_sizing method. The number of iterations and the number of samples per training can be specified with the train_sizes parameter. Consecutive runs of the same model are saved with the model's acronym followed by the fraction of rows in the training set (the . is removed from the fraction!). For example, a Random Forest in a run with 80% of the training samples would become model RF08 . Click here for a train sizing example. Tip Use the plot_learning_curve method to see the model's performance per size of the training set.","title":"Training"},{"location":"user_guide/training/#training","text":"The training phase is where the models are fitted and evaluated. After this, the models are attached to the trainer, and you can use the plotting and predicting methods. The pipeline applies the following steps iteratively for all models: The optimal hyperparameters for the model are selected using a bayesian optimization algorithm (optional). The model is fitted on the training set using the best combination of hyperparameters found. After that, the model is evaluated on the tes set. Calculate various scores on the test set using a bootstrap algorithm (optional). There are three approaches to run the training. Direct training: DirectClassifier DirectRegressor Training via successive halving : SuccessiveHalvingClassifier SuccessiveHavingRegressor Training via train sizing : TrainSizingClassifier TrainSizingRegressor The direct fashion repeats the aforementioned steps only once, while the other two approaches repeats them more than once. Just like the data cleaning and feature engineering classes, it's discouraged to use these classes directly. Instead, every approach can be called directly from atom through the run , successive_halving and train_sizing methods respectively. Models are called through their acronyms , e.g. atom.run(models=\"RF\") will train a Random Forest . If you want to run the same model multiple times, add a tag after the acronym to differentiate them. atom . run ( models = [ \"RF1\" , \"RF2\" ], est_params = { \"RF1\" : { \"n_estimators\" : 100 }, \"RF2\" : { \"n_estimators\" : 200 }, } ) For example, this pipeline will fit two Random Forest models, one with 100 and the other with 200 decision trees. The models can be accessed through atom.rf1 and atom.rf2 . Use tagged models to test how the same model performs when fitted with different parameters or on different data sets. See the Imbalanced datasets example. Additional things to take into account: If an exception is encountered while fitting an estimator, the pipeline will automatically jump to the next model. The exceptions are stored in the errors attribute. Note that when a model is skipped, there is no model subclass for that estimator. When showing the final results, a ! indicates the highest score and a ~ indicates that the model is possibly overfitting (training set has a score at least 20% higher than the test set). The winning model (the one with the highest mean_bootstrap or metric_test ) can be accessed through the winner attribute. In case of a tie, the model that trained fastest is selected as winner.","title":"Training"},{"location":"user_guide/training/#metric","text":"ATOM uses sklearn's scorers for model evaluation. A scorer consists of a metric function and some parameters that define the scorer's properties , such as if a higher or lower score is better (score or loss function) or if the function needs probability estimates or rounded predictions (see the make_scorer function). ATOM lets you define the scorer for the pipeline in three ways: The metric parameter is the name of a predefined scorer . The metric parameter is a function with signature metric(y, y_pred). In this case, use the greater_is_better , needs_proba and needs_threshold parameters to specify the scorer's properties. The metric parameter is a scorer object. Note that all scorers follow the convention that higher return values are better than lower return values. Thus, metrics which measure the distance between the model and the data (i.e. loss functions), like max_error or mean_squared_error , will return the negated value of the metric.","title":"Metric"},{"location":"user_guide/training/#predefined-scorers","text":"ATOM accepts all sklearn's scorers as well as some custom acronyms and custom scorers. Since some of sklearn's scorers have quite long names and ATOM is all about lazy fast experimentation, the package provides acronyms for some of the most commonly used ones. These acronyms are case-insensitive and can be used in the metric parameter instead of the scorer's full name, e.g. atom.run(\"LR\", metric=\"BA\") will use balanced_accuracy . The available acronyms are: \"AP\" for \"average_precision\" \"BA\" for \"balanced_accuracy\" \"AUC\" for \"roc_auc\" \"LogLoss\" for \"neg_log_loss\" \"EV\" for \"explained_variance\" \"ME\" for \"max_error\" \"MAE\" for \"neg_mean_absolute_error\" \"MSE\" for \"neg_mean_squared_error\" \"RMSE\" for \"neg_root_mean_squared_error\" \"MSLE\" for \"neg_mean_squared_log_error\" \"MEDAE\" for \"neg_median_absolute_error\" \"MAPE\" for \"neg_mean_absolute_percentage_error\" \"POISSON\" for \"neg_mean_poisson_deviance\" \"GAMMA\" for \"neg_mean_gamma_deviance\" ATOM also provides some extra common metrics for binary classification tasks. \"TN\" for True Negatives \"FP\" for False Positives \"FN\" for False Negatives \"TP\" for True Positives \"FPR\" for False Positive rate (fall-out) \"TPR\" for True Positive Rate (sensitivity, recall) \"TNR\" for True Negative Rate (specificity) \"FNR\" for False Negative Rate (miss rate) \"MCC\" for Matthews Correlation Coefficient (also for multiclass classification)","title":"Predefined scorers"},{"location":"user_guide/training/#multi-metric-runs","text":"Sometimes it is useful to measure the performance of the models in more than one way. ATOM lets you run the pipeline with multiple metrics at the same time. To do so, provide the metric parameter with a list of desired metrics, e.g. atom.run(\"LDA\", metric=[\"r2\", \"mse\"]) . If you provide metric functions, don't forget to also provide a sequence of values to the greater_is_better , needs_proba and needs_threshold parameters, where the n-th value in corresponds to the n-th function. If you leave them as a single value, that value will apply to every provided metric. When fitting multi-metric runs, the resulting scores will return a list of metrics. For example, if you provided three metrics to the pipeline, atom.knn.metric_bo could return [0.8734, 0.6672, 0.9001]. Only the first metric of a multi-metric run is used to evaluate every step of the bayesian optimization and to select the winning model. Info Some plots let you choose which of the metrics to show using the metric parameter.","title":"Multi-metric runs"},{"location":"user_guide/training/#automated-feature-scaling","text":"Models that require feature scaling will automatically do so before training, unless the data is sparse or already scaled. The data is considered scaled if it has one of the following prerequisites: The mean value over the mean of all columns is <0.05 and the mean of the standard deviation over all columns lies between 0.9 and 1.1. There is a transformer in the pipeline whose __name__ contains the word scaler . The scaling is applied using a Scaler with default parameters. It can be accessed from the model through the scaler attribute. The scaled dataset can be examined through the model's data attributes . Use the available_models method to see which models require feature scaling.","title":"Automated feature scaling"},{"location":"user_guide/training/#parameter-customization","text":"By default, the parameters every estimator uses are the same default parameters they get from their respective packages. To select different ones, use est_params . There are two ways to add custom parameters to the models: adding them directly to the dictionary as key-value pairs or through dictionaries. Adding the parameters directly to est_params (or using a dict with the key 'all') shares them across all models in the trainer. In this example, both the XGBoost and the LightGBM model use n_estimators=200 . Make sure all the models do have the specified parameters or an exception will be raised! atom . run ( models = [ \"XGB\" , \"LGB\" ], est_params = { \"n_estimators\" : 200 }) To specify parameters per model, use the model name as key and a dict of the parameters as value. In this example, the XGBoost model uses n_estimators=200 and the Multi-layer Perceptron uses one hidden layer with 75 neurons. atom . run ( models = [ \"XGB\" , \"MLP\" ], est_params = { \"XGB\" : { \"n_estimators\" : 200 }, \"MLP\" : { \"hidden_layer_sizes\" : ( 75 ,)}, } ) Some estimators allow you to pass extra parameters to the fit method (besides X and y). This can be done adding _fit at the end of the parameter. For example, to change XGBoost's verbosity, we can run: atom . run ( models = \"XGB\" , est_params = { \"verbose_fit\" : True }) Note If a parameter is specified through est_params , it is ignored by the bayesian optimization, even if it's added manually to bo_params[\"dimensions\"] !","title":"Parameter customization"},{"location":"user_guide/training/#hyperparameter-tuning","text":"In order to achieve maximum performance, it's important to tune an estimator's hyperparameters before training it. ATOM provides hyperparameter tuning using a bayesian optimization (BO) approach implemented with scikit-optimize . The BO is optimized on the first metric provided with the metric parameter. Each step is either computed by cross-validation on the complete training set or by randomly splitting the training set every iteration into a (sub) training and validation set. This process can create some minimum data leakage towards specific parameters (since the estimator is evaluated on data that is used to train the next estimator), but it ensures maximal use of the provided data. However, the leakage is not present in the independent test set, thus the final score of every model is unbiased. Note that, if the dataset is relatively small, the BO's best score can consistently be lower than the final score on the test set due to the considerable lower fraction of instances on which it is trained. After running the BO, the parameters that resulted in the best score (in case of a tie, the call with the shortest training time is selected) are used to train the model on the complete training set. There are many possibilities to tune the BO to your liking. Use n_calls and n_initial_points to determine the number of iterations that are performed randomly at the start (exploration) and the number of iterations spent optimizing (exploitation). If n_calls is equal to n_initial_points , every iteration of the BO will select its hyperparameters randomly. This means the algorithm is technically performing a random search . Extra things to take into account: The n_calls parameter includes the iterations in n_initial_points , i.e. calling atom.run(models=\"LR\", n_calls=20, n_intial_points=10) will run 20 iterations of which the first 10 are random. If n_initial_points=1 , the first call is equal to the estimator's default parameters. The train/validation splits are different per call but equal for all models. Re-evaluating the objective function at the same point automatically skips the calculation and returns the same score as the equivalent call. Tip The hyperparameter tuning output can become quite wide for models with many hyperparameters. If you are working in a Jupyter Notebook, you can change the output's width running the following code in a cell: from IPython.core.display import display , HTML display ( HTML ( \"<style>.container { width:100% !important; }</style>\" )) Other settings can be changed through the bo_params parameter, a dictionary where every key-value combination can be used to further customize the BO. By default, which hyperparameters are tuned and their corresponding dimensions are predefined by ATOM. Use the 'dimensions' key to customize these. Just like with est_params , you can share the same parameters across models or use a dictionary with the model name as key to specify the parameters for every individual model. Use the key 'all' to tune some hyperparameters for all models when you also want to tune other parameters only for specific ones. The following example tunes the n_estimators parameter for both models but the max_depth parameter only for the Random Forest. atom . run ( models = [ \"ET\" , \"RF\" ], n_calls = 30 , bo_params = { \"dimensions\" : { \"all\" : \"n_estimators\" , \"RF\" : \"max_depth\" }}, ) Like the columns parameter in atom's methods, you can exclude parameters from the BO adding ! before its name. It's possible to exclude multiple parameters, but not to combine inclusion and exclusion for the same model. For example, to optimize a Random Forest using all its predefined parameters except n_estimators , run: atom . run ( models = \"ET\" , n_calls = 15 , bo_params = { \"dimensions\" : \"!n_estimators\" }) If just the parameter name is provided, the predefined dimension space is used. It's also possible to provide custom dimension spaces, but make sure the dimensions are compliant with skopt's API . See every model's individual documentation in the API section for an overview of their hyperparameters and dimensions. from skopt.space.space import Categorical , Integer atom . run ( models = [ \"ET\" , \"RF\" ], n_calls = 30 , bo_params = { \"dimensions\" : { \"all\" : Integer ( 10 , 100 , name = \"n_estimators\" ), \"RF\" : [ Integer ( 1 , 10 , name = \"max_depth\" ), Categorical ([ None , \"sqrt\" , \"log2\" , 0.7 ], name = \"max_features\" ), ], }, }, ) Note When specifying dimension spaces manually, make sure to import the dimension types from scikit-optimize: from skopt.space.space import Real, Categorical, Integer . Warning Keras' models can only use hyperparameter tuning when n_jobs=1 or bo_params={\"cv\": 1} . Using n_jobs > 1 and cv > 1 raises a PicklingError due to incompatibilities of the APIs. Read here more about deep learning models. The majority of skopt's callbacks to stop the optimizer early can be accessed through bo_params . Other callbacks can be included through the callbacks key. atom . run ( models = \"LR\" , n_calls = 30 , bo_params = { \"callbacks\" : custom_callback ()}, ) It's also possible to include additional parameters for skopt's optimizer as key-value pairs. atom . run ( \"LR\" , n_calls = 10 , bo_params = { \"acq_func\" : \"EI\" })","title":"Hyperparameter tuning"},{"location":"user_guide/training/#bootstrapping","text":"After fitting the estimator, you can assess the robustness of the model using the bootstrap technique. This technique creates several new data sets selecting random samples from the training set (with replacement) and evaluates them on the test set. This way we get a distribution of the performance of the model. The sets are the same for every model. The number of sets can be chosen through the n_bootstrap parameter. Tip Use the plot_results method to plot the boostrap scores in a boxplot.","title":"Bootstrapping"},{"location":"user_guide/training/#early-stopping","text":"XGBoost , LightGBM and CatBoost allow in-training evaluation. This means that the estimator is evaluated after every round of the training, and that the training is stopped early if it didn't improve in the last early_stopping rounds. This can save the pipeline much time that would otherwise be wasted on an estimator that is unlikely to improve further. Note that this technique is applied both during the BO and at the final fit on the complete training set. There are two ways to apply early stopping on these models: Through the early_stopping key in bo_params . This approach applies early stopping to all models in the trainer and allows the input of a fraction of the total number of rounds. Filling the early_stopping_rounds parameter directly in est_params . Don't forget to add _fit to the parameter to call it from the fit method. After fitting, the model gets the evals attribute, a dictionary of the train and test performances per round (also if early stopping wasn't applied). Click here for an example using early stopping. Tip Use the plot_evals method to plot the in-training evaluation on the train and test set.","title":"Early stopping"},{"location":"user_guide/training/#successive-halving","text":"Successive halving is a bandit-based algorithm that fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason, we recommend only to use this technique with similar models, e.g. only using tree-based models. Use successive halving through the SuccessiveHalvingClassifier / SuccessiveHalvingRegressor classes or from atom via the successive_halving method. Consecutive runs of the same model are saved with the model's acronym followed by the number of models in the run. For example, a Random Forest in a run with 4 models would become model RF4 . Click here for a successive halving example. Tip Use the plot_successive_halving method to see every model's performance per iteration of the successive halving.","title":"Successive halving"},{"location":"user_guide/training/#train-sizing","text":"When training models, there is usually a trade-off between model performance and computation time, that is regulated by the number of samples in the training set. Train sizing can be used to create insights in this trade-off, and help determine the optimal size of the training set. The models are fitted multiple times, ever-increasing the number of samples in the training set. Use train sizing through the TrainSizingClassifier / TrainSizingRegressor classes or from atom via the train_sizing method. The number of iterations and the number of samples per training can be specified with the train_sizes parameter. Consecutive runs of the same model are saved with the model's acronym followed by the fraction of rows in the training set (the . is removed from the fraction!). For example, a Random Forest in a run with 80% of the training samples would become model RF08 . Click here for a train sizing example. Tip Use the plot_learning_curve method to see the model's performance per size of the training set.","title":"Train sizing"}]}